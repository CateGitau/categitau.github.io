<?xml version="1.0" encoding="UTF-8" ?>
<!-- This is a WordPress eXtended RSS file generated by WordPress as an export of your site. -->
<!-- It contains information about your site's posts, pages, comments, categories, and other content. -->
<!-- You may use this file to transfer that content from one site to another. -->
<!-- This file is not intended to serve as a complete backup of your site. -->

<!-- To import this information into a WordPress site follow these steps: -->
<!-- 1. Log in to that site as an administrator. -->
<!-- 2. Go to Tools: Import in the WordPress admin panel. -->
<!-- 3. Install the "WordPress" importer from the list. -->
<!-- 4. Activate & Run Importer. -->
<!-- 5. Upload this file using the form provided on that page. -->
<!-- 6. You will first be asked to map the authors in this export file to users -->
<!--    on the site. For each author, you may choose to map to an -->
<!--    existing user on the site or to create a new user. -->
<!-- 7. WordPress will then import each of the posts, pages, comments, categories, etc. -->
<!--    contained in this file into your site. -->

	<!-- generator="WordPress/5.3.2" created="2020-03-08 14:38" -->
<rss version="2.0"
	xmlns:excerpt="http://wordpress.org/export/1.2/excerpt/"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:wp="http://wordpress.org/export/1.2/"
>

<channel>
	<title>Catherine Gitau</title>
	<link>http://categitau.com</link>
	<description>All things Data</description>
	<pubDate>Sun, 08 Mar 2020 14:38:37 +0000</pubDate>
	<language>en-US</language>
	<wp:wxr_version>1.2</wp:wxr_version>
	<wp:base_site_url>http://categitau.com</wp:base_site_url>
	<wp:base_blog_url>http://categitau.com</wp:base_blog_url>

		<wp:author><wp:author_id>1</wp:author_id><wp:author_login><![CDATA[categitau]]></wp:author_login><wp:author_email><![CDATA[catherinegitau94@gmail.com]]></wp:author_email><wp:author_display_name><![CDATA[categitau]]></wp:author_display_name><wp:author_first_name><![CDATA[]]></wp:author_first_name><wp:author_last_name><![CDATA[]]></wp:author_last_name></wp:author>

				
	<generator>https://wordpress.org/?v=5.3.2</generator>

<image>
	<url>https://categitau.com/wp-content/uploads/2018/01/cropped-Baby-Boy-3-32x32.png</url>
	<title>Catherine Gitau</title>
	<link>http://categitau.com</link>
	<width>32</width>
	<height>32</height>
</image> 
<site xmlns="com-wordpress:feed-additions:1">141896596</site>
		<item>
		<title>First things first</title>
		<link>http://categitau.com/first-blog-post/</link>
		<pubDate>Wed, 15 Mar 2017 12:10:01 +0000</pubDate>
		<dc:creator><![CDATA[categitau]]></dc:creator>
		<guid isPermaLink="false">https://categitau.wordpress.com/?p=4</guid>
		<description></description>
		<content:encoded><![CDATA[Whoop....welcome to my first blog post!<!--more keep reading!-->

First things first, who am I? why have I decided to start blogging? and what do I intend to do with my blog?

<strong>Who am I?</strong>

At this time, I'm just a final year student at the Jomo Kenytatta University of Agriculture and Technology pursuing a degree in Mathematics and Computer science, majoring in statistics (<em>Best decision I've ever made! </em>I'll get back to this in a later post).

<strong>The Why</strong>

I learnt something from one of my favorite motivational speakers:
<blockquote>“People don't buy what you do; they buy why you do it. And what you do simply proves what you believe”
― <a href="https://www.goodreads.com/author/show/3158574.Simon_Sinek">Simon Sinek</a>, <a href="https://www.goodreads.com/work/quotes/7367737">Start with Why: How Great Leaders Inspire Everyone to Take Action</a></blockquote>
Everyone needs to know their why. Why they are doing things they do, by why I mean their purpose or cause - Why do you wake up every morning to go to work? Why do you go to school? Why does your company exist? Why do you do the things you do? Knowing your why is what keeps you going, it's what gets you motivated and ready to face the world every morning. Since this is my first blog post, I’d like to jot down my why, this will be the reason I’ll come back and add content to my blog.

So anyway, I recently discovered that blogging can be a fantastic way to demonstrate my skills, learn topics in more depth through research and also build an audience. Being in my final year, I believe that this will be a great way to share what I know, learn by sharing and display my skills through projects that I've done and just everything I’m interested in in general.

<strong>What?</strong>

So, what do I intend to post on this blog? Well, a couple of months ago, I discovered my niche...wait for it.........drum roll......<strong>DATA SCIENCE!</strong> and everything data! So, this blog will be somewhat my journal, displaying things that I've learnt about data science, everything data and post my projects for people to comment on, and discuss.

So let's get Numpying!..get it?Numpy?.. Numpying..Never mind, bad Joke, bad joke!]]></content:encoded>
		<excerpt:encoded><![CDATA[This is the excerpt for your very first post.]]></excerpt:encoded>
		<wp:post_id>28</wp:post_id>
		<wp:post_date><![CDATA[2017-03-15 09:10:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-03-15 09:10:01]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[first-blog-post]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[sharing_disabled]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{i:0;i:1;}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[switch_like_status]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{i:0;i:1;}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[37]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_publicize_job_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[2886261870]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1583554905;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:865;}i:1;a:1:{s:2:"id";i:141;}i:2;a:1:{s:2:"id";i:1917;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_facebook_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_cache_timestamp]]></wp:meta_key>
		<wp:meta_value><![CDATA[430787]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_pinterest_image_url]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_totes]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>NumPy..What??</title>
		<link>http://categitau.com/numpy-what/</link>
		<pubDate>Tue, 21 Mar 2017 22:57:44 +0000</pubDate>
		<dc:creator><![CDATA[categitau]]></dc:creator>
		<guid isPermaLink="false">https://categitau.wordpress.com/?p=25</guid>
		<description></description>
		<content:encoded><![CDATA[<!--more-->In my last post <a href="https://categitau.wordpress.com/2017/03/15/first-blog-post/">first things first</a> , I made a Data science 'Joke' at the end , Mentioning some weird word <strong><em>NumPy</em></strong>.. So, what's <strong>NumPy?</strong>

NumPy stands for Numerical Python. It is one of the many libraries in python used for scientific computations and data analysis. It also provides basic routines for manipulating large arrays and matrices of numeric data. So, no matter what the data is, the first step of making it analyzable is by transforming it into arrays.

There are other Libraries used in python such as:
<ul>
	<li><strong>Matplotlib</strong> which is used in plotting graphs</li>
	<li><strong>Pandas </strong>that are used for structured data operations and manipulations.</li>
	<li><strong>SciPy t</strong>hat stands for Scientific Python which is built on Numpy.</li>
	<li><strong>SciKit Learn </strong>which is used in Machine learning and built on Numpy, Matplotlib and SciPy.</li>
	<li><strong>Bokeh </strong>for creating interactive plots and data applications on modern web browsers.</li>
</ul>
...and others, but today I'll just talk about NumPy and just a little bit of Pandas.

The most powerful feature of NumPy is N-Dimensional array. An <em>ndarray (N-Dimensional array) </em>is usually a multidimensional container of items of the same size and type. The number of dimensions in an array is defined by Its shape, which is a tuple of N positive integers that specify the size of each dimension.

Example:

<img class="alignnone size-full wp-image-139" src="http://categitau.com/wp-content/uploads/2017/03/ndarrays.png" alt="ndarrays" width="546" height="125" />

NumPy also has various attributes such as <em>ndim</em> (number of dimensions), <em>shape</em>(size of each dimension and <em>size</em>(the total size of the array):

<img class="alignnone size-full wp-image-105" src="http://categitau.com/wp-content/uploads/2017/03/attributes.png" alt="attributes" width="553" height="121" />

You can get more data types on <a href="http://localhost:8888/notebooks/02.01-Understanding-Data-Types.ipynb">Understanding Data Types in Python</a>

The central feature in <strong>NumPy</strong> is the <em>array</em> object class, arrays are somewhat similar to lists in python, except that every element of an array must be of the same time. i.e. int, Boolean, strings must be the same in every array, but in Python, you can mix it up with lists, strings and Boolean.

NumPy also includes a bunch of convenient functions such as mean (), std () , you can also use these functions on python lists but if your data is in NumPy array, the these functions will be faster.

Some similarities of NumPy arrays and Python Lists are that in both you can access elements by position, you can also access a range of elements e.g. [3:5] and you can use for loops in both.

NumPy's <em>ndarray</em> data structure provides essential features for the type of clean and well organized data. While this feature serves its purpose well, it’s disadvantage becomes very clear when we need more flexibility. Panda, and in particular its S<em>eries</em> and <em>Data Frame</em> objects, builds on Numpy array structure and provides access to these data hacking tasks.

Gosh, I could go on and on about what I know about NumPy, but you can find some great Numpy tutorials out there for you to practice on and learn about them together with other Python libraries. Some tutorials can be found on my <a href="https://github.com/CateGitau/Intro-to-Data-Analysis">GitHub</a> account. Check them out and also give me a follow if you're on GitHub yourself.

If you don't have NumPy installed, you can go to <a href="http://www.numpy.org/">http://www.numpy.org/</a> and follow the installation instructions.

Pandas Installation can be found on <a href="http://pandas.pydata.org/">Pandas Documentation</a>. Once it is installed, you can import it and check the version.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>38</wp:post_id>
		<wp:post_date><![CDATA[2017-03-21 19:57:44]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-03-21 19:57:44]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[numpy-what]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="data-science"><![CDATA[Data Science]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[94]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_rest_api_published]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_rest_api_client_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[-1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_publicize_job_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[3121394090]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1583558432;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:451;}i:1;a:1:{s:2:"id";i:28;}i:2;a:1:{s:2:"id";i:1692;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_totes]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_pinterest_image_url]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_cache_timestamp]]></wp:meta_key>
		<wp:meta_value><![CDATA[431790]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_facebook_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_pinterest_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_twitter_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_linkedin_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_google_plus_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_total_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>#NTW2017 Day 1</title>
		<link>http://categitau.com/ntw2017-day-1/</link>
		<pubDate>Tue, 28 Mar 2017 10:28:55 +0000</pubDate>
		<dc:creator><![CDATA[categitau]]></dc:creator>
		<guid isPermaLink="false">https://categitau.wordpress.com/?p=141</guid>
		<description></description>
		<content:encoded><![CDATA[<strong>Nairobi Tech Week (NTW)</strong> is sub-Saharan's largest annual Tech event. This year they were holding their second event in Strathmore University, Nairobi Kenya.<!--more keep on reading!-->

Was so excited to attend this year's event for two reasons: one, since the previous one was really engaging and very insightful and second, they actually accepted my application for attending this year’s event! :b  after rejecting my application last year (<em>even though I still found my way to attend the event</em>)<em>.</em>

Anyway, it's <em>#day 1</em> ,at around 0945hrs, the event officially started . We were introduced to some of the #NTW sponsors from IBiz, Angelhack and the Co-founder of Moringa school. After the introductions, we were then asked to take breakfast and later we got to our rooms where some workshops and talks were being done according to your interests.

The first talk I attended was by <strong>Safaricom: Intellectual Property Rights</strong> by Reuben K Lang'at. <strong>Intellectual Property</strong> refers to the creations of the human mind such as inventions, images, symbols used in commerce. While <strong>IPR</strong>(Intellectual Property Rights) is a set of Legal rights granted by national regional government authority to creators of new ideas being protected. It was not surprising how only about ten people showed up to this talk. It was clear that most people were more interested in the tech talks. But I believe that it's also as important to know how your idea/application or software can be protected.

In this talk we got to understand more about how one can protect their ideas, brand name, applications and software. I also got to learn about a Patent, Copyright related rights, Utility model, why one should protect trademarks and much more. If any one of you guys would want to protect your idea or brand just go to <strong>KIPI(Kenya Industrial Property Institute).</strong>You can thank me later :)

Aaaaannnddd...It was Lunch time!!My second favorite thing after Sleep! We were served some <em>Bhajiah</em> looking potatoes together with tasty chicken and juice with an apple! mh! mh! mh. During lunch we were presented with some questions about how to empower women in tech and had a chat about it with my table mates. I enjoyed it!

At around 1400hrs, we were asked to wind up and start heading to the afternoon workshops. I was torn between attending a workshop by <strong>JUMO,</strong> a fintech company and Artificial Intelligence by <strong>Intel. </strong>But since my interest lays between Data science and Machine learning I ended up spending my afternoon being shown applications of Artificial intelligence and the Deep Learning techniques that Intel uses.

During this talk, I learnt about the Machine learning pipeline :

<em><img class="alignnone size-full wp-image-215" src="http://categitau.com/wp-content/uploads/2017/03/ml-pipeline.png" alt="ML Pipeline" width="348" height="58" /></em>

We first feed Data to the Machine Learning model together with our algorithm so as to make a prediction.

There are 3 types of ML: <strong>Supervised, Unsupervised and Reinforcement Learning</strong>, maybe we can get deeper into this at a later post :) and there are also 3 Machine learning solutions which are; Classification, regression and clustering. Then after some theory about AI and Machine learning we were shown a Demo on how to use one of the Intel deep Learning SDK. It was amazing!

After the Intel workshop, we had a small break and people dispersed to the final workshops of the day and I was super excited about the one I was attending!

It was <strong>Think Like Data Scientist</strong> by Chris Orwa! Oh, how I had always wanted to meet this man! How he gets insights from data is amazing. We once had an assignment in one of our course units to do from one of his data science articles/Project about <strong>Breaking Safaricom scratch card code</strong> . Did you know that if the third number of a Safaricom scratch card is greater than zero, the sixth number is the same number as the third minus one! e.g. if the 3rd number is 4 then the sixth number is 3, if its 5 then the sixth is 4 and so on. How cool is that! with some statistics and an algorithm he was able to find some relation between the third and sixth number!you can learn more about this <a href="https://www.google.com/amp/s/blackorwa.com/2013/10/05/breaking-safaricom-scratch-card-code/amp/">here</a> .

Anyway, going on... During his workshop, we were able to think like data scientist and we didn't even have to use code!!!how cool is that! I’ll take you guys through this activity:

The "<em>Theme"</em> of this workshop , if I may call it was<strong> "what is the root of all evil?"</strong>

So, we were given a couple of shapes:

<img class="alignnone size-full wp-image-235" src="http://categitau.com/wp-content/uploads/2017/03/shapes.png" alt="shapes" width="492" height="65" />

Then asked to make clusters out of the shapes and take a photo. Well, my group was so confused, we tried making other shapes from the shapes we were given but what we were supposed to do was totally different. Chris then explained what he was trying to put across.

we first created a matrix showing which shapes changed and which ones didn’t when rotated at 45 degrees, 90 degrees, 180 and 360 degrees. our table looked like this:

<img class="alignnone size-full wp-image-247" src="http://categitau.com/wp-content/uploads/2017/03/rotate.png" alt="rotate" width="350" height="144" />

The 1's show that the shape changed and the 0's show that the shape didn't change. from this table, we can now put together the shapes that had the same number of changes. The circle clearly doesn't change shape no-matter how many times you rotate it. That's the first cluster. The square rotated once, second cluster. The rectangle twice, third cluster, and the triangle, semi-circle and quarter circle rotated thrice,  the final cluster.

from this we came up with  a tree diagram by putting the clusters together:

<img class="alignnone size-full wp-image-253" src="http://categitau.com/wp-content/uploads/2017/03/cluster.png" alt="cluster" width="326" height="324" />

We can see that from the circle, we can get a square, from a square you can get a rectangle, from a rectangle you can make it into a triangle, a semi-circle or a quarter-circle, so in this case our root of all evil is the circle!

We also did an exercise on how Java or any restaurant would know the root of all evil by getting data from customers by asking them why they wouldn’t suggest the place to their friends. From this information, we created a matrix using some features from the data , then put them in clusters ,and through the same process with the shapes, you can find out where the root of all evil is I.e. you can know the main reason why some people would not recommend java to their friends.

This concept is known as the <strong>Dendrogram concept. </strong>Whatever is mostly common stays on top. You can go on and read about it if my explanation didn't make any sense .

So, I got to meet Chris and other interesting people, I'd say my day was really insightful and I learnt a lot! I bet the other two days will also have a lot to offer! :)]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>141</wp:post_id>
		<wp:post_date><![CDATA[2017-03-28 07:28:55]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-03-28 07:28:55]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[ntw2017-day-1]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="events-meetups"><![CDATA[Events &amp; Meetups]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_rest_api_published]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_rest_api_client_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[-1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_publicize_job_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[3354370491]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1582939960;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:1719;}i:1;a:1:{s:2:"id";i:1493;}i:2;a:1:{s:2:"id";i:1835;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_totes]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_cache_timestamp]]></wp:meta_key>
		<wp:meta_value><![CDATA[431646]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_facebook_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_pinterest_image_url]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_pinterest_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_twitter_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_linkedin_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_google_plus_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_total_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>What&#039;s next?</title>
		<link>http://categitau.com/whats-next/</link>
		<pubDate>Wed, 05 Apr 2017 17:53:49 +0000</pubDate>
		<dc:creator><![CDATA[categitau]]></dc:creator>
		<guid isPermaLink="false">https://categitau.wordpress.com/?p=339</guid>
		<description></description>
		<content:encoded><![CDATA[I was shown this <a href="https://www.r-bloggers.com/are-you-fluent-in-r/" target="_blank" rel="noopener">blog</a> post by a classmate with the heading <strong>'Are you fluent in R?'</strong>... Well at first I thought it was just talking about techniques used in R and since I'm not really focusing on it at the moment, I wasn't as interested, but after reading the first few lines I realized that this blog post applies to almost everyone who wants to venture into Data Science.<!--more keep on reading!-->

I got to ask myself a few questions :

After taking courses on Data Science in python and R , can I write the codes from memory?.. Do I still Google search to find out how to execute basic data science techniques?

Then it just hit me that I'm still struggling to remember certain essentials, even after taking lots of courses on Data science and reading a lot of material.

So, I decided to start my own small project that will help me gain knowledge and become fluent in the basics of python code to perform data visualization, data manipulation, and data analysis. My aim will be to be able to write code proficiently, accurately, rapidly and confidently.Well, at least the basics.

My next posts will focus on these essentials with some fun and simple projects.We'll be able to get data, manipulate, clean, wrangle, visualize it then finally get insights from it.

Excited? I don't know about you but I am :)

Watch this space!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>339</wp:post_id>
		<wp:post_date><![CDATA[2017-04-05 14:53:49]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-04-05 14:53:49]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[whats-next]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_rest_api_published]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_rest_api_client_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[-1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_publicize_job_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[3664581015]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1583381671;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:1835;}i:1;a:1:{s:2:"id";i:668;}i:2;a:1:{s:2:"id";i:1880;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_facebook_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_cache_timestamp]]></wp:meta_key>
		<wp:meta_value><![CDATA[431868]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_pinterest_image_url]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_totes]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_pinterest_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_twitter_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_linkedin_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_google_plus_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_total_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Credit Score Model</title>
		<link>http://categitau.com/credit-score-modelintroduction/</link>
		<pubDate>Tue, 18 Jul 2017 16:02:34 +0000</pubDate>
		<dc:creator><![CDATA[categitau]]></dc:creator>
		<guid isPermaLink="false">https://categitau.wordpress.com/?p=451</guid>
		<description></description>
		<content:encoded><![CDATA[Like I promised, here’s a project I was working on about a month ago.<!--more-->

A <strong>credit score model</strong> is defined as
<blockquote>“a mathematical model which attempts to provide a quantitative estimate of the probability that a customer will display a defined behavior (e.g. Loan default, bankruptcy or a lower level of delinquency) with respect to their current or proposed credit position with a lender.”</blockquote>
In other words, a credit score assesses an entities ability to pay its financial obligations. They are also predictive when used with advanced tools that evaluate the likelihood of one to pay or default a loan based on some entities.
Usually, the parties that assess the credit worthiness of an entity e.g. Financial Institutions, Banks etc. use their own methodology to measure the credit worthiness and use their own rating scale.

An example of an Interesting company that determines whether you're likely to repay a loan is <strong><em>Zestfinance. </em></strong>The unique thing about Zestfinance, is the fact that it doesn't necessarily need your financial history when determining whether you're likely to repay a loan or not . It instead uses Machine learning to determine based on many data sources, whether you're eligible. Among the many factors they look at is whether you type your name with proper capitalization or in all caps. According to founder and chief executive of ZestFinance Douglas Merrill,  <em>"If you fill in your name in all caps, you're a much higher risk"</em>. Interesting right? or a bit absurd? I don't know.

So credit scoring isn't the only way one can determine the credit worthiness of a person.

There are also a couple of FinTech companies in Kenya that use the credit scoring method to provide loans to people. These are : <strong>Pezesha</strong>, <strong>Tala</strong>, <strong>Branch</strong>, <strong>Umati capital</strong> just to mention a few.

Honestly, I had no idea what this was until a few months ago when I participated in the <strong>DATAHACK4FI</strong>  challenge that was being hosted by Brave Venture labs, Kenya. This was a competition that was aimed to develop Innovative data-driven digital solutions to advance access to financial services for low-income earners in Kenya.

I was approached to be part of a team whose idea was about hacking the problem of getting some working capital for small business owners using a credit score model to determine their credit worthiness and providing loans to small business owners. To my surprise, we came in third!

After the competition I got the chance to play around with MPESA Statements and eventually came up with a simple credit score model. The following diagram shows the steps I took to come up with the model.

<img class="alignnone size-full wp-image-657" src="http://categitau.com/wp-content/uploads/2017/07/capture22.png" alt="Capture22" width="588" height="425" />
<ul>
 	<li><span style="text-decoration: underline;"><strong><em>Loading the data</em></strong></span></li>
</ul>
[code language="python"]

import pandas as pd

df=pd.read_csv("home/creditscore_model/data.csv")

[/code]

The code above should load the data.csv file in a DataFrame 'df'.

You can get an article on Analytics Vidhya that talks about how to read various file formats in Data Science(Using Python) <a href="https://www.analyticsvidhya.com/blog/2017/03/read-commonly-used-formats-using-python/" target="_blank" rel="noopener">here</a>.
<ul>
 	<li><span style="text-decoration: underline;"><em><strong>Merge</strong></em></span></li>
</ul>
[code language="python"]

import pandas as pd

df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],
'B': ['B0', 'B1', 'B2', 'B3'],
'C': ['C0', 'C1', 'C2', 'C3'],
'D': ['D0', 'D1', 'D2', 'D3']})

df2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'],
'B': ['B4', 'B5', 'B6', 'B7'],
'C': ['C4', 'C5', 'C6', 'C7'],
'D': ['D4', 'D5', 'D6', 'D7']})

df3 = pd.DataFrame({'A': ['A8', 'A9', 'A10', 'A11'],
'B': ['B8', 'B9', 'B10', 'B11'],
'C': ['C8', 'C9', 'C10', 'C11'],
'D': ['D8', 'D9', 'D10', 'D11']})

frames = [df1, df2, df3]
result = pd.concat(frames,keys=['M1','M2','M3'])

[/code]

The above code merges the DataFrames df1, df2, and df3. This is how I merged the 50 Individual Financial statements that I used for my model. You can get more info on merging and sorting and concatenate <a href="http://pandas.pydata.org/pandas-docs/stable/merging.html">Here</a>.
<ul>
 	<li><em><strong>Data Janitoring and Exploration</strong></em></li>
</ul>
This is just my fancy way of calling data cleaning. There are various ways of ensuring that your data is clean and consistent. In my case, given 50 Financial statements I had to:
<ol>
 	<li>Ensure the values in the data are of a certain type. For example the Date and time.</li>
 	<li>Ensure that there are no missing values in the required fields such as someones National ID, since without it you're not able to determine or identify the person and also transaction types.</li>
 	<li>Ensure that certain unique values are not repeated such as the National IDs.</li>
 	<li>Ensure that the data has no errors.</li>
</ol>
Did you know that most data scientists take about 70% of their project time exploring, cleaning and preparing data? If you didn't, now you know. Data exploration plays a big role in data analysis. No one can build the best predictive models without exploring their data first. A lot of work is done during this step.
<ul>
 	<li><span style="text-decoration: underline;"><strong><em>Feature Engineering</em></strong></span></li>
</ul>
Feature Engineering is the science of extracting more information from the existing data. No data is being added or being removed, but rather you are trying to make the data you have more useful. Feature engineering is usually done after the data has been explored. You can find more information about it <a href="https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/#four">here</a>

In this case having the financial statements with the Variables : <strong>Transaction Time, Transaction Type, Transactee, Sent Amount, Received Amount</strong>. What more information can we get from this data?

For example, we could try and get the average amount each individual earned that year:

[code language="python"]

#Import libraries

import numpy as np

import pandas as pd

import matplotlib as plt

#load in the dataset

df = pd.read_csv(" ")

#drop the unnecessary columns

df = df.drop("['TransactionId','SentAmount','transactee']")

#view data

df.head()

#remove all the rows in column Receipt that have NaNs

receipt = df.dropna(axis=0, how='any')

#to view the mean

receipt.describe()

[/code]

We could also get some more features such as the months, to find out which months most MPESA transactions done.
<ul>
 	<li><span style="text-decoration: underline;"><em><strong>Criteria and Weightages</strong></em></span></li>
</ul>
To build the model, you need to come up with various criteria that will determine whether the person is worth getting a loan or not. In this case, I came up with 5 and gave each of them weights that total up to 100% as shown in the diagram below:

<img class="alignnone size-full wp-image-654" src="http://categitau.com/wp-content/uploads/2017/07/criteria.png" alt="Criteria" width="603" height="423" />

<em><strong>The Methodology used:</strong></em>
<ol>
 	<li><strong>Credit History( Taken Credit or Not) -</strong> Those who had taken credit before got a score of <strong>100</strong> and those that had not got a <strong>0. </strong>So, If an individual had taken a loan, from the diagram above we see that the weight given is 35% so we multiply this value with 100. So, they get a score of 35 in this criteria.</li>
 	<li><strong>Average Monthly Receipts</strong> <strong>-</strong> Those who have an average monthly income of  &lt;1,000Ksh I gave them a score of <strong>0</strong>, 1K-4K got a score of <strong>10</strong>, 5k-7k got <strong>20</strong> and so on and those greater than 60,000Ksh got <strong>100</strong>. e.g. If there's an individual with an average income of lets say 6,000Ksh, we'd take the weight of 30% multiply it with 20, so their final score here will be 6.</li>
 	<li><strong>Loan Amount not paid back</strong> <strong>-</strong> In this case, those that had more loan debt were a higher risk than those that had less loan debt to pay. So, those with &lt;500Ksh in debt got a score of <strong>100</strong>, those with 500-100 got <strong>90</strong> , those with greater than 10,000 got a <strong>0</strong>. Multiply these scores with the weights and you'll get the final scores.</li>
 	<li><strong>No. of times a loan is taken</strong> <strong>-</strong> I assumed that those who've borrowed more times are at a lower risk than those who had borrowed less. So, those who borrowed once got a <strong>0</strong>, those who borrowed 1-4 times got a score of <strong>20</strong>, those above 20 times got <strong>100</strong>.</li>
 	<li><strong>No. of Organizations a loan is taken -</strong> In this case I assumed that those who borrowed from many organizations might be experiencing financial stress, hence high risk. So those that borrowed from more than 3 organizations got a score of <strong>25</strong>, those who borrowed from 1 got <strong>100.</strong></li>
</ol>
After calculating their total scores and adding them, I visualized the results and we can now see the people who are of more risk and others of less risk. The ones with high scores are more credit worthy than those with less scores.

<img class="alignnone size-full wp-image-656" src="http://categitau.com/wp-content/uploads/2017/07/score.png" alt="score" width="988" height="557" />

<span style="text-decoration: underline;"><strong>The End</strong></span>

&nbsp;]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>451</wp:post_id>
		<wp:post_date><![CDATA[2017-07-18 13:02:34]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-07-18 13:02:34]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[credit-score-modelintroduction]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="data-science"><![CDATA[Data Science]]></category>
		<category domain="category" nicename="projects"><![CDATA[Projects]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[457]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_rest_api_published]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_rest_api_client_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[-1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_publicize_job_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[7215904615]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1583569062;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:1787;}i:1;a:1:{s:2:"id";i:1774;}i:2;a:1:{s:2:"id";i:2385;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_pinterest_image_url]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_cache_timestamp]]></wp:meta_key>
		<wp:meta_value><![CDATA[431868]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_facebook_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_totes]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_linkedin_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_twitter_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_pinterest_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_google_plus_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_total_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Twitter data Mining 101 using R</title>
		<link>http://categitau.com/twitter-data-mining-101-using-r/</link>
		<pubDate>Wed, 30 Aug 2017 18:02:53 +0000</pubDate>
		<dc:creator><![CDATA[categitau]]></dc:creator>
		<guid isPermaLink="false">https://categitau.wordpress.com/?p=668</guid>
		<description></description>
		<content:encoded><![CDATA[<strong>R</strong> is a tool used widely in data analysis and statistical computing.<!--more--> To learn data science, one has to choose either R or Python as a starter. I chose to learn both and I use them depending on the project I am working on. However, my love for R is gradually increasing every single day!

So, for this specific task, I decided to use <strong>R</strong> to gain more knowledge on the language. This is a project I did while still in the University just out of curiosity but I just figured out how cool it would be to actually analyze data from twitter. This would've probably been more fun during election period, but we can still have fun getting insights from trending topics on twitter. But first, let me show you guys how I extracted this data.

PREREQUISITES:
<ul>
 	<li>Already installed R and R Studio.</li>
 	<li>A Twitter account to create your Twitter application so as to be able to extract tweets. You can get instructions on how to get a Twitter application <a href="http://docs.inboundnow.com/guide/create-twitter-application/" target="_blank" rel="noopener">here</a>. After getting your consumer key and consumer secret, then you're good to go.</li>
</ul>
<strong>NB</strong>:Your OAuth settings, i.e. your consumer key and consumer secret, should be kept private and if anyone was to access them they could easily access your twitter account.

After getting your OAuth Keys, we can now get started with installing and loading the R packages necessary for our task. One of the reasons why I love R is the availability of its many packages which are customized to perform various tasks.

Let's install and load the required packages:

[sourcecode language="r"]

#install required packages
install.packages(&quot;twitteR&quot;)
install.packages(&quot;RCurl&quot;)
install.packages(&quot;httr&quot;)
install.packages(&quot;devtools&quot;)
devtools::install_github(&quot;r-lib/httr&quot;)

#Load necessary packages
library(twitteR)
library(RCurl)
library(base64enc)

[/sourcecode]

Remember your consumer key, consumer secret, access token and access token secret? Yeah, so this is where we'll require them. Provide these credentials in place of these empty string values that are defined as placeholders.

[sourcecode language="r"]

Access_token &lt;- &quot;&quot;
Access_token_secret &lt;- &quot;&quot;
consumer_key &lt;- &quot;&quot;
consumer_secret &lt;- &quot;&quot; setup_twitter_oauth(consumer_key,consumer_secret,Access_token,Access_token_secret) #the output should be --&amp;gt; [1] &quot;Using direct authentication&quot;

#with no error

[/sourcecode]

With that done, we can start extracting data from twitter!

Let's start by extracting Trending topics in Kenya. The <strong>twitteR </strong>package has <strong>getTrends</strong> function that can be used to extract the twitter trends based on an input parameter (woeid). A <a href="https://en.wikipedia.org/wiki/WOEID">WOEID</a>(Where On Earth IDentifier) identifies any feature on earth. I used the WOEID for Kenya to extract trending topics from Kenya. You can use this <a href="http://woeid.rosselliot.co.nz/">link</a> for WOEID lookup.

To extract Trending topics in Kenya, you run the following code:

[sourcecode language="r"]

#Extracting Trends using getTrends Function
KE_WOE_ID = 23424863
current_trends &lt;- getTrends(KE_WOE_ID)

[/sourcecode]

To extract tweets from a certain user using their handle:

[sourcecode language="r"]

tweets &lt;- userTimeline(&quot;POTUS&quot;, 50)

[/sourcecode]

To extract tweets in a certain Trending topic you use the following code. In my analysis I decided to look into #IEBC:

[sourcecode language="r"]

IEBC_Tweets &lt;- searchTwitter(&quot;IEBC&quot;, n=100, lang = &quot;en&quot;)

[/sourcecode]

There are many other functions that can be used to extract more data from Twitter. If this excites you, you can look into the <a href="https://github.com/SMAPPNYU/smappR" target="_blank" rel="noopener">smappr</a> package that offers many more functions and tools for analysis of twitter data. If you'd like to play around with my code, you can find it <a href="https://gist.github.com/CateGitau/d1c2b7d4244eb732b5ed6cad8bcf16f6" target="_blank" rel="noopener">here</a>.

Till next time, keep fit and have fun coding ! :)]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>668</wp:post_id>
		<wp:post_date><![CDATA[2017-08-30 15:02:53]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-08-30 15:02:53]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[twitter-data-mining-101-using-r]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="data-science"><![CDATA[Data Science]]></category>
		<category domain="post_tag" nicename="mining"><![CDATA[mining]]></category>
		<category domain="post_tag" nicename="text-analytics"><![CDATA[Text analytics]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[670]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_publicize_job_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[8786551427]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_rest_api_published]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_rest_api_client_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[-1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1583568100;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:865;}i:1;a:1:{s:2:"id";i:1835;}i:2;a:1:{s:2:"id";i:339;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_cache_timestamp]]></wp:meta_key>
		<wp:meta_value><![CDATA[431868]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_pinterest_image_url]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_facebook_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_totes]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_pinterest_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_twitter_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_linkedin_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_google_plus_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_total_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Text Analysis with R</title>
		<link>http://categitau.com/text-mining-with-r/</link>
		<pubDate>Thu, 21 Sep 2017 18:50:24 +0000</pubDate>
		<dc:creator><![CDATA[categitau]]></dc:creator>
		<guid isPermaLink="false">https://categitau.wordpress.com/?p=865</guid>
		<description></description>
		<content:encoded><![CDATA[Do you know the most frequently used words on your twitter timeline? Or, the most frequently mentioned tweeps(twitter followers) on your timeline?<!--more-->

In the last couple of weeks I've been learning a lot on text analysis and Natural language processing techniques and as I was trying to figure out what I'll post on my blog, I thought, why not analyze data from my twitter timeline and write about it?So, here we are. If you'd like to analyze your own twitter data and don't know how to extract it, you can refer to my previous <a href="https://categitau.wordpress.com/2017/08/30/twitter-data-mining-101-using-r/">article</a>.

In this post, I am going to walk you through how I analyzed Twitter text data and built a Word cloud using R.

<strong>Data Cleaning
</strong>

According to estimates, 80% of the available data is unstructured. Data is being produced as we speak, tweet, send texts via Whatsapp and various other activities that we do everyday. Most of this data being generated is not organized in a pre-defined manner and is text heavy. Few examples of this data include : <em>tweets, posts on social media, chat conversations, reports e.t.c.</em>

Twitter data is highly unstructured since it's an informal way of communication. The data therefore consists a lot of<em> 'noise'</em>. i.e. typos, bad grammar, presence of unwanted content like URLs, Expressions, emojis, Stopwords etc. In order to produce any meaningful insights from text data, It is really important to clean or pre-process it first before getting into analyzing the data.

For this exercise, I first loaded the <strong>tm</strong> library which is a framework for text mining applications in R :

[sourcecode language="r"]

library(tm)

[/sourcecode]

Built a corpus(myCorpus), which is a collection of written texts and specified the source to be a character of vectors.

[sourcecode language="r"]

myCorpus &lt;- Corpus(VectorSource(tweets.df$text))

[/sourcecode]

Never in my life had I thought that I'd come to despise emoticons. If you have ever retrieved data From Twitter or Facebook with R, you might have noticed that while R seems to be able to display some emoticons properly, many other times it does not thus making any further analysis impossible unless you get rid of them. It took me a while to figure out what I was doing wrong. The following code worked for me:

[sourcecode language="r"]
#remove emoticons from corpus
myCorpus &lt;-  tm_map(myCorpus, function(x) iconv(enc2utf8(x), sub = &quot;byte&quot;))

[/sourcecode]

After this step, I then transformed each letter to lowercase before cleaning:

[sourcecode language="r"]

#convert myCorpus into lowercase
myCorpus &lt;- tm_map(myCorpus, content_transformer(tolower))

[/sourcecode]

I then went ahead and removed URLs, hashtags, mentions, RTs, Stopwords etc. Final step was to build a term document matrix. A <strong>Term Document Matrix</strong> is a two dimensional matrix that consists of terms used or that have appeared in a corpus of documents at the rows and the columns consist of these documents. So, each entry(i,j) represents the frequency of the term i in the document j. The code below shows how to create the tdm(term document matrix):

[sourcecode language="r"]

tdm &lt;- TermDocumentMatrix(myCorpus_copy,control = list(wordlengths = c(1,Inf)))

[/sourcecode]

&nbsp;

<strong>Frequent Words
</strong>

Getting the frequency of the terms I used on my timeline was quite interesting. The insight I got wasn't really surprising at all. I first inspected the terms with a frequency greater than 50, then created a bar graph to visualize the results with the following codes:

[sourcecode language="r"]

#inspect frequent words
freq.terms &lt;- findFreqTerms(tdm, lowfreq = 50)
View(freq.terms)

#visualize frequent terms
library(ggplot2)

ggplot(df,aes(x = reorder(df$term, +df$freq), y = freq, fill=df$freq)) + geom_bar(stat = &quot;identity&quot;) +
scale_colour_gradientn(colors = terrain.colors(10)) + xlab(&quot;Terms&quot;) + ylab(&quot;Count&quot;) + coord_flip()

[/sourcecode]

From the bar graph below, we see that the most frequently used word on my twitter timeline is <em>"haha" </em>and the tweep I've mentioned most on my timeline is murimikamau.  Interesting right?

<img class="alignnone size-full wp-image-1052" src="http://categitau.com/wp-content/uploads/2017/09/rplot03.png" alt="Rplot03" width="385" height="381" />

We can also view this data using a Word Cloud. Word clouds are a fun method for visually presenting text data, and are popular for text analysis because they make it very easy to spot word frequencies. The more frequently the word is used, the larger and bolder it is displayed, as we can see from the word cloud I generated below:

[caption id="attachment_1080" align="alignnone" width="696"]<img class="alignnone size-full wp-image-1080" src="http://categitau.com/wp-content/uploads/2017/09/capture-11.png" alt="Capture.1" width="696" height="678" /> Word cloud displaying frequently used terms on my Twitter timeline[/caption]

So cool right? You can also create word clouds based on any shape you'd like(like on my featured Image), also customize fonts and many other cool stuff. You can have a look at different ways you can build your Word Cloud <a href="https://cran.r-project.org/web/packages/wordcloud2/vignettes/wordcloud.html">here</a> .

Get access to the entire code used for this exercise Including the word cloud on my github account <a href="https://gist.github.com/CateGitau/05e6ff80b2a3aaa58236067811cee44e">here</a>.

Unfortunately, I didn't quite figure out why my wordcloud wasn't displaying all the words shown on the bar graph above. What am I missing here? Let me know in the comments and I'll add it in!

Till next time .. :)]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>865</wp:post_id>
		<wp:post_date><![CDATA[2017-09-21 15:50:24]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-09-21 15:50:24]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[text-mining-with-r]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="data-science"><![CDATA[Data Science]]></category>
		<category domain="category" nicename="tutorials"><![CDATA[Tutorials]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[1018]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_rest_api_published]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_rest_api_client_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[-1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_publicize_job_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[9521834956]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_2e7b27493eb26e4f0ca10b63345b901c]]></wp:meta_key>
		<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1583381407;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:668;}i:1;a:1:{s:2:"id";i:1851;}i:2;a:1:{s:2:"id";i:1835;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_cache_timestamp]]></wp:meta_key>
		<wp:meta_value><![CDATA[431877]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_facebook_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_pinterest_image_url]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_totes]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_pinterest_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_twitter_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_linkedin_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_google_plus_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_total_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Finding groups in data : Clustering techniques</title>
		<link>http://categitau.com/finding-groups-in-data-clustering-techniques/</link>
		<pubDate>Mon, 23 Oct 2017 17:07:28 +0000</pubDate>
		<dc:creator><![CDATA[categitau]]></dc:creator>
		<guid isPermaLink="false">https://categitau.wordpress.com/?p=1115</guid>
		<description></description>
		<content:encoded><![CDATA[In the last couple of months, I got an opportunity to Intern at a startup whose main aim is to <em>Unleash the best in people </em>by matching talent with job opportunities using Machine Learning techniques. I was honored to work with a great team and in turn built a tremendous amount of knowledge thanks to them.

This post will give an Introduction to Machine learning and the two main Clustering algorithms I got to learn and that every aspiring data scientist should know.

&nbsp;

<!--more-->

Wikipedia defines Machine Learning as:
<blockquote>A field in computer science that gives computers the ability to learn without being explicitly programmed.</blockquote>
It basically explores the study and the construction of generic algorithms that can learn from and make predictions on data.

There are several prominent applications of machine learning such as:
<ul>
 	<li>Segmentation of customer behavior for targeted advertising</li>
 	<li>Identification of unwanted spam messages in e-mail</li>
 	<li>Identifying loans that are about to default</li>
 	<li>Prediction of popular election outcomes</li>
</ul>
Just to mention a few.

Machine Learning algorithms come in different flavors, one commonly used class is <em>Supervised</em> and the other is <em>Unsupervised.</em>
<h3>     1. Supervised Machine learning Algorithms</h3>
Supervised Machine learning methods are those that build models that predict an outcome. An example is determining customer churn or predicting whether a person is likely to default on paying a loan or not. Supervised learning requires that you have a training data set of situations of known outcomes. So, If I want to divide customers into two groups, say - “likely to churn” and “not likely to churn”, then I provide the computer with historical examples of such features (training data) and tell it to assign new data to one of the two groups, that’s supervised.
<h3>     2. Unsupervised Machine Learning Algorithms</h3>
Unsupervised machine learning methods are those that don't make explicit outcome predictions, but they discover hidden relationships in the data. For example, you come across a situation where your boss wants to understand customers better so that they can market their products in a better way. This is where unsupervised machine learning comes in. You'll have to find structures in the data or patterns that will help you to group the customers based on what they usually purchase.

An example of a popular unsupervised machine learning algorithm is the <strong>Clustering algorithm</strong>:
<blockquote><em>Clustering </em>is the method of identifying similar groups of data in a data set. Entities that are in the same group are more similar to each other than with the entities in other groups.</blockquote>
One of the stores that take advantage of this technique is<em> <u><a href="https://en.wikipedia.org/wiki/Target_Corporation" target="_blank" rel="noopener">Target corporation</a></u>. </em>Target is a general merchandise store that sells a wide variety of products, from groceries to clothing, to household furnishings to electronic appliances<em>. </em>Target is always learning new ways to market and merchandise products within its stores by studying consumption patterns of customers to figure out what one likes, what you will need and which coupons are most likely to make you happy. They also figured out a way to find out whether you have a baby on the way long before you need to start buying diapers by just analyzing shopping patterns. Awesome right? You can read about how target did this in one of the best books I've read this year <a href="http://charlesduhigg.com/books/the-power-of-habit/" target="_blank" rel="noopener"><strong>The Power of Habit - Why We Do What We Do in Life and in Business</strong></a>

Enough about that, let's discuss some of the clustering algorithms. The most popular clustering algorithms are: <strong><em>K Means clustering</em></strong> and <em><strong>Hierarchical clustering.</strong></em>

<strong><em>K</em> Means Clustering</strong>

<em>K</em> means is an example of a centroid model, which is an iterative clustering algorithm where the idea of similarity is defined by the closeness of the data point to the center/ centroid of the  cluster. The goal in <em>K</em> means clustering is to take some points lets say, <em>n</em> points and then partition them into <em>k</em> clusters. Where k is any number of groups you'd like. Each group is defined by a point in the center which is known as the mean.

Since humans are visual beings, I'll try to explain how the K- means clustering algorithm works using diagrams:

i) First, K means clustering demands that you specify the number of clusters you'd like to assign to the data points. In this case, I'll use 3(<em>k=3</em>).

ii) The algorithm then plants 3 flags (the shapes in blue) or<em> means </em>within the data randomly.

[gallery ids="1383,1384" type="rectangular"]

iii)Clusters are then created by associating every observation with the nearest mean. Partitions are created representing a<strong> <a href="https://en.wikipedia.org/wiki/Voronoi_diagram" target="_blank" rel="noopener">voronoi diagram</a></strong> which was generated by the <em>means</em>. Voronoi diagrams Indicate the areas that are closer to one cluster center/ mean than any other.

[gallery ids="1385,1382" type="rectangular"]

iv) Steps ii and iii are then repeated until we reach the global optima.

Now that you're done clustering. The next step would be trying to understand what those clusters mean.

<strong>Hierarchical clustering</strong>

Hierarchical clustering is an example of a connectivity model where the data points that are closer in data space, exhibit more similarity than those that are farther away.

Hierarchical clustering follows two approaches:

Agglomerative approach - This is a "bottom-up" approach where the data/ population is classified into different clusters first and aggregating them as the distance decreases or as one moves up the hierarchy.

Divisive approach - This is a "top-down approach" where the data or population is classified as one single cluster and then partitioned recursively and divided into smaller clusters as the distance decreases or as one moves down the hierarchy.

[caption id="attachment_1396" align="alignnone" width="518"]<img class="alignnone size-full wp-image-1396" src="http://categitau.com/wp-content/uploads/2017/10/capture.png" alt="Capture" width="518" height="409" /> Dendrogram[/caption]

A good example of hierarchical clustering might be the standard plant taxonomy where the plants are first classified by family then genus, species etc.

in R there's a function <strong><em>hclust()</em></strong> that takes an input as a distance matrix, which records the distances between all pairs of the points in the data then returns a <strong><em>dendrogram. </em></strong>Remember me talking about some kind of dendrogram in this <a href="https://categitau.wordpress.com/2017/03/28/ntw2017-day-1/">post</a>? Well, now I have a good understanding of what it's all about! :-)
<h3><strong>Conclusion</strong></h3>
In this post, I have given and introduction on the two most popular clustering algorithms that a data scientist should know. One of the soft skills that a data scientist should have is being able to communicate hard things using simple words that people can understand. This is one of the skills that I thought I'd practice on in this post, which explains why there wasn't any code. Next time I'll try to do a post on some Structured Machine learning algorithms as well.

Any feedback is welcome.

Till next time !
<h3><strong>References</strong></h3>
<em><strong>Machine Learning in R By Brett lantz</strong> - </em><em><a href="https://www.packtpub.com/big-data-and-business-intelligence/machine-learning-r-second-edition" target="_blank" rel="noopener">https://www.packtpub.com/big-data-and-business-intelligence/machine-learning-r-second-edition</a></em>

<strong><em>Wikipedia</em></strong> -<a href="https://en.wikipedia.org/wiki/Machine_learning">https://en.wikipedia.org/wiki/Machine_learning</a>

<a href="https://en.wikipedia.org/wiki/K-means_clustering" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/K-means_clustering</a>

<a href="https://en.wikipedia.org/wiki/Hierarchical_clustering" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Hierarchical_clustering</a>

<em><strong>Practical data science in R by Nina-Zumel and John Mount-</strong></em><a href="https://www.amazon.com/Practical-Data-Science-Nina-Zumel/dp/1617291560" target="_blank" rel="noopener">https://www.amazon.com/Practical-Data-Science-Nina-Zumel/dp/1617291560</a>

<em><strong>An Introduction to Clustering and different methods of clustering -</strong></em><a href="https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/">https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/</a>

<em><strong>Data Smart: using Data Science to transform information into insight By John W.Foreman</strong></em> -<a href="https://www.amazon.com/Data-Smart-Science-Transform-Information/dp/111866146X" target="_blank" rel="noopener">https://www.amazon.com/Data-Smart-Science-Transform-Information/dp/111866146X</a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1115</wp:post_id>
		<wp:post_date><![CDATA[2017-10-23 17:07:28]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-10-23 14:07:28]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[finding-groups-in-data-clustering-techniques]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="data-science"><![CDATA[Data Science]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[geo_public]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[1472]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_rest_api_published]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_rest_api_client_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[-1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_publicize_job_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[10641578513]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_publicize_done_external]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:7:"twitter";a:1:{i:18590462;s:56:"https://twitter.com/swtkatelyn/status/922464518420287488";}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_publicize_done_18777098]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_18590462]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[publicize_twitter_user]]></wp:meta_key>
		<wp:meta_value><![CDATA[swtkatelyn]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[publicize_linkedin_url]]></wp:meta_key>
		<wp:meta_value><![CDATA[https://www.linkedin.com/updates?discuss=&scope=481862994&stype=M&topic=6328230211900968960&type=U&a=jn9Y]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_publicize_done_18777103]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_18590466]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1583663993;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:2385;}i:1;a:1:{s:2:"id";i:1835;}i:2;a:1:{s:2:"id";i:1917;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_facebook_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_cache_timestamp]]></wp:meta_key>
		<wp:meta_value><![CDATA[431842]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_pinterest_image_url]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_totes]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_pinterest_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_twitter_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_linkedin_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_google_plus_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_total_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>GDG DevFest Nairobi 2017!</title>
		<link>http://categitau.com/gdg-devfest-nairobi-2017/</link>
		<pubDate>Fri, 24 Nov 2017 15:29:09 +0000</pubDate>
		<dc:creator><![CDATA[categitau]]></dc:creator>
		<guid isPermaLink="false">https://categitau.wordpress.com/?p=1493</guid>
		<description></description>
		<content:encoded><![CDATA[Who knew that the next time I would be attending a tech conference, I'd be attending as a speaker? Well, neither did I.

<!--more-->
<blockquote>GDG DevFests are large, community-run developer events happening around the globe focused on community building and learning about Google’s technologies.</blockquote>
GDG DevFest Nairobi 2017 was an all-day developer conference that offered speaker sessions across multiple product areas, code-labs, hackathon and much more! This years' DevFest in Nairobi, Kenya was held at Strathmore University on the 11th of November 2017.

One of the co-organizers of <em><a href="https://www.meetup.com/Nairobi-Women-in-Machine-Learning-Data-Science/?_cookie-check=pWellrN4P0g5wePP" target="_blank" rel="noopener">WIMLDS</a> (Women in Machine Learning and Data Science), </em>was seeking interested folks to do a beginner Machine Learning code-lab at DevFest Nairobi 2017 and I was happy to take up the challenge. I knew that this would be a great opportunity for me to learn, improve on my communication skills and a step towards becoming a visible expert in my field.

[caption id="attachment_1495" align="aligncenter" width="239"]<img class="alignnone  wp-image-1495" src="http://categitau.com/wp-content/uploads/2017/11/img-20171122-wa0002-e1511513514555.jpg" alt="IMG-20171122-WA0002" width="239" height="356" /> My DevFest Nairobi Speaker tag[/caption]

So, my awesome co-speaker <a href="https://towardsdatascience.com/@apondihazel">Hazel Apondi</a> and I decided to do an introductory presentation on Machine Learning together with a code-lab on sentiment analysis on movie reviews, which we thought would be an interesting topic to discuss and learn in more detail.

While preparing for the presentation I got to learn 2 main things:
<h2>   1. Why Naive though?</h2>
Do you know why the Naive Bayes algorithm is known as Naive? If not, don't worry, I got you!
<blockquote>In Machine Learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong(naive) independence assumptions between the features.

-Wikipedia</blockquote>
In other words, it assumes that the existence of a certain feature in a class does not depend on other features in the same class. A popular example: A fruit is said to be an apple if it's let's say round, red/green and 3 inches in diameter. Even if these features depend on each other or in the existence of other features, it is assumed that all these features independently contribute to the  probability of the fruit being an apple and that is why it is known as 'Naive'. This makes the classifier to perform better compared to other models and you need less training data. You can get more information on Naive Bayes <a href="https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/" target="_blank" rel="noopener">here</a>
<h2>   2. Jupyter Notebook Slides!</h2>
While getting ready for the code-lab and trying to find out the best way to present the code, I came across Jupyter notebook slides! This great program is useful especially for anyone interested in presenting code. I've been using Jupyter notebook for a while now but I was unaware that it could do so much more than just writing code. You can check out a post on medium on how to Present code using Jupyter notebook <a href="https://medium.com/@mjspeck/presenting-code-using-jupyter-notebook-slides-a8a3c3b59d67" target="_blank" rel="noopener">here</a>.

Apart from those two, I also learnt how to do sentiment analysis using the Naive Bayes Classifier, discovered how cool NLTK toolkit is, among many other things. Hoping to be a part of more conferences like this.

You can find our presentations on the links below:
<ul>
	<li>Power point presentation by Hazel Apondi: <a href="https://docs.google.com/presentation/d/1s-FM88iW6f3ZJQNGH2Gd4ziDa2xMjO75YzusVyQa4xQ/edit#slide=id.p" target="_blank" rel="noopener">Machine Learning Demystified</a></li>
	<li>Code-lab by Catherine Gitau:  <a href="http://nbviewer.jupyter.org/gist/CateGitau/8a5421ccfac01979208a58cce392bd68" target="_blank" rel="noopener">Sentiment Anlaysis Code-lab</a></li>
</ul>
&nbsp;

[gallery ids="1499,1498" type="rectangular"]

[caption id="attachment_1497" align="aligncenter" width="315"]<img class="alignnone  wp-image-1497" src="http://categitau.com/wp-content/uploads/2017/11/der_0843.jpg" alt="DER_0843" width="315" height="210" /> Presenting a code-lab on sentiment analysis[/caption]

To more Learning!

cheers.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1493</wp:post_id>
		<wp:post_date><![CDATA[2017-11-24 15:29:09]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-11-24 12:29:09]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[gdg-devfest-nairobi-2017]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="data-science"><![CDATA[Data Science]]></category>
		<category domain="category" nicename="events-meetups"><![CDATA[Events &amp; Meetups]]></category>
		<category domain="category" nicename="machine-learning"><![CDATA[Machine Learning]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_publicize_done_external]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:7:"twitter";a:1:{i:18590462;s:56:"https://twitter.com/swtkatelyn/status/934036193737674752";}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_rest_api_published]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_rest_api_client_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[-1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_publicize_job_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[11783513221]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_publicize_done_18777098]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_18590462]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[publicize_twitter_user]]></wp:meta_key>
		<wp:meta_value><![CDATA[swtkatelyn]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[publicize_linkedin_url]]></wp:meta_key>
		<wp:meta_value><![CDATA[https://www.linkedin.com/updates?discuss=&scope=481862994&stype=M&topic=6339801887860232192&type=U&a=c0CV]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_publicize_done_18777103]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_18590466]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1583381382;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:1719;}i:1;a:1:{s:2:"id";i:141;}i:2;a:1:{s:2:"id";i:1585;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_totes]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_pinterest_image_url]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_cache_timestamp]]></wp:meta_key>
		<wp:meta_value><![CDATA[430787]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_facebook_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>A glimpse into 2018!</title>
		<link>http://categitau.com/a-glimpse-into-2018/</link>
		<pubDate>Wed, 10 Jan 2018 14:38:48 +0000</pubDate>
		<dc:creator><![CDATA[categitau]]></dc:creator>
		<guid isPermaLink="false">http://categitau.com/?p=1585</guid>
		<description></description>
		<content:encoded><![CDATA[First off, I'd like to wish you all a Happy New Year!

On that note, what's a new year post without setting some new year <del datetime="2018-01-08T11:13:06+00:00">resolutions</del> goals?<!--more-->

I'm always skeptical about sharing my goals with people because sometimes nothing gets really done, and by sharing them, I feel like it sometimes changes the purpose from focusing on the process to achieve the goal to trying to justify to my friends that I can actually keep to my goals and attain them and if I don't, then I'm a sucker and that's not the case. Also, I love the feeling that comes with sharing an achievement with people who weren't expecting it.

It took me a while to decide what my first post this year will be about and I hate myself for conforming to this "new year goals" trend, but what the heck.

This short post will outline not goals but some of the things I'm looking forward to regarding data science. Which basically are:
<ul>
 	<li><strong>Learning something new</strong></li>
</ul>
In this world of data science, you just can't stop learning. I literally come across something new every single day. But what I mean is learning a totally different thing from what I've been up to in the last couple of months. Maybe get my hands dirty and learn me some <a href="https://www.tensorflow.org/">Tensor flow</a>? Neural Nets? BI? Apache Spark? We'll see. Really excited about this.
<ul>
 	<li><strong>Contributing to others' data science paths</strong></li>
</ul>
This includes writing content that helps people out there who are starting out on data science and I am also excited to be a part of <a href="https://www.meetup.com/Nairobi-Women-in-Machine-Learning-Data-Science/">WiMLDS</a> community here in Nairobi, Kenya. Hoping to meet you at one of the meetups!
<ul>
 	<li><strong>Finish a Kaggle competition</strong></li>
</ul>
Ever since I joined Kaggle, I've always left competitions unfinished. I've never even finished the beginner titanic one, but I learnt a lot on data exploration from it. So, this year I'll work towards completing at least one. Yay to that!
<ul>
 	<li><strong>More and more conferences</strong></li>
</ul>
Last but not the least, I am looking forward to attending as many conferences as I possibly can this year. This way, I can get to share ideas and experiences with more people who are members of the Data science community, build my network and possibly attain new skills.

Let's see what 2018 has in store for us!

&nbsp;

&nbsp;

&nbsp;

&nbsp;]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1585</wp:post_id>
		<wp:post_date><![CDATA[2018-01-10 14:38:48]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-01-10 11:38:48]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[a-glimpse-into-2018]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[1597]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1583381589;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:28;}i:1;a:1:{s:2:"id";i:1759;}i:2;a:1:{s:2:"id";i:1115;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_cache_timestamp]]></wp:meta_key>
		<wp:meta_value><![CDATA[430760]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_facebook_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_pinterest_image_url]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_totes]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Feature Engineering on text data using R</title>
		<link>http://categitau.com/feature-engineering-on-text-data-using-r/</link>
		<pubDate>Sun, 28 Jan 2018 21:44:18 +0000</pubDate>
		<dc:creator><![CDATA[categitau]]></dc:creator>
		<guid isPermaLink="false">http://categitau.com/?p=1602</guid>
		<description></description>
		<content:encoded><![CDATA[The ability to work with text data is one that I have been able to hone in the past couple of months. With the vast amount of unstructured data that's being generated every day due to the rise of the use of social media platforms, forums, review sites and web pages, this is a skill that every data scientist should possess.<!--more-->

You'll find that most companies own tonnes of text data which has either been crawled from the Internet, extracted from social media sites, forums and so on depending on the company's aim. This data is usually pretty messy but it holds a lot of information beneath that can help companies gain insights which can be used to boost their businesses. This is where <strong>Natural Language Processing and Feature Engineering</strong> come in.

<strong>Natural Language Processing</strong> or <strong>NLP</strong> in short, is a field of study that focuses on the interaction of human Language and Computers. It consists of processes that are used in analyzing, understanding and extracting useful information from text data in an efficient way.

<strong>Feature Engineering </strong> is the process of extracting useful information/ features from raw data. In this case, text data.

This post will highlight a technique that can be used to extract features from text. Which is:<strong> Named Entity Recognition</strong><strong>, </strong>using R programming language.
<h4><strong>Named Entity Recognition</strong></h4>
Entities are defined as the most important parts of a sentence. Named Entity Recognition is the process of detecting these entities which include location of a place, a person, organizations e.t.c from a string of text.

Example:

<strong>Sentence</strong> - "<em>My name is Catherine Gitau, I work at Ongair Limited in Nairobi Kenya</em>."

<strong>Named Entities</strong> -<em>("Person":"Catherine Gitau"),("Organization": "Ongair Limited"),("Location": "Nairobi Kenya")</em>

A typical NER model basically goes through the following process:
<h4> 1. Tokenization</h4>
This is breaking out texts into units with meaning known as Tokens. In this case, the sentence is broken down into words. If we had a paragraph, we could've broken it down into both sentences and words. This is done by the use of word annotators. Annotators are created by functions which mark the places in the string where words and sentences start and end.

[sourcecode language="r"]
#create annotators for words and sentences

word_ann &lt;- Maxent_Word_Token_Annotator()
sent_ann &lt;- Maxent_Sent_Token_Annotator()
[/sourcecode]

&nbsp;
<h4>2. Identifying and extracting Noun Phrases</h4>
This deals with extracting noun phrases from a text(Entities). Among the various kinds of annotators provided by the package <a href="https://www.rdocumentation.org/packages/openNLP/versions/0.0-7">OpenNLP</a>, is the entity annotator which extracts the various nouns/ names from a document. OpenNLP can find various entities such as times, locations, organizations, persons, percentages e.t.c.

These annotator functions are created the same way the word and sentence annotators were created:

[sourcecode language="r"]

#creates annotators of kind person, location and organization

person_ann &lt;- Maxent_Entity_Annotator(kind = &quot;person&quot;)
location_ann &lt;- Maxent_Entity_Annotator(kind = &quot;location&quot;)
organization_ann &lt;- Maxent_Entity_Annotator(kind = &quot;organization&quot;)
[/sourcecode]

&nbsp;

&nbsp;

This is just an overview of how Named Entity recognition is done. You can get the rest of the code <a href="https://gist.github.com/CateGitau/3eac49225636ffdd7cc9268f4f1c94c6">here</a>.

Apart from NER, there are other various features that can be extracted from text such as:

<strong>i) Parts of Speech</strong> - Every word in a sentence is associated with a part of speech tag such as verbs, nouns, pronouns,adverbs etc. This defines the usage of a word in a sentence.

i<strong>i) N-grams</strong> -  This is the combination of (N) words together. It is a more informative approach than using Unigrams. I'm more familiar with <a href="https://en.wikipedia.org/wiki/Bigram">Bigrams</a> which are the combination of two words in a corpus. <a href="https://gist.github.com/CateGitau/35fe4406005ea7d738c8c4cf02070f71">Here's</a> a code on how to extract Bigrams from a sentence. These may come in handy in speech recognition, Identifying Language and much more.

There are many other features that can be extracted from text other than the ones that I have mentioned above.

To read more about NLP and other features, you can check out the following links :

Analytics Vidhya, Ultimate Guide to understand and implement Natural Language Processing codes in Python -<em><a href="https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/">https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/</a></em>

RPubs, Natural Language Processing - <em><a href="https://rpubs.com/lmullen/nlp-chapter">https://rpubs.com/lmullen/nlp-chapter</a></em>

<strong>Future Mini Project:</strong> Find out which areas in Nairobi, Kenya have a high frequency of accidents occurring. You can extract data from social media platforms then perform the PoS tagging to identify words such as "accidents", "collide" e.t.c, then perform NER to extract locations mentioned. After this, you can now get the frequency of the locations mentioned. This is a project that is ongoing but is giving me quite a headache. It would be awesome to have some people trying it out though, then compare results. Ping me if you manage to do this :)

Till next time :)]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1602</wp:post_id>
		<wp:post_date><![CDATA[2018-01-28 21:44:18]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-01-28 18:44:18]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[feature-engineering-on-text-data-using-r]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="data-science"><![CDATA[Data Science]]></category>
		<category domain="category" nicename="machine-learning"><![CDATA[Machine Learning]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_9c8ad0959c0eadd2331507324d87c32f]]></wp:meta_key>
		<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_ab5743c3eca874af2a7c9cb621c25c19]]></wp:meta_key>
		<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1583578710;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:865;}i:1;a:1:{s:2:"id";i:1851;}i:2;a:1:{s:2:"id";i:1759;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[1642]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_pinterest_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_twitter_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_facebook_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_cache_timestamp]]></wp:meta_key>
		<wp:meta_value><![CDATA[431866]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_pinterest_image_url]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_totes]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_linkedin_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_google_plus_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_total_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Using rvest to scrape data from Wikipedia</title>
		<link>http://categitau.com/using-rvest-to-scrape-data-from-wikipedia/</link>
		<pubDate>Wed, 14 Feb 2018 16:39:55 +0000</pubDate>
		<dc:creator><![CDATA[categitau]]></dc:creator>
		<guid isPermaLink="false">http://categitau.com/?p=1663</guid>
		<description></description>
		<content:encoded><![CDATA[&nbsp;
<blockquote><strong>Web scraping</strong>, <strong>web harvesting</strong> or <strong>web data extraction</strong> is <a href="https://en.wikipedia.org/wiki/Data_scraping">data scraping</a> used for extracting data from websites.

<em>-Wikipedia</em></blockquote>
A couple of days ago, I was looking for project ideas on <a href="https://medium.com/"><strong>medium</strong></a> and I remembered having stumbled upon this <a href="https://medium.com/@jasonkgoodman/advice-on-building-data-portfolio-projects-c5f96d8a0627"> post</a>  sometime back which gives advice on <em>building data portfolio projects</em>. At the end of the post, the author pitched a project idea on finding out the divorce rates of actors and actresses on Wikipedia. I decided to take up the challenge and see if I can actually scrape the biography pages of actors and actresses on Wikipedia and get any interesting insights then finally build a model around it.

This post will highlight how I got to scraping out this data using R's package <strong>rvest. rvest</strong> is an R package that makes it easy for us to scrape data from the web. So, brace yourselves, technical post ahead!
<h4>1. Getting Started</h4>
<h5>Pre-requisites:</h5>
<ul>
 	<li>To get started with web scraping in R you'll obviously need some working knowledge of R programming language.</li>
 	<li>Throughout this post/tutorial we'll be working with the rvest package which you can install using the following code:</li>
</ul>

[sourcecode language="r"]

install.packages(&quot;rvest&quot;)

[/sourcecode]

<ul>
 	<li>Some knowledge of HTML and CSS will also be an added advantage. If you don't have any knowledge on HTML and CSS, worry not, you can use an opensource software known as Selector Gadget. You can simply access it by downloading the Selector Gadget extension from <a href="http://selectorgadget.com/">this</a> website . Using the extension you can select the parts of any website and get the relevant tags by simply clicking on the part of the website you'd like to scrape out.</li>
 	<li>Finally, you also need Google chrome.</li>
</ul>
Awesome! Now, let's get started on scraping Wikipedia:
<h4>2. Scraping Wikipedia using R</h4>
<ul>
 	<li>
<h5>Step 1</h5>
</li>
</ul>
After searching through <a href="https://www.wikipedia.org/">Wikipedia's website</a>, I came across <a href="https://en.wikipedia.org/wiki/List_of_American_film_actresses">this</a> page that has a list of around 1500 american actresses and links to their Wikipedia pages where you can access more information about them.

[caption id="attachment_1667" align="alignnone" width="676"]<img class="wp-image-1667 size-large" src="http://categitau.com/wp-content/uploads/2018/02/Capture-700x163.png" alt="" width="676" height="157" /> List of American actresses[/caption]

below is a screenshot of Jennifer Aniston's Wikipedia page that we're going to scrape. The highlighted part is the part we need in this case. It contains information about the actor's place and date of birth, occupation e.t.c.

[caption id="attachment_1669" align="alignnone" width="1187"]<img class="wp-image-1669 size-full" src="http://categitau.com/wp-content/uploads/2018/02/Screenshot-14.png" alt="" width="1187" height="668" /> Highlighted Wikipedia page using Selector Gadget[/caption]

I've actually highlighted that part using <a href="https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb?hl=en">Selector Gadget</a>, which helps in getting the specific CSS selectors that we want to use so as to scrape the page. From the screenshot above, at the bottom middle you can see that our CSS selector for the highlighted biography table is <em><strong>".vcard" </strong></em>which we are going to need later<em><strong>.</strong></em> You could also use Google Chrome's developer tools to look at the HTML behind the biography table as shown highlighted in the screenshot below and copy the CSS selector by right clicking on the html code.

<img class="alignnone wp-image-1678 size-full" src="http://categitau.com/wp-content/uploads/2018/02/Screenshot-16.jpg" alt="" width="1219" height="479" />

&nbsp;
<ul>
 	<li>
<h5>Step 2</h5>
</li>
</ul>
To read the web page into R we will need the rvest package and also the <a href="https://cran.r-project.org/web/packages/magrittr/index.html">magrittr</a> package which uses the operator <strong>%&lt;%</strong> that takes the output from the left and uses it as the first argument of input on the right.

[sourcecode langauge="r"]

#load in the Rvest and magrittr package

library(rvest)

library(magrittr)

[/sourcecode]

The function we're going to use first is the <em><strong>read_html()</strong></em> which is used in reading HTML pages. You do this using the following code:

[sourcecode language="r"]
#read HTML code from the website
webpage &lt;- read_html(&quot;https://en.wikipedia.org/wiki/Jennifer_Aniston&quot;)
[/sourcecode]

Next, since we have already identified our CSS selector <em><strong>"v.card"</strong></em>, we use the following code to extract the information
in the biography table.

[sourcecode language="r"]
table &lt;- webpage %&lt;%
html_nodes(&quot;table.vcard&quot;) %&lt;%
html_table(header=F)
table &lt;- table[[1]]

#add the table to a dataframe
dict &lt;- as.data.frame(table)

[/sourcecode]

We then come up with the table below:
<img class="alignnone wp-image-1674 size-full" src="http://categitau.com/wp-content/uploads/2018/02/Capture-1.png" alt="" width="515" height="236" />

Easy right?
<h5>Conclusion</h5>
So, you've just learnt how to scrape a html table from a web page using R. A lot goes into the code when scraping each bio table from the list of actresses. You can access the code and data I extracted <a href="https://github.com/CateGitau/Web-Scraping-in-R">here</a>.
<h5>References</h5>
Analytics Vidhya, Beginner's guide on web scraping - <a href="https://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/">https://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/</a>

&nbsp;

Till next time :)

&nbsp;]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1663</wp:post_id>
		<wp:post_date><![CDATA[2018-02-14 16:39:55]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-02-14 13:39:55]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[using-rvest-to-scrape-data-from-wikipedia]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1583704749;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:668;}i:1;a:1:{s:2:"id";i:1692;}i:2;a:1:{s:2:"id";i:1759;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[1677]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_832785e9c969ea5221271fcae0e57827]]></wp:meta_key>
		<wp:meta_value><![CDATA[<blockquote class="wp-embedded-content" data-secret="J4OZKsvodN"><a href="https://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/">Beginner’s Guide on Web Scraping in R (using rvest) with hands-on example</a></blockquote><iframe class="wp-embedded-content" sandbox="allow-scripts" security="restricted" style="position: absolute; clip: rect(1px, 1px, 1px, 1px);" src="https://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/embed/#?secret=J4OZKsvodN" data-secret="J4OZKsvodN" width="600" height="338" title="&#8220;Beginner’s Guide on Web Scraping in R (using rvest) with hands-on example&#8221; &#8212; Analytics Vidhya" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_832785e9c969ea5221271fcae0e57827]]></wp:meta_key>
		<wp:meta_value><![CDATA[1518608084]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_facebook_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_pinterest_image_url]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_cache_timestamp]]></wp:meta_key>
		<wp:meta_value><![CDATA[431868]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_totes]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_pinterest_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_twitter_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_linkedin_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_google_plus_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_total_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Fuzzy String Matching in Python</title>
		<link>http://categitau.com/fuzzy-string-matching-in-python/</link>
		<pubDate>Wed, 28 Feb 2018 10:17:58 +0000</pubDate>
		<dc:creator><![CDATA[categitau]]></dc:creator>
		<guid isPermaLink="false">http://categitau.com/?p=1692</guid>
		<description></description>
		<content:encoded><![CDATA[As a data scientist, you are forced to retrieve information from various sources by either leveraging publicly available API's, asking for data, or by simply scraping your own data from a web page. All this information is useful if we are able to combine it and not have any duplicates in the data. But how do we make sure that there are no duplicates?

<!--more-->

I know ... <em>"duh! you can just use a function that retrieves all the unique </em><em>information thus removing duplicates"</em>. Well, that's one way, but our function probably can't tell that a name like<em> "Barack Obama"</em> is the same as<em> "Barack H. Obama" </em>right? (Assuming we were retrieving names of the most famous people in the world). We can clearly tell that these names are different but they are probably referring to the same person. So, how do we match these names?

This is where Fuzzy String Matching comes in. This post will explain what Fuzzy String Matching is together with its use cases and give examples using Python's Library <a href="https://pypi.python.org/pypi/fuzzywuzzy"><em>Fuzzywuzzy</em></a>.
<h4><b>Fuzzy Logic</b></h4>
<blockquote><strong>Fuzzy(<em>adjective</em>)</strong>: difficult to perceive; indistinct or vague

-Wikipedia</blockquote>
<a href="https://en.wikipedia.org/wiki/Fuzzy_logic" target="_blank" rel="noopener">Fuzzy logic</a> is a form of multi-valued logic that deals with reasoning that is approximate rather than fixed and exact. Fuzzy logic values range between 1 and 0. i.e the value may range from completely true to completely false. In contrast, <em><strong>Boolean Logic</strong></em> is a two-valued logic: true or false usually denoted 1 and 0 respectively, that deals with reasoning that is fixed and exact. Fuzzy logic tends to reflect how people think and attempts to model our decision making hence it is now leading to new intelligent systems(expert systems).

So, if we are comparing two strings using fuzzy logic, we would be trying to answer the question <em>"How similar are string A and string B?", </em>and rephrasing it as <em>"Are string A and String B the same?"</em> when using the Boolean Logic.
<h4><strong>Fuzzy String Matching</strong></h4>
<a href="https://en.wikipedia.org/wiki/Approximate_string_matching" target="_blank" rel="noopener">Fuzzy String Matching</a>, also known as Approximate String Matching, is the process of finding strings that approximately match a pattern. The process has various applications such as <em>spell-checking</em>, <em>DNA analysis and detection,</em> spam detection, <em>plagiarism detection e.t.c</em>
<h5>Introduction to <em>Fuzzywuzzy</em> in Python</h5>
<strong>Fuzzywuzzy</strong> is a python library that uses <strong>Levenshtein Distance</strong> to calculate the differences between sequences and patterns that was developed and also open-sourced by <a href="https://seatgeek.com/" target="_blank" rel="noopener">SeatGeek,</a> a service that finds events from all over the internet and showcase them on one platform. The big problem they were facing was the labeling of the same events as stated on their <a href="http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/" target="_blank" rel="noopener">blog</a>. This is the same as the example I gave at the beginning of the post where an entity such as a person's name can be labelled differently on different sources.
<h5>Installation</h5>
To install the library, you can use pip:

[sourcecode language="python"]

pip install fuzzywuzzy

pip install python-Levenshtein

[/sourcecode]

<h5>Examples</h5>
First we have to import the fuzzywuzzy modules:

[sourcecode language="python"]

from fuzzywuzzy import fuzz
from fuzzywuzzy import process

[/sourcecode]

Now, we can get the similarity score of two strings by using the following methods; ratio() or partial_ratio():

[sourcecode language="python"]

fuzz.ratio(&quot;Catherine M Gitau&quot;,&quot;Catherine Gitau&quot;)

#91

fuzz.partial_ratio(&quot;Catherine M. Gitau&quot;,&quot;Catherine Gitau&quot;)

#100

[/sourcecode]

You're probably wondering why the scores are different. This is because the fuzz.ratio() method just calculates the edit distance between some ordering of the token in both input strings using the <code>difflib.ratio.</code> You can find out more about the difflib.ratio <a href="https://docs.python.org/2/library/difflib.html#difflib.SequenceMatcher.ratio" target="_blank" rel="noopener">here</a>. The<strong> <em>fuzz.partial_ratio()</em></strong> takes in the shortest string, which in this case is "Catherine Gitau" (length 14) , then matches it with all the sub-strings of length(14) in "Catherine M. Gitau" which means matching with "Catherine Gitau" which gives 100%. You can play around with the strings until you get the gist.

What if we switched up two names in one string? In the following example, I've interchanged the name "Catherine Gitau" to "Gitau Catherine" .Let's see the scores:

[sourcecode language="python"]

fuzz.ratio(&quot;Catherine M Gitau&quot;,&quot;Gitau Catherine&quot;)

#55

fuzz.partial_ratio(&quot;Catherine M. Gitau&quot;,&quot;Gitau Catherine&quot;)

#60

[/sourcecode]

We see that both methods are giving out low scores, this can be rectified by using <strong><em>token_sort_ratio()</em></strong> method. This method attempts to account for similar strings that are out of order. Example, if we used the above strings:

[sourcecode language="python"]

fuzz.token_sort_ratio(&quot;Catherine Gitau M.&quot;, &quot;Gitau Catherine&quot;)

#94

[/sourcecode]

As you can see, we get a high score of 94.
<h4><strong>Conclusion</strong></h4>
This article has introduced Fuzzy String Matching which is a well known problem that is built on Leivenshtein Distance. From what we have seen, it calculates how similar two strings are. This can also be calculated by finding out the number of operations needed to transform one string to the other .e.g with the name "Barack", one might spell it as "Barac". Only one operation is needed to correct this i.e adding a K at the end. You can try this out using the <em>stringdist</em> library in <strong>r</strong> as such:

[sourcecode langauge="r"]

adist(&quot;Barack&quot;, &quot;Barac&quot;)
#[1]

[/sourcecode]

<h4><strong>Sources</strong></h4>
https://marcobonzanini.com/2015/02/25/fuzzy-string-matching-in-python/

&nbsp;

Till next time:)

&nbsp;]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1692</wp:post_id>
		<wp:post_date><![CDATA[2018-02-28 10:17:58]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-02-28 07:17:58]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[fuzzy-string-matching-in-python]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="data-science"><![CDATA[Data Science]]></category>
		<category domain="category" nicename="python"><![CDATA[Python]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[1743]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_29f597a9179d7465fd59b13e39104492]]></wp:meta_key>
		<wp:meta_value><![CDATA[<a href="https://marcobonzanini.com/2015/02/25/fuzzy-string-matching-in-python/">Fuzzy String Matching in&nbsp;Python</a>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_29f597a9179d7465fd59b13e39104492]]></wp:meta_key>
		<wp:meta_value><![CDATA[1521050679]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_962b91b00e0eca1fc4e684196931ecbb]]></wp:meta_key>
		<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1583472085;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:38;}i:1;a:1:{s:2:"id";i:668;}i:2;a:1:{s:2:"id";i:1602;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_linkedin_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_pinterest_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_twitter_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_totes]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_pinterest_image_url]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_cache_timestamp]]></wp:meta_key>
		<wp:meta_value><![CDATA[431882]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_facebook_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_2e26a10348c77a0c3ac5076d1de2a19f]]></wp:meta_key>
		<wp:meta_value><![CDATA[<a href="https://marcobonzanini.com/2015/02/25/fuzzy-string-matching-in-python/">Fuzzy String Matching in&nbsp;Python</a>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_total_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_google_plus_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_f49685d4973bb5cc242c341338f4b644]]></wp:meta_key>
		<wp:meta_value><![CDATA[<a href="https://marcobonzanini.com/2015/02/25/fuzzy-string-matching-in-python/">Fuzzy String Matching in&nbsp;Python</a>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_f49685d4973bb5cc242c341338f4b644]]></wp:meta_key>
		<wp:meta_value><![CDATA[1583507227]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_2e26a10348c77a0c3ac5076d1de2a19f]]></wp:meta_key>
		<wp:meta_value><![CDATA[1554778092]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_dd690d1c053de1648e0b652b6e11baca]]></wp:meta_key>
		<wp:meta_value><![CDATA[<a href="https://marcobonzanini.com/2015/02/25/fuzzy-string-matching-in-python/">Fuzzy String Matching in&nbsp;Python</a>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_dd690d1c053de1648e0b652b6e11baca]]></wp:meta_key>
		<wp:meta_value><![CDATA[1554794278]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_c9bdcaff11b19d7473e8da16b49feba8]]></wp:meta_key>
		<wp:meta_value><![CDATA[<a href="https://marcobonzanini.com/2015/02/25/fuzzy-string-matching-in-python/">Fuzzy String Matching in&nbsp;Python</a>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_c9bdcaff11b19d7473e8da16b49feba8]]></wp:meta_key>
		<wp:meta_value><![CDATA[1560067031]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Recap: Nairobi WiDS Conference 2018</title>
		<link>http://categitau.com/wids-conference-2018/</link>
		<pubDate>Wed, 14 Mar 2018 21:52:26 +0000</pubDate>
		<dc:creator><![CDATA[categitau]]></dc:creator>
		<guid isPermaLink="false">http://categitau.com/?p=1719</guid>
		<description></description>
		<content:encoded><![CDATA[<strong>Happy belated Women's day!</strong>

<strong>tl;dr</strong>

As people celebrated women all over the world last week, I decided to spend the day with incredible women in the tech industry especially those in the field of Machine Learning and Data Science at the <strong><a href="https://medium.com/@muthoniwanyoike/were-the-2018-nairobi-ambassadors-of-the-stanford-women-in-data-science-conference-24027973a9bb">Nairobi Women in Data Science Conference</a>.</strong><!--more-->

The <a href="http://www.widsconference.org/about2018.html">Women in Data Science (WiDS) Conference</a> is an annual global conference that is aimed at inspiring and educating data scientists all over the world regardless of gender and support the women in the field. This year's global WiDS conference was held at Stanford University together with over 100 regional events all over the world where of course, included Nairobi.

The conference was hosted by the <a href="http://hub.africabiosciences.org/">Biosciences eastern and central Africa hub</a> at the International Livestock research Institute (ILRI) here in Nairobi and its aim was to give attendees the opportunity to learn, network and explore the avenues for collaboration and partnerships.

[caption id="attachment_1721" align="aligncenter" width="676"]<img class="wp-image-1721 size-large" src="http://categitau.com/wp-content/uploads/2018/03/DX5FidEVQAAE7iF-700x463.jpeg" alt="" width="676" height="447" /> Attendees of Nairobi Women in Data Science Conference at ILRI[/caption]

Nairobi WiDS conference kicked off at 9:00 am with a Keynote address from <strong>Dr. Aisha Walcott-Bryant</strong> who is a research scientist and manager at IBM Research Africa - Nairobi, Kenya. Dr. Aisha went ahead and discussed how IBM Research Africa is focusing on leveraging data to reduce traffic in Nairobi.

We all know that this is one of the major challenges in our cities due to the rapid urbanization over the past years. This challenge according to Dr. Aisha tends to be difficult to tackle mostly due to lack of transportation data and of course the traffic in our cities especially in Nairobi is really hard to predict (I relate, coming from someone who travels from Thika to Nairobi almost daily!)

So, for this reason, IBM's solution was to employ smartphones with sensors that are mounted on waste collection vehicles to gather real-time data about the condition of Nairobi's streets and the location of traffic delays due to the existence of potholes, speed-bumps, flooding and other obstructions that are on the road.

This reminded me of an ongoing project <a href="https://whatisaroad.crowdmap.com/">#whatisaroad</a> that maps out potholes in Nairobi using information they get from Twitter. You can also check out this <a href="https://blackorwa.com/2016/09/06/a-theory-on-formation-of-potholes-in-nairobi/">post</a> by <strong>Chris Orwa</strong> to find out what exactly causes the formation of potholes in Nairobi.

I think what IBM Research is doing is an awesome step towards reducing traffic in Nairobi considering the fact that there are various other factors that tend to also influence traffic such as the weather, the quality of the road, accidents and also the time of day. Someone should also take the initiative to find out what exactly causes traffic when it rains. That has always been something that I am yet to understand.

Going on, Dr.Aisha also pointed out some <em><strong>Cautions of Data Science </strong></em>which included:
<ul>
 	<li><strong>AI programs excluding African American Voices -</strong> For those using Siri or Alexa, I guess you can relate. Sometimes these AI systems due to lack of enough training data with different examples of accents, tend to have a hard time understanding some people i.e. if you have an unusual accent.</li>
 	<li><strong>False classification of Images</strong></li>
</ul>
<strong>       <img class="alignnone wp-image-1720 size-full" src="http://categitau.com/wp-content/uploads/2018/03/ahahah.jpg" alt="" width="634" height="536" /> </strong>This photo just tells it all.
<ul>
 	<li><strong>Facial recognition - </strong> Facial recognition is becoming a common tool in the biometric identification industry. There's a ton of research that is being done around facial recognition where some people have already attempted to use AI facial recognition for purposes of identifying terrorists and potential lawbreakers.</li>
</ul>
Another point that Dr. Aisha pointed out as I conclude her session was: <em><strong>"We are the future of data science"</strong>. </em>The volume of data is increasing at a very high rate and the potential value of data to the society is increasing rapidly.<em> </em>It's our role to make good use of this data and build systems that will be of benefit to the society.

The next speaker was<strong> <a href="https://medium.com/@LinetData">Linet Kwamboka.</a> </strong>She is the CEO of Data Science Limited in Nairobi, Kenya. Her talk was on Data Protection and Privacy.
<blockquote><em>"Data is the currency you use to access free services"</em>

-<em><strong>Linet Kwamboka</strong></em></blockquote>
I liked the above statement. Wonder why Twitter, Facebook and Instagram are free? Well, there's your answer. These social media platforms use information they get from you to make tons of money. The Data Science LTD sent a survey to 200 respondents to understand whether people generally knew about data protection. The survey revealed that a lot of people know about Data Privacy and also surprisingly, more people read the terms and conditions. Funny right? I personally don't read the Terms and Conditions because, one, they are too long or two, if I don't click yes to having read and understood them, I won't be able to use the service so I might as well just skip through. You can find out what else the survey entailed from this <a href="https://medium.com/read-write-participate/the-internet-of-you-78673de72ca6">post</a>.

Another thing she talked about was <strong>Identity Theft </strong>where she mentioned coming across an Instagram photo where the same individual had cloned himself 6 times on 6 different national identity cards. You'd think that the ID numbers on those six fake identity cards are made up but from further research she found out that those numbers actually belonged to registered voters of Kenya. Surprise!

We tend to give out so much information about ourselves without even knowing. From our fingerprints (yes, you who thinks that unlocking your phone using your finger is convenient, think twice), to those forms we fill out during events, when we give out our ID's while entering a building and so much more.

You can read more about data protection on Linet's medium account which you can find <a href="https://medium.com/read-write-participate/identity-theft-is-this-really-you-bdf095857838">here</a>. Did you also know that there's a Data Protection Bill? Neither did I. You can find it <a href="http://icta.go.ke/data-protection-bill-2012/">here</a>.
<blockquote><em>"Security starts with you!"</em></blockquote>
The other Speaker that graced the occasion was <strong>Victoria Okoth</strong> from Safaricom. Her talk was on the now, before and future of analytics in Telecommunication, where she talked about how Safaricom is leveraging data and analytics to provide better and affordable services to their customers.

After that, we watched a clip from the WiDS Conference that happened in Stanford University by <strong>Maria Klawe the President of Harvey Mudd College</strong>. You can find the video <a href="https://www.youtube.com/watch?v=BJxHoo4ZYjw">here</a>. She talks about why we need women in Data Science and why women should want to be in Data Science. So, if you're in tech and still not sure whether to get into data science or not, I hope this video helps you make your decision.

The next speaker was<strong> Lorna Maria Aine</strong> who is the Data Lead at <a href="http://pollicy.org/">Pollicy</a>. Pollicy is a company in Uganda that works with governments, civil societies, NGO's and the private sector to improve and re-design services around citizen needs and demands. She went ahead and discussed how Pollicy uses data and how they collaborate with the government to improve services around their citizens.

[caption id="attachment_1728" align="alignright" width="300"]<img class="wp-image-1728 size-medium" src="http://categitau.com/wp-content/uploads/2018/03/DXxJu10XkAAwl4F-300x200.jpeg" alt="" width="300" height="200" /> Round table discussions on how data science can be used to solve challenges in Africa.[/caption]

Other than the speaker sessions, we also had a very informative panel discussion on Education/Career tracks for women in data science and finally, round table discussions where we were divided into groups to discuss some of the African challenges data science can help solve.

To conclude, I'd like to thank the <a class="markup--anchor markup--p-anchor" href="https://www.meetup.com/Nairobi-Women-in-Machine-Learning-Data-Science" target="_blank" rel="nofollow noopener" data-href="https://www.meetup.com/Nairobi-Women-in-Machine-Learning-Data-Science">Nairobi Women in Data Science &amp; Machine Learning Community</a> together with the  <a class="markup--anchor markup--p-anchor" href="http://hub.africabiosciences.org/" target="_blank" rel="nofollow noopener" data-href="http://hub.africabiosciences.org/">Biosciences eastern and central Africa hub</a> who were the Nairobi WiDS ambassadors this year for organizing this very informative and mind opening event. Also, the ILRI employees for sharing tea and cake with us as we celebrated International Women's Day.

&nbsp;

[caption id="attachment_1724" align="alignleft" width="300"]<img class="wp-image-1724 size-medium" src="http://categitau.com/wp-content/uploads/2018/03/DXw4LsxW4AAlT_Q-300x199.jpeg" alt="" width="300" height="199" /> Celebrating International Women's Day with ILRI employees[/caption]

[caption id="attachment_1723" align="alignright" width="300"]<img class="wp-image-1723 size-medium" src="http://categitau.com/wp-content/uploads/2018/03/DXw3GGnWAAEn7FD-300x199.jpeg" alt="" width="300" height="199" /> International Livestock research Institute (ILRI)[/caption]

&nbsp;

&nbsp;]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1719</wp:post_id>
		<wp:post_date><![CDATA[2018-03-14 21:52:26]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-03-14 18:52:26]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[wids-conference-2018]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="data-science"><![CDATA[Data Science]]></category>
		<category domain="category" nicename="events-meetups"><![CDATA[Events &amp; Meetups]]></category>
		<category domain="category" nicename="machine-learning"><![CDATA[Machine Learning]]></category>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_wp_old_slug]]></wp:meta_key>
		<wp:meta_value><![CDATA[recap-nairobi-wids-conference-2018]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1582881056;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:1493;}i:1;a:1:{s:2:"id";i:1585;}i:2;a:1:{s:2:"id";i:1835;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[1753]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_pinterest_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_total_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[8]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_google_plus_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_twitter_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_linkedin_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_totes]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_pinterest_image_url]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_cache_timestamp]]></wp:meta_key>
		<wp:meta_value><![CDATA[431868]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_facebook_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[8]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Data Scientist vs Data Analyst</title>
		<link>http://categitau.com/data-scientist-vs-data-analyst/</link>
		<pubDate>Thu, 29 Mar 2018 09:00:29 +0000</pubDate>
		<dc:creator><![CDATA[categitau]]></dc:creator>
		<guid isPermaLink="false">http://categitau.com/?p=1759</guid>
		<description></description>
		<content:encoded><![CDATA[<em><strong>"Data is the new oil"</strong>, </em>is a statement that's being used almost everywhere you look. Data is now being considered as the most valuable asset in the 21st century and like oil, for those who see data's essential value and learn to extract and use it, there will be huge rewards. This buzz has forced nearly all companies to start collecting data in different forms while hiring various professionals in the data industry such as data analysts, business analysts, data scientists, data engineers and so on to comb through their data and find insights that will be beneficial to the company as well as help them drive business decisions effectively.

Most people including executives at these companies have heard about data science and data analytics with little to no knowledge of what these roles actually do. These roles have been around for quite sometime now and was recently called "The Sexiest Job of the 21st Century" by the <a title="Harvard Business Review" href="https://en.wikipedia.org/wiki/Harvard_Business_Review">Harvard Business Review</a> , which has also driven people from all industries to go online in search of data science and analysis certifications. Truth be told, there's nothing sexy about data science.

So, you've heard of the job title "data analyst" or "data scientist" lately and it sounds like an intriguing career, the idea of working with data and technology has intrigued you, but what do people in these roles do? are they similar? If not, what makes them different from each other? I've come across these questions a lot. A major reason for this ambiguity is that the industry is quite new and a lot of companies have different definitions of what a data scientist or a data analyst is based on their needs and preferences.

First of all, data scientists and analysts have one common goal which is: to apply analysis in data and get insights from it. However, there are other tasks that overlap these two roles and there's this misconception that they are both the same but I tend to disagree as we'll see in this post.

<strong>Disclaimer</strong>: This post will not share the tools one needs to equip themselves to be either a data analyst or a data scientist, neither will it show the approximate salaries, but will try to define what these roles entail. I'll share some useful links that talk about the skills one needs to have be a data analyst or data scientist at the bottom of the post.
<h4>DATA ANALYST</h4>
<blockquote><strong><b>Data analysis</b></strong> is a process of inspecting, <a title="Data cleansing" href="https://en.wikipedia.org/wiki/Data_cleansing">cleansing</a>, <a title="Data transformation" href="https://en.wikipedia.org/wiki/Data_transformation">transforming</a>, and <a title="Data modeling" href="https://en.wikipedia.org/wiki/Data_modeling">modeling</a> <a title="Data" href="https://en.wikipedia.org/wiki/Data">data</a> with the goal of discovering useful information, suggesting conclusions, and supporting decision-making.

-Wikipedia</blockquote>
When I come across the term data analysis, all I can picture is reports, visualizations, dashboards and the stock market for some reason. Some common data analyst job duties involve some of those things including, spotting patterns, collecting data, communicating results of the analysis and so on.

In this medium <a href="https://medium.com/@rchang/my-two-year-journey-as-a-data-scientist-at-twitter-f0c13298aee6">post</a> Robert Chang, a former data scientist at Twitter defines a data analyst as a <strong>Type "A" Data Scientist</strong> where he says: <em>"The A is for Analysis. This type is primarily concerned with making sense of data or working with it in a fairly static way. The Type A Data Scientist is very similar to a statistician (and may be one) but knows all the practical details of working with data that aren’t taught in the statistics curriculum: data cleaning, methods for dealing with very large data sets, visualization, deep knowledge of a particular domain, writing well about data, and so on</em>.”

In a nutshell, a <strong>Data Analyst or Business Intelligence Engineer or an Analyst </strong> is a person who can do <a href="https://www.investopedia.com/terms/d/descriptive_statistics.asp">descriptive statistics</a>, visualize data and communicate data points which lead to conclusions. In other words, they use data to gain meaningful insights and solve problems. Data analysts analyze well defined data i.e. <a href="https://www.webopedia.com/TERM/S/structured_data.html">structured data</a> to answer business related questions such as <em>which product generates the most revenue, Which customers purchase the most products, what time is best to reach out to customers, why sales dropped at a certain quarter and so much more. </em>Companies in nearly every industry hire data analysts from retail shops, fast food vendors, healthcare providers and more.

From what I have  come to learn over the past couple of months, a data analyst is like a mini data scientist. Most of the time, a data scientist will find themselves doing a tone of data analyst work before building complex algorithms. If the data scientist is lucky enough to be in a company with a data team, they'll probably forgo the hustle of collecting, cleaning and pre-processing the data, which is part of what the analyst does.
<h4>DATA SCIENTIST</h4>
<blockquote><b>Data science</b>, also known as <b>data-driven science</b>, is an interdisciplinary field of scientific methods, processes, algorithms and systems to extract <a title="Knowledge" href="https://en.wikipedia.org/wiki/Knowledge">knowledge</a> or insights from <a title="Data" href="https://en.wikipedia.org/wiki/Data">data</a> in various forms, either structured or unstructured,<sup id="cite_ref-:0_1-0" class="reference"></sup><sup id="cite_ref-2" class="reference"></sup>similar to <a title="Data mining" href="https://en.wikipedia.org/wiki/Data_mining">data mining</a>.

-Wikipedia</blockquote>
Again, Robert Chang defines this kind of a data scientist as a <strong>Type "B" Data </strong><b>Scientist. </b>Where he says: <em>"The B is for Building. Type B Data Scientists share some statistical background with Type A, but they are also very strong coders and may be trained software engineers. The Type B Data Scientist is mainly interested in using data “in production.” They build models which interact with users, often serving recommendations (products, people you may know, ads, movies, search results)."</em>

<strong>Data Scientists</strong> are expected to go much deeper in terms of analysis by gathering loads of data which may be unstructured, structured or semi-structured that's been accumulated across various sources and put them in a proper dataset on which machine learning is applied to perform predictive analytics, sentiment analysis e.t.c to extract information from the data that's been collected. They tend to estimate what is unknown by asking the right questions, writing complex algorithms and building models. Some people define a data scientist as <em>a person who is better at statistics than any software engineer and better at software engineering than any statistician.</em>

A data scientist if I must say, needs to learn a lot more coding languages than a data analyst. I might be wrong about this because in the <strong><a href="https://www.wiley.com/en-us/Data+Smart%3A+Using+Data+Science+to+Transform+Information+into+Insight-p-9781118661468">Data Smart Book</a>, </strong>John W. Foreman shows how one can use spreadsheets to process raw data into actionable insights. He goes ahead to perform things like<strong> K-means clustering</strong>, building<strong> Optimization models</strong> e.t.c within the familiar environment of a spreadsheet. But I would assume that only works with a considerable amount of data.

Data science is a large field which entails very many things that are but not limited to: <em>Natural Language processing, Deep Learning, predictive analytics, building recommendation systems, sentiment analysis and unsupervised learning</em>. Looking for a job as a data scientist? I found <a href="https://towardsdatascience.com/the-two-sides-of-getting-a-job-as-a-data-scientist-a4571acc58bc">this</a> great read that will help you get started. You could also look at <a href="https://towardsdatascience.com/what-getting-a-job-in-data-science-might-look-like-f94ddb788a5e">this</a> one as well.
<h4>Conclusion</h4>
Data Analyst != Data Scientist

But a Data Scientist can also be a Data Analyst.

I would also love to get some thoughts you might have on this topic.
<h4>Related Resources</h4>
<a href="https://medium.springboard.com/the-data-science-process-the-complete-laymans-guide-to-what-a-data-scientist-actually-does-ca3e166b7c67">Data Science process the Complete Laymans guide</a> (Medium)

<a href="https://www.quora.com/What-is-a-data-scientist-3">What is a data scientist</a> (Quora)

<a href="https://towardsdatascience.com/the-two-sides-of-getting-a-job-as-a-data-scientist-a4571acc58bc">Two sides of getting a job as a data scientist</a> (Medium)

<a href="https://medium.com/@Level_Analytics/data-analytics-vs-data-science-the-breakdown-696f4c0594d4">Data analytics vs Data science</a> (Medium)

<a href="https://medium.com/indeed-data-science/theres-no-such-thing-as-a-data-scientist-8dae923c14e3">There's no such thing as a Data scientist</a> (Medium)

<a href="https://towardsdatascience.com/blurred-lines-data-analytics-vs-data-science-12ff92a3bd4e">Blurred lines: Data analyst vs Data science</a> (Towards Data Science)]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1759</wp:post_id>
		<wp:post_date><![CDATA[2018-03-29 09:00:29]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-03-29 06:00:29]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[data-scientist-vs-data-analyst]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="data-science"><![CDATA[Data Science]]></category>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_1a12e8456274d4de5e84b636fdc257d0]]></wp:meta_key>
		<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_3eaba42117dd6663616413b3b7b6a3b9]]></wp:meta_key>
		<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_c8fb54e5651b718b8ed6eaaf301f33ae]]></wp:meta_key>
		<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_f9e8de3bcd8e06aa5dc7d0384dfda195]]></wp:meta_key>
		<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[1763]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1583584389;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:1835;}i:1;a:1:{s:2:"id";i:451;}i:2;a:1:{s:2:"id";i:1719;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_mess]]></wp:meta_key>
		<wp:meta_value><![CDATA[My post on the comparison between a Data Scientist and a Data Analyst]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
		<wp:meta_value><![CDATA[@realcategitau]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_totes]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_facebook_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_pinterest_image_url]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_cache_timestamp]]></wp:meta_key>
		<wp:meta_value><![CDATA[431868]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_pinterest_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_twitter_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_linkedin_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_google_plus_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_total_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Classification in Supervised Learning - All you need to know!</title>
		<link>http://categitau.com/introduction-to-supervised-learning/</link>
		<pubDate>Fri, 13 Apr 2018 09:00:23 +0000</pubDate>
		<dc:creator><![CDATA[categitau]]></dc:creator>
		<guid isPermaLink="false">http://categitau.com/?p=1774</guid>
		<description></description>
		<content:encoded><![CDATA[In one of my previous posts, I introduced <a href="https://en.wikipedia.org/wiki/Machine_learning">Machine Learning</a> and talked about the two most common types of learning which are <strong>supervised learning</strong> and <strong>unsupervised learning</strong>. I also went ahead and explained some algorithms used in unsupervised machine learning. If you need to bethink yourself, you can find the post <a href="http://categitau.com/finding-groups-in-data-clustering-techniques/">here</a>.

<!--more-->

In this post, I'll get deeper into <a href="https://en.wikipedia.org/wiki/Supervised_learning">Supervised Learning</a> with a focus on <a href="https://en.wikipedia.org/wiki/Statistical_classification">Classification Learning(Statistical Learning)</a> which is one of the two supervised learning problems. Let's first start by reminding ourselves what supervised machine learning is.
<h3>Supervised Machine Learning</h3>
<strong>Supervised machine learning</strong> is a type of machine learning algorithm that uses a known dataset which is recognized as the training dataset to make predictions. The training dataset includes input variables (X) and response variables(Y). From these variables, a supervised learning algorithm builds a model that can make predictions of the response variables(Y) for a new dataset(testing data) that is used to check the accuracy of a model. An example of a supervised learning problem is predicting whether a customer will default in paying a loan or not. The input variables here can be details of the customer such as: airtime used, monthly salary, their credit history etc. The response variables will either be "defaulted" or "paid". With this information, the model is able to learn and given new data(test data), it is able to tell which person is likely to default on paying a loan or not.

There are various supervised learning use cases such as:
<ul>
 	<li>Predicting customer churn</li>
 	<li>Sentiment Analysis</li>
 	<li>Customer segmentation</li>
 	<li>Gene classification in bio-informatics</li>
</ul>
Supervised learning includes two categories of algorithms: <em>regression</em> and <em>classification</em> algorithms. There's a significant difference between the two:

<strong>Classification - </strong>Classification is a problem that is used to predict which class a data point is part of which is usually a discrete value. From the example I gave above, predicting whether a person is likely to default on a loan or not is an example of a classification problem since the classes we want to predict are discrete: "likely to pay a loan" and "not likely to pay a loan".<span id="__w2_hVQvriT_answer_content" class="inline_editor_value"></span>

<strong>Regression - </strong>Regression is a problem that is used to predict continuous quantity output. A continuous output variable is a real-value, such as an integer or floating point value. For example, where classification has been used to determine whether or not it will rain tomorrow, a regression algorithm will be used to predict the amount of rainfall.

The rest of this post will focus on classification. I'll dive into regression in a later post.
<h4>Classification: predicting a class/label</h4>
Classification is used to predict a <strong>discrete class or label(Y)</strong>. Classification basically involves assigning new input variables (X) to the class to which they most likely belong in based on a classification model that was built from the training data that was already labeled. Labeled data is used to train a classifier so that the algorithm performs well on data that does not have a label(not yet labeled). Repeating this process of training a classifier on already labeled data is known as "learning".

Some of the questions that a classification model helps to answer are:
<ul>
 	<li><em>Is this a picture of a cat or a dog?</em></li>
 	<li><em>Is this email Spam or not?</em></li>
 	<li><em>Is it going to rain or not?</em></li>
 	<li><em>Is this borrower going to repay their loan?</em></li>
 	<li><em>Is this post negative or positive?</em></li>
 	<li><em>What is the genre of this song/movie?</em></li>
 	<li><em>Which type of gene is this?</em></li>
</ul>
<p id="f65d" class="graf graf--p graf-after--p">Classification is again divided into three other categories or problems which are: <em><strong>Binary classification</strong>,<strong> Multi-class/Multinomial classification</strong> and <strong>Multi-label classification</strong>.</em></p>

<h5><strong>Binary classification</strong></h5>
This is a task of classifying the elements/input variables of a given set into two groups i.e predicting which of the two groups each variable belongs to. Problems like predicting whether a picture is of a cat or dog or predicting whether an email is Spam or not are Binary classification problems.
<h5><strong>Multi-class/Multinomial classification</strong></h5>
This is the task of classifying elements/ input variables into one of three or more classes/groups. Contrary to binary classification where elements are classified into one of two classes. Some use cases of this type of classification can be: <em>classifying news into different categories(sports/entertainment/political)</em>,<em> sentiment analysis;classifying text into either positive negative or neutral</em>, <em>segmenting customers for marketing purposes</em> etc.

Note that sentiment analysis can either be a binary classification or a multi-class classification depending on the number of classes you want to be used to classify text elements. In binary, one would predict whether a statement is "negative" or "positive", while in multi-class, one would have other classes to predict such as <em>sadness, happiness, fear/surprise and anger/disgust</em>.
<h5><strong>Multi-label classification</strong></h5>
This classification problem can be easily confused with the multi-class classification but they have a distinct difference. <strong>Multi-label</strong> is a generalization of multi-class which is a single-label problem of categorizing instances into precisely one of more than two classes. In this case, we have more than one discrete classes. Don't panic if you don't understand, here's an example that will help you out:

<strong>Explaining the difference between multi-class and multi-label classification</strong>

Let's take a movie classification problem where we'd like to classify movies based on their rating. A movie might be rated as "G"(general audiences),"PG"(parental guidance) or "R"(restricted) but the classifier is sure that each movie can only be categorized with only one out of those three types of rates i.e a movie can't be both R rated and PG rated at the same time. That's an example of a <strong>Multi-Class classification problem</strong>. However, if we are to classify movies based on their genres, a movie can be both comedy+thriller/romance+horror etc. Here each movie could fall into one or more different sets of genres. therefore each instance/input variable can be assigned with multiple categories. This is therefore a <strong>Multi-Label classification</strong>.

[caption id="attachment_1789" align="aligncenter" width="676"]<img class="wp-image-1789 size-large" title="Figure 1" src="http://categitau.com/wp-content/uploads/2018/04/Classification-700x350.png" alt="" width="676" height="338" /> First image shows an example of a Multi-labeled movie. Second Image shows an example of an R rated movie notification.[/caption]
<h4>Classification Algorithms</h4>
There are various classification algorithms that are used to make predictions such as:

<a href="https://en.wikipedia.org/wiki/Artificial_neural_network"><strong>Neural Networks</strong></a> - Has various use cases. An example is in Computer Vision which is done through c<em>onvolutional neural networks(CNN). </em>You can read more on how Google classifies people and places using Computer Vision together with other use cases on a post on <a href="http://symonmk.com/intro-computer-vision/">Introduction to Computer Vision</a> that my boyfriend wrote.

<strong><a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">K-NN</a> - </strong><em>K-Nearest Neighbors </em>is often used in search applications where you are looking for "similar" items. One of the biggest use cases of K-NN search is in the development of Recommender Systems.

<a href="https://en.wikipedia.org/wiki/Decision_tree"><strong>Decision Trees</strong></a> - Decision trees are used in both <em>regression</em> and <em>classification</em> problems. A decision tree can be used to visually and explicitly represent decisions and decision making. They can be used to assess the characteristics of a client that leads to the purchase of a new product in a direct marketing campaign.

<strong><a href="https://en.wikipedia.org/wiki/Random_forest">Random Forests </a>- </strong>Random Forest algorithms can also be used in both <em>regression</em> and <em>classification</em> problems. It builds multiple decision trees and merges them together to get a more accurate and stable prediction. It can be used in a number of circumstances including <em>image classification</em>, <em>recommendation engines</em>, <em>feature selection</em>, etc.

<strong><a href="https://en.wikipedia.org/wiki/Support_vector_machine">Support Vector Machines(SVM)</a> -</strong> This is a fundamental data science algorithm which can be used for both regression or classification problems. However, it is mostly used in classification problems. It has a plethora of use cases such as face detection, handwriting recognition and classification of images just to mention a few.

<strong><a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">Naive Bayes</a> - </strong>This is a simple and easy to implement algorithm. A classical use case for Naive Bayes is document classification where it determines whether a given text document corresponds to one or more categories. It can be used in classifying whether an email is Spam or not Spam or to classify a news article about technology, politics or sports. I've also previously done sentiment analysis using Naive Bayes. You can find the notes and code <a href="https://gist.github.com/CateGitau/6608912ca92733036c090676c61c13cd">here</a>.
<h4>Conclusion</h4>
If you made it thus far, congratulations! You now have an understanding of what supervised machine learning is together with its two categories with some perception of classification models. You should also be able to create your own use cases where classification models can be used, then group them into either <em>multi-label</em>, <em>multi-class</em> and <em>binary classification</em> problems. Building a classification prediction model doesn't end here. As a data scientist your motive is not to just build a predictive model alone, but creating a model which gives high accuracy out of sample data. So, you're done building your classification model using the various algorithms that I have outlined, the next step should be to evaluate its performance and determine if it will do a good job of predicting the target/output variables on new and future data.

In my next post, I'll be going through the various ways of evaluating classification models. Please share your opinions and thoughts in the comment section below!

To more learning:)
<h4>Reading material</h4>
<a href="https://medium.com/machine-learning-for-humans/supervised-learning-2-5c1c23f3560d">Machine Learning for Humans:Supervised Learning</a> (Medium)

&nbsp;

&nbsp;]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1774</wp:post_id>
		<wp:post_date><![CDATA[2018-04-13 09:00:23]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-04-13 06:00:23]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[introduction-to-supervised-learning]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="data-science"><![CDATA[Data Science]]></category>
		<category domain="category" nicename="machine-learning"><![CDATA[Machine Learning]]></category>
		<category domain="category" nicename="supervised-learning"><![CDATA[supervised learning]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
		<wp:meta_value><![CDATA[@realcategitau]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1583504300;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:2385;}i:1;a:1:{s:2:"id";i:2403;}i:2;a:1:{s:2:"id";i:1115;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_mess]]></wp:meta_key>
		<wp:meta_value><![CDATA[Classification in Supervised Learning - All the theory you'll need to know!..... I think :) #datascience #machinelearning #rstats #python]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[1793]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_skip_18590462]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_skip_18590466]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_cache_timestamp]]></wp:meta_key>
		<wp:meta_value><![CDATA[431864]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_totes]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_pinterest_image_url]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_facebook_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_pinterest_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_twitter_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_linkedin_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_google_plus_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_total_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>1893</wp:comment_id>
			<wp:comment_author><![CDATA[Foundations of Machine Learning: Part-1 &#8211; Catherine Gitau]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>https://categitau.com/foundations-of-machine-learning-part-1/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[192.185.33.134]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2020-01-07 02:49:06]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2020-01-06 23:49:06]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Note that a classification problem requires the examples to be classified into one of two or more classes and can be divided further into binary-classification problem, multi-class classification problem and multi-label classification problem. I&#8217;ve written a comprehensive introduction to supervised learning explaining these in a previous post which you can find here [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[a:3:{s:4:"time";d:1578481300.8997099399566650390625;s:5:"event";s:15:"status-approved";s:4:"user";s:9:"categitau";}]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[a:2:{s:4:"time";d:1578354546.262034893035888671875;s:5:"event";s:9:"check-ham";}]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>How to Evaluate Classification Models</title>
		<link>http://categitau.com/evaluating-classification-models/</link>
		<pubDate>Fri, 20 Apr 2018 19:39:06 +0000</pubDate>
		<dc:creator><![CDATA[categitau]]></dc:creator>
		<guid isPermaLink="false">http://categitau.com/?p=1787</guid>
		<description></description>
		<content:encoded><![CDATA[Let's say you have the last 3 years sales data by month and you want to predict the next year's sales (<em>this is an example of a regression problem</em>). What you would do is give your machine a sample of this data from the past years and ask it to predict the remainder of the known results you have. Once it does that, you can then compare its prediction to what actually happened to figure out how accurate it was. You will have to run lots of statistical models to try to come up with the most accurate prediction and use that.<!--more-->

[caption id="attachment_1808" align="aligncenter" width="676"]<img class="wp-image-1808 size-large" src="http://categitau.com/wp-content/uploads/2018/04/1WjXHRFcFT-7jPRWJ9Q5Ww-700x326.jpeg" alt="" width="676" height="315" /> Machine Learning Process[/caption]

&nbsp;

In this post, I'll be explaining some of the metrics used to evaluate the accuracy of models with a focus on<strong> Binary Classification</strong> <strong>models</strong> which basically involve classifying the data into two groups e.g. whether or not a customer buys a particular product, based on discrete input variables such as gender, age, location etc. To gain some more knowledge on various classification problems, have a look at my previous post on <a href="http://categitau.com/introduction-to-supervised-learning/"> Supervised Machine Learning</a>.

So, what makes a good classifier and how do we measure its performance?
<h4>Understanding false positives and false negatives</h4>
[caption id="attachment_1780" align="alignright" width="500"]<img class="wp-image-1780 size-full" src="http://categitau.com/wp-content/uploads/2018/04/TP.jpg" alt="" width="500" height="375" /> Figure.1[/caption]

There are two outcomes of the quality of predictions when the model is evaluated that we don't want but occur quite a number of times:
<ul>
 	<li><strong>False positive(Type I Error) - </strong>A test result which wrongly indicates that a particular condition or attribute is present. e.g A doctor claiming that a patient is pregnant but is not.</li>
 	<li><strong>False negative(Type II Error) - </strong>A test result which wrongly indicates that a particular condition or attribute is absent. e.g A doctor claiming that a patient is not pregnant when they actually are.<strong>(See Figure:1)</strong></li>
</ul>
<strong>Another example would be:</strong>

<em>Let's say you've started your own small lending company where you give out small loans to say.. people in the university that pay back the money with some interest after a period of time. In the list you have, there are obviously some people who are your friends and total strangers. Assuming you're just starting out and don't have any credit information about these people you want to lend to so you trust your gut and give out loans to the few you kinda trust abit. So, between the friends and the strangers who do you think would default and who do you think will not default? Here, let's assume you trust your friends more than the strangers so your friends are more likely to pay the loan. Unfortunately, you find out that your friends are the ones that defaulted on paying back. This scenario would fall under <strong>False positive Error</strong>. Since you made a positive claim that your friends will be able to pay back but did not. Whereas the strangers that you thought would default, end up paying. This then is a<strong> False negative Error</strong>, since you thought that the strangers are more likely to default but they ended up paying the loan.
</em>

With that in mind, we can now look at the various metrics used to evaluate classification models:
<h4>1. Confusion Matrix</h4>
I've been avoiding the Confusion Matrix for the longest time now simply because I don't like confusion and as the name describes it, I thought it was quite confusing but it really isn't. Hopefully I'll shed some light to people who are like me. A <a href="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix</a>  is an N x N matrix that is often used to summarize the performance of a classification model. It helps by giving us a better idea of what out classification model is predicting right and what the type of errors it is making. The number of correctly and incorrectly predicted values are summarized and stored in the table against the actual values as shown:

&nbsp;

[caption id="attachment_1806" align="aligncenter" width="673"]<img class="wp-image-1806" src="http://categitau.com/wp-content/uploads/2018/04/Screenshot-from-2018-04-19-10-42-06.png" alt="" width="673" height="382" /> Figure. 2[/caption]

&nbsp;

Let's say we have built a classification model that predicts whether a person is likely to pay a loan or not and our target variables/output variable (Y) are: "delinquent" (positive) and "not delinquent" (negative). The <strong>True Positive </strong>will contain the number of delinquent persons that our model was able to classify correctly. The <strong>True Negative </strong>section(<strong>Figure.2)</strong> will contain all the non delinquent persons that our model was able to classify correctly as non delinquent. The <strong>False Positive</strong> section which is our Type I Error will contain the number of non delinquent persons that our model classified wrongly as being delinquent while the <strong>False Negative</strong>(Type II Error) will contain the number of delinquent persons that our model predicted wrongly as being non-delinquent.

From the Confusion Matrix a couple of metrics are computed which are:
<ul>
 	<li><strong>Accuracy - </strong>This shows how often the classifier was correct. You can get this by adding the TP and the TN then dividing it with the total observations.</li>
</ul>
<p style="text-align: center;"><strong>Accuracy = (TP+TN)/(TP+TN+FN+FP)</strong></p>

<ul>
 	<li><strong>Precision/Positive Predicted Value(PPV) - </strong>This shows how often the model was correct when it predicted that a person is delinquent.</li>
</ul>
<p style="text-align: center;"><strong>Precision = TP/(TP+FP)</strong></p>

<ul>
 	<li><strong>Recall/True Positive Rate/Sensitivity - </strong>This shows how often the model predicted that a person is delinquent when they are actually delinquent.</li>
</ul>
<p style="text-align: center;"><strong>Recall/TPR = TP/(TP+FN)</strong></p>

<ul>
 	<li><strong>True Negative Rate/ Specificity - </strong>This shows how often the model predicted that a person is not delinquent when they are actually not.</li>
</ul>
<p style="text-align: center;"><strong>TNR</strong> = <strong>TN/(TN+FP)</strong></p>
&nbsp;

So as to create an ordering of observations that "separates" the two classes "delinquent and not delinquent" or "TNR and FNR" a <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">Receiver Operating Characteristic curve</a> (ROC curve) is used.
<h4>2. Receiver Operating Characteristic Curve</h4>
This is one other metric that is widely used in the industry. <strong>ROC</strong> curves are meant to give us the ability to assess the performance of the classifier over its entire operating range. It can be used to select a threshold for a classifier which maximizes the true positives while minimizing the false positives. Using the same example I've been using throughout this post, of a model that classifies a person as either a defaulter or a person that pays a loan, using unknown test set, our model can classify it in three ways as shown in the diagram below:

[caption id="attachment_1809" align="aligncenter" width="782"]<img class="wp-image-1809 size-full" src="http://categitau.com/wp-content/uploads/2018/04/2.png" alt="" width="782" height="461" /> Figure.3[/caption]

From the figure above <strong>(Figure.3) </strong>The separation between the defaulters(in red) and those that have paid(in blue) is quite poor since we have a number false positives. The second classification is not as bad since only two people have been classified wrongly. The classification on top is an example of a perfect separation since the defaulters are completely separated from the payers. ROC curves let us quantify these orderings in a graph as shown below:

<img class="wp-image-1810 size-full aligncenter" src="http://categitau.com/wp-content/uploads/2018/04/3.png" alt="" width="763" height="470" />

So, for every person classified as a defaulter, we move up the Y axis once and for every person classified as a payer, we move across the X axis once. This gives us the graphs above. The third graph gives an example of a <strong>Zero false positive model</strong> which perfectly classifies the defaulters and payers correctly. The ROC curve is the plot between<strong> sensitivity</strong> and (1- specificity). (1- specificity) is also known as<strong> false positive rate</strong> and sensitivity is also known as <strong>True Positive rate</strong>.

[caption id="attachment_1812" align="aligncenter" width="488"]<img class="wp-image-1812 size-full" src="http://categitau.com/wp-content/uploads/2018/04/ROC.png" alt="" width="488" height="298" /> Graph of the ROC curve[/caption]

From the image above, the diagonal red line is for a random model, so the further your model curve is from the random line, the better. The ROC gives a visual representation of how well the two classes "separate". What if we want to measure how well the classifier separates those two classes? This is were the <strong>Area Under the Receiver Operating Characteristic curve(AUC) </strong> comes in which is basically the area under the<strong> ROC curve, </strong>shaded green in the diagram below.

<img class="aligncenter wp-image-1811 size-full" src="http://categitau.com/wp-content/uploads/2018/04/4.png" alt="" width="779" height="476" />

&nbsp;

Other than the metrics I have mentioned above, there are other various metrics used for classification model evaluation that you can go have a look at such as:
<ul>
 	<li><strong>Kolmogorov Smirnov test</strong></li>
 	<li><strong>Gain and Lifts</strong></li>
 	<li><strong>F Scores</strong></li>
</ul>
<h4>Conclusion</h4>
I've just gone through a number of metrics used in evaluating classification models. This is an important and crucial part of the whole Machine Learning modeling process. You build a model, get feedback from the metrics, make some improvements on it and continue with this process until you achieve a desirable accuracy. This topic is discussed widely by other people, I'm also just one of those people but hopefully I've done it in a way that is useful for your learning, like it was for me. Credit goes to <strong><em>Skyler Speakman</em></strong>, a Research Scientist at IBM Africa who's taking a group of people including myself through a Data Science course in<a href="https://moringaschool.com/"><strong> Moringa School</strong></a>.
<h4>Reading Sources</h4>
<a href="https://towardsdatascience.com/model-evaluation-i-precision-and-recall-166ddb257c7b">Precision and Recall</a> (Medium)

<a href="http://www.ritchieng.com/machine-learning-evaluate-classification-model/">Evaluating a Classification Model</a> By Ritchie Ng

<a href="https://medium.com/@andygon/eli5-roc-curve-auc-metrics-ac4fe482f018">ROC Curve and AUC Metrics</a> (Medium)]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1787</wp:post_id>
		<wp:post_date><![CDATA[2018-04-20 19:39:06]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-04-20 16:39:06]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[evaluating-classification-models]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="data-science"><![CDATA[Data Science]]></category>
		<category domain="category" nicename="machine-learning"><![CDATA[Machine Learning]]></category>
		<category domain="category" nicename="supervised-learning"><![CDATA[supervised learning]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_abf59ba92ce9bf866f66106752f23d33]]></wp:meta_key>
		<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
		<wp:meta_value><![CDATA[@realcategitau]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1583381321;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:1774;}i:1;a:1:{s:2:"id";i:2385;}i:2;a:1:{s:2:"id";i:2403;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_c377b5814f7a34871f04f69919785a9e]]></wp:meta_key>
		<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[1804]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_skip_18590462]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_skip_18590466]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_pinterest_image_url]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_cache_timestamp]]></wp:meta_key>
		<wp:meta_value><![CDATA[431880]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_totes]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_facebook_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_pinterest_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_twitter_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_linkedin_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_google_plus_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_total_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Must Haves: 10 Data Science Books</title>
		<link>http://categitau.com/must-haves-10-data-science-books/</link>
		<pubDate>Tue, 01 May 2018 21:14:49 +0000</pubDate>
		<dc:creator><![CDATA[categitau]]></dc:creator>
		<guid isPermaLink="false">http://categitau.com/?p=1835</guid>
		<description></description>
		<content:encoded><![CDATA[<blockquote><i><span style="font-weight: 400;">Data Science is both an art and a science. The science accords means to measure accuracy, significance, and data manipulation strategies. Art infuses creative concepts on problem solving techniques.”</span></i>

-Chris Orwa</blockquote>
<!--more-->

<span style="font-weight: 400;">If you tour the World Wide Web today, you'll come across tonnes of Data Science material. Ranging from books, articles, online tutorials, you name it. These can all get a little overwhelming and confusing. So one may opt not to dig in; as finding the right book to help you understand basic concepts can be daunting.</span>

<span style="font-weight: 400;">I decided to do some research to find out at least 10 of the most helpful books every aspiring Data Scientist should lay their hands on. It is in these efforts that I interacted with various other professionals in the industry to discover the most relevant Data Science books to data scientists or anyone seeking a better understanding.</span>

Success in data science is not the ability to build prediction models in Python or R but mainly driven by knowledge of the subject. You must have sound knowledge of how things are done and also what algorithms, tools and techniques are being used.

<span style="font-weight: 400;">One of the ways you could get this knowledge is by reading books and being confident to start off in the field. I’ve displayed a mix of technical and non-technical books for you from my findings… Do note that the reviews and accounts are of the two experts I spoke to, the one and only <b>Chris Orwa(</b><a href="https://blackorwa.com/"><b>Black Orwa</b></a><b>) - Data Scientist Safaricom Limited </b> and the widely read and experienced<a href="https://github.com/Shuyib"> <b>Ben Mainye</b></a><b> of Africa’s Talking</b>.
</span>
<h3></h3>
<h3>List of 10 Must Have Data Science Books</h3>
<h5><b>1) Superforecasting: The Art and Science of Prediction </b><i><span style="font-weight: 400;">By Philip E. Tetlock and Dan Gardner</span></i></h5>
I've heard that no list of forecasting books is complete without reference to Superforecasting, The Art and Science of Prediction. This is definitely going into my "to-read" library this year.

<img class="size-medium wp-image-1830 alignleft" src="http://categitau.com/wp-content/uploads/2018/04/sup-196x300.jpg" alt="" width="196" height="300" />

<span style="font-weight: 400;">"...S<em>uperforecasting has been my favorite Data Science book so far! Not just for algorithms, but in providing concepts on how to think and become a great forecaster. Philip Tetlock’s experience running the</em></span><em><a href="https://goodjudgment.com/"><span style="font-weight: 400;"> Good Judgement Project</span></a><span style="font-weight: 400;"> is its basis.</span></em>

<em><span style="font-weight: 400;">The American Intelligence community had become weary of their ability to predict world events. They had missed 9/11 attack besides incorrectly identified WMD in Iraq and Shah’s overthrow in Iran. So they turned to social scientist, Professor Philip E. Tetlock for help.</span></em>

<em><span style="font-weight: 400;">Philip Tetlock had done a research project in which he monitored the predictions of political pundits. When he tallied the data, it proved political pundits were not better than the general public in predicting geopolitical events. It is this research that caught the eye of IARPA (Intelligence Advanced Research Projects Activity). IARPA needed a similar analysis of their intelligence analyst.</span></em>

<em><span style="font-weight: 400;">The result was the Good Judgment Project. Philip Tetlock setup an experiment where ordinary citizens with access only to public information could compete against CIA analysts with access to confidential information in prediction geopolitical events.</span></em>

<em><span style="font-weight: 400;">Guess what? After the first two years of the experiment, the citizen team, with their little training, were 20 percent more accurate in predicting world events compared to the intelligence community. This outcome brought to fore the conclusion that good analysis comes from good thinking rather than more or privileged data. As a Data Scientist, this was a wake up call to debunk the notion that Big Data leads to better prediction.</span></em>

<em><span style="font-weight: 400;">The book proceeds to include interviews with top forecasters whom Philip refers to as super forecasters. They are the subject of the book. In introducing the theory of superforecasters, Tetlock also introduced the Brier Score. The Brier Score is a metric that measures the gap between forecasts and reality for each person. Brier score keeps tabs on how accurate a person’s predictions are over time.</span></em>

<em><span style="font-weight: 400;">In Machine Learning, there’s the temptation to build a model and assume it will work in all circumstances. As such it would be fantastic to include a Brier Score to know how a model performs every time it makes a real-world prediction.</span></em>

<em><span style="font-weight: 400;">Over and above the technicalities, Tetlock also tackles the personalities of great forecasters. Most people who made accurate predictions were not experts in those field. They relied on good research to make conclusions. Experts sometime ignore research and rely on their experience which becomes a pitfall.</span></em>

<em><span style="font-weight: 400;">The amateurs were ready to make mistakes, while experts most times assume making mistakes is a sign of being less knowledge. Overall, the book is full of data science nuggets. You will learn of the origin of Randomized Control Trials (RCT) in medicine and the German army command structure in WW2 that made the highly effective (</span><a href="https://en.wikipedia.org/wiki/Mission-type_tactics"><span style="font-weight: 400;">auftragstaktik</span></a></em><span style="font-weight: 400;"><em>). In the end, the book helps to tie thinking and problems. A concept forgotten while running algorithms.</em>" - <strong>Chris Orwa</strong></span>
<h5><b>2) The Signal and the Noise: Why So Many Predictions Fail but Some Don’t</b><i><span style="font-weight: 400;">. By Nate Silver</span></i></h5>
This is one of the highly recommended books online. I've had this book for the longest time and it's about time I started on it and finished it. If you're one of those people that doesn't enjoy the mathematical basis that go behind data science, this books is for you!

<img class="size-medium wp-image-1822 alignleft" src="http://categitau.com/wp-content/uploads/2018/04/sig-190x300.jpg" alt="" width="190" height="300" />

<em><span style="font-weight: 400;">"</span><span style="font-weight: 400;">...This possibly my second-best Data Science book. </span><a href="https://en.wikipedia.org/wiki/Nate_Silver"><span style="font-weight: 400;">Nate Silver</span></a><span style="font-weight: 400;"> is an Economist who made a career performing statistical analysis on baseball matches. Otherwise known as </span><a href="https://en.wikipedia.org/wiki/Sabermetrics"><span style="font-weight: 400;">Sabermetrics</span></a><span style="font-weight: 400;">. In 2008, he turned his interest to politics and made accurate prediction for all States in the US except for one. He writes to give advice on how to make good predictions.</span></em>

<em>The book has overlapping concept’s with Tetlock’s Superforecasting book. It talks about the pitfall of Big Data and how political pundits make poor predictions. Nate’s book also adds information on how he was able to make accurate baseball predictions. For a statistical nerd, the details on determining a player’s performance is gold! In it, you will learn about<a href="https://en.wikipedia.org/wiki/PECOTA"> PECOTA</a>, the algorithm developed by Nate Silver to predict baseball matches outcome while working at KPMG.</em>

<em>Nate Silver now runs an amazing data journalism website<a href="http://fivethirtyeight.com/"> Five Thirty Eight</a>.</em><em style="font-size: 1.1em;"><span style="font-weight: 400;">" - <strong>Chris Orwa</strong></span></em>
<h5><b>3) The Quants: How a New Breed of Math Whizzes Conquered Wall Street and Nearly Destroyed</b> <b>It</b><span style="font-weight: 400;">. </span><i><span style="font-weight: 400;">By Scott Patterson</span></i></h5>
<strong>Quants</strong> - Quantitative analysts. The Quants is suited for people with a non-maths background or a manager, executive or data analyst who is interested in learning how to make decisions using numbers &amp; analysis, rather than intuition.

<img class="size-medium wp-image-1831 alignleft" src="http://categitau.com/wp-content/uploads/2018/04/The-Quants-194x300.jpg" alt="" width="194" height="300" />

<span style="font-weight: 400;">"...<em>Once upon a time, I made my living from trading currencies. During this period, I came about this book. It talks about how probability theory was first applied to trading and used to beat the market. </em></span>

<em><span style="font-weight: 400;">Ed Thorp, a mathematician (PhD) who had applied Brownian motion to black jack experimented on the same concept on price volatility and hit a jackpot</span><span style="font-weight: 400;">.</span></em>

<em><span style="font-weight: 400;">During this period, it was believed that it was impossible to ‘beat the market’. A phrase coined due to the Efficient Market Hypothesis (EMH).</span><span style="font-weight: 400;"> EMH states that the current price of a stock factors in all available information hence making it impossible to make above average returns.</span><span style="font-weight: 400;"> Using his model and ability to predict volatility, Thorp realized many stocks that appeared to be mispriced. Thorp had stumbled upon a gold mine full of arbitrage opportunities. He could now short overpriced stocks.</span></em>

<span style="font-weight: 400;"><em>The book ends with the 2009 market crash that was ostentatiously created by quants</em>." - <strong>Chris Orwa</strong></span>
<h5><b>4) Black Swan: The Impact of the Highly Improbable</b><span style="font-weight: 400;">. </span><i><span style="font-weight: 400;">By Nassim Taleb</span></i></h5>
This is another non-technical book about unpredictable events where you'll get to learn the limits of statistical methods.

<img class="size-medium wp-image-1832 alignleft" src="http://categitau.com/wp-content/uploads/2018/04/swan-195x300.jpg" alt="" width="195" height="300" />

<span style="font-weight: 400;">"...<em>Black Swan explores the limits of statistics. Nassim Taleb, an ex-quant, develops a brilliant idea about certain events that are impossible to predict e.g the 9/11 attack. He refers to this event as a black swan in line with the thought among Europeans that swans were white until they discovered black swans in Australia. Using this metaphor, Taleb dives into life events where statistics fail.</em></span>

<em><span style="font-weight: 400;">He has other books that compliment this title. They are:</span></em>
<ul>
 	<li><em><span style="font-weight: 400;">‘Fooled by Randomness’,</span></em></li>
 	<li><em>‘Antifragile’,</em></li>
 	<li><em>‘Bed of Procrustes’ and,</em></li>
 	<li><em>‘Skin in the Game’.</em></li>
</ul>
<em><span style="font-weight: 400;">The Black Swan is important in helping Data Scientists understand that we cannot solve all problems with statistics. This could be as a result of inadequate understanding or possibly being too far out in the future.</span></em>

<span style="font-weight: 400;"><em>Taleb builds a good concept of mediocristan and extremistan where he critics The Bell Curve and how quants apply it to every scenario. He writes, ‘Consequently, if we are in the domain of Extremistan, and we use analytical tools from Mediocristan for prediction, say risk management, we can face enormous surprises. Some of these surprises may be positive and some negative. Their impact will however most likely exceed what we are prepared for.’</em>" -<strong> Chris Orwa</strong></span>
<h5><b>5) Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy</b><span style="font-weight: 400;">. </span><i><span style="font-weight: 400;">By Cathy O’Neil</span></i></h5>
<em>Weapons of Math Destruction </em>has been labeled as being <em>captivating</em> and<em> insightful. </em>It talks about the increasing influence of machine learning to control the news we see, the jobs we can get and the politicians we vote for. Also a must read for me.

<img class="size-medium wp-image-1833 alignleft" src="http://categitau.com/wp-content/uploads/2018/04/wof-199x300.jpg" alt="" width="199" height="300" />

<span style="font-weight: 400;">"...<em>The book reads like a continuation of Ted Kaczynski's manifesto </em></span><em><span style="font-weight: 400;">'The Industrial Society and Its Future</span><span style="font-weight: 400;">'.</span></em>

<em><span style="font-weight: 400;">Cathy focusses on machine learning and its use in coercing behaviour change as well as discriminating the poor and disadvantaged. From the examples provided in the book, there are three categories of Weapons of Math Destructions (WMD):</span></em>

<em><strong>The first WMD, Poor Statistics - </strong></em><em><span style="font-weight: 400;">These are incorrectly calculated stats used to infer human behaviour and performance. In them, is lack of understanding on interpretation or validation of certain statistics. A good example are proxy variables, such as geography used to infer purchase power, reoffending propensity et cetera.</span></em>

<em><strong>The second WMD, Misused Correct Statistics - </strong></em><em><span style="font-weight: 400;">These seem to be the majority of the case in WMD. It is more of an ethical issue rather than machine taking over human lives. For instance when a company utilizes zip code to steer customers to high interest loans, that qualifies as unethical use of machine learning output and not necessarily anything wrong with the machine learning processes themselves.</span></em>

<em><strong>The last WMDs, Dataset - </strong></em><span style="font-weight: 400;"><em>From the book, certain attributes within data should never be used for prediction purposes. For instance race, gender, income and zip code. This is because they are likely to correlate with outputs connected with discrimination.</em>" - <strong>Chris Orwa</strong></span>

&nbsp;

Here are some two other non-technical books that I thought you should also have:
<h5><strong>6) Predictive Analytics: The Power to Predict Who will Click Buy, Lie or Die. </strong><em>By Eric Siegel</em></h5>
<img class="size-full wp-image-1827 alignleft" src="http://categitau.com/wp-content/uploads/2018/04/pred.jpeg" alt="" width="183" height="276" />

In “<a href="https://www.amazon.com/Predictive-Analytics-Power-Predict-Click/dp/1119145678/ref=pd_sim_14_3?_encoding=UTF8&amp;psc=1&amp;refRID=A60MNAEMYYXVEZH23GYX">Predictive Analytics</a>“, Eric Siegel, a renowned expert in data analytics and former professor at Columbia University, explains to us how data scientists use data to help predict anything - from what you will buy, to where you will travel to when you're likely to quit your job and much more.

This was one of my first non-technical data science book that I got for myself.. Sadly I'm still yet to finish it. Though, I still highly recommend it to anyone who wants to really understand what data science is all about. The book entails a plethora of real word examples. These examples can be generalized into a number of different applications throughout a company and has a tone of relevance to multiple business departments
<h5> <strong>7) Storytelling With Data : A data Visualization Guide for Business professionals.</strong> By Cole Nussbaumer Knaflic</h5>
<img class="wp-image-1839 alignleft" src="http://categitau.com/wp-content/uploads/2018/05/st-241x300.jpg" alt="" width="200" height="249" /><em>"storytelling with data</em> teaches you the fundamentals of data visualization and how to communicate effectively with data. You'll discover the power of storytelling and the way to make data a pivotal point in your story. The lessons in this illuminative text are grounded in theory, but made accessible through numerous real-world examples—ready for immediate application to your next graph or presentation."

Another book I'll recommend to those interested in finding insights from data through data visualization. Another "to read" in 2018!
<h4>Mathematical/Technical Books</h4>
<h5><b>8) OpenIntro Statistics.</b> <i><span style="font-weight: 400;">By David Diez, Christopher Barr, and Mine Çetinkaya-Rundel</span></i></h5>
Want to start getting your hands dirty with Statistics, then I highly recommend this book. All the source code that went into making this book is freely accessible online and all you need is some basic skills in R to run them

<img class="size-medium wp-image-1819 alignleft" src="http://categitau.com/wp-content/uploads/2018/04/open_intro-240x300.jpg" alt="" width="240" height="300" />

<span style="font-weight: 400;">"...<em>This book is available for free at</em></span><em><a href="http://leanpub.com/"><span style="font-weight: 400;"> leanpub.com</span></a><span style="font-weight: 400;">.</span></em>

<em><span style="font-weight: 400;">It was used as a book resource for the course </span><span style="font-weight: 400;">Data Analysis and Statistical Inference in <a href="http://coursera.org">coursera.org</a></span><span style="font-weight: 400;">. It begins by giving you an overview of data, probability, inferential techniques. These range from numerical data and categorical data, linear regression and multiple and logistic regression with numerous examples to walk you through the material. </span></em>

<em><span style="font-weight: 400;">The book and the course emphasize the need to learn for instance, to calculate hypothesis testing by hand and think about the problem as well as knowing how to code it in a programming language called R. </span></em>

<em><span style="font-weight: 400;">The team made a whole package where you can practice the concepts they cover which is available for download here</span><a href="https://www.rdocumentation.org/packages/openintro/versions/1.7.1"><span style="font-weight: 400;"> https://www.rdocumentation.org/packages/openintro/versions/1.7.1</span></a><span style="font-weight: 400;">. </span></em>

<span style="font-weight: 400;"><em>I learned statistics and more importantly, to be more data curious. This laid the foundation to me questioning a lot of things and doing a lot of observational studies and experiments. I recommend it if you are starting out, doing computer science, data science and genomic data science or just curious</em>." - <strong>Ben Mainye</strong></span>
<h5><b>9) Introduction to Machine Learning with Python: A guide for Data Scientists</b> <i><span style="font-weight: 400;">By Andreas Müller and Sarah Guido</span></i></h5>
This is a perfect book to get introduced to supervised and machine learning algorithms using python so as to make pretty good predictions. Will come in handy if you're planning on getting into doing some <a href="https://www.kaggle.com/">Kaggle</a> competitions.

<img class="size-medium wp-image-1820 alignleft" src="http://categitau.com/wp-content/uploads/2018/04/intro_to_py-229x300.jpg" alt="" width="229" height="300" />

<span style="font-weight: 400;">"...<em>As the title of the book says, it is a guide for data scientists.</em></span>

<em><span style="font-weight: 400;">The authors go through machine learning algorithms and concepts using examples. With an emphasis on data visualization to understand how the models make decision boundaries, for instance support vector machines and K-nearest neighbours. As a bonus, they discuss the strengths and weaknesses of several algorithms. You don’t need to be a pro at machine learning. You can just pick it up and start building your own models. I learned how machine learning algorithms work and how to implement them in my own work. </span></em>

<em><span style="font-weight: 400;">All the work that I’ve done so far I've always used this book as reference. If you don’t believe me check my <a href="https://github.com/Shuyib">github</a> repository. I recommend it because it was my favorite 2017 book and it has been my handbook while doing competitions in Kaggle and DrivenData. </span></em>

<em><span style="font-weight: 400;">Andreas Müller and Hugo Bowne Anderson also made a course about Supervised Learning With Sci-Kit Learn which  pumped me up further to get the book. It's here: </span><a href="https://www.datacamp.com/courses/supervised-learning-with-scikit-learn"><span style="font-weight: 400;">https://www.datacamp.com/courses/supervised-learning-with-scikit-learn</span></a>. - <strong>Ben Mainye</strong></em>
<h4><b>10) Deep Learning with Python.</b> <i><span style="font-weight: 400;">By François Chollet</span></i><span style="font-weight: 400;">.</span></h4>
Does deep learning tickle your fancy? Then this is a good book to get you started on building deep learning models using <em><a href="https://en.wikipedia.org/wiki/Keras">Keras</a> </em>which is a high-level neural network API that is written in Python. The author of this book is the creator of Keras. You can get this book <a href="https://livebook.manning.com/">here</a>.

<img class="size-medium wp-image-1821 alignleft" src="http://categitau.com/wp-content/uploads/2018/04/deepl-239x300.jpeg" alt="" width="239" height="300" />

<span style="font-weight: 400;">"...<em>François uses examples wrapped with theory to teach Deep Learning. </em></span>

<em><span style="font-weight: 400;">He goes through deep learning in parts: Part One focuses on fundamentals of machine learning where you’ll learn the basics of machine learning experiments and how they’re transferable to other areas.</span></em>

<em><span style="font-weight: 400;">Part Two focuses on the practicals where you’ll apply the knowledge you’ve gained in the first part to real world datasets and new concepts are also presented -- you’ll code a lot here.</span></em>

<em><span style="font-weight: 400;">I like the arrangement of the book, the practical exercises and the advice he gives as you go through the book. I learned to structure my machine learning experiments better and how to tune hyperparameters better especially for neural networks. Plus, i enjoyed chapter 7, 8 and 9 the most.</span></em>

<span style="font-weight: 400;"><em>I recommend the reader to get it because the advice the author gives about machine learning and deep learning is priceless. He even guarantees that you’ll become a Keras expert after reading his book. So get it!</em>" - <em><strong>Ben Mainye</strong></em></span>
<h4>Conclusion</h4>
This article has outlined some of the data science books that you need to have in your library to get you started with data science, machine learning and deep learning as well. If you don't yet have your hands on any of these books then what are you waiting for?

Should you have any thoughts to add on the books mentioned or have other books that you'd like to recommend, you can comment below or find me on twitter<a href="https://twitter.com/realcategitau"> @realcategitau</a>. You can get some of these books at <a href="https://prestigebookshop.com/"><b>Prestige Bookshop</b></a> Nairobi, Kenya or on Amazon.

I'd like to give credit to <a href="https://twitter.com/blackorwa?lang=en"><em>Chris Orwa</em></a> and<a href="https://twitter.com/Shuyin_ben?lang=en"> <em>Ben Mainye</em></a> for taking their time to give reviews of the books mentioned and the awesome<a href="https://medium.com/@apondihazel"> Hazel Apondi</a> for helping out with the editing of this article.

&nbsp;]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1835</wp:post_id>
		<wp:post_date><![CDATA[2018-05-01 21:14:49]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-01 18:14:49]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[must-haves-10-data-science-books]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="data-science"><![CDATA[Data Science]]></category>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[1836]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
		<wp:meta_value><![CDATA[@realcategitau]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1583637968;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:1759;}i:1;a:1:{s:2:"id";i:1719;}i:2;a:1:{s:2:"id";i:1115;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_mess]]></wp:meta_key>
		<wp:meta_value><![CDATA[Must Haves: 10 Data Science Books]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_skip_18590462]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_skip_18590466]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_totes]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_pinterest_image_url]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_cache_timestamp]]></wp:meta_key>
		<wp:meta_value><![CDATA[431878]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_facebook_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[7]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_pinterest_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_twitter_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_linkedin_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_google_plus_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_total_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[7]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>What is Machine Learning&#039;s role in Dynamic Pricing?</title>
		<link>http://categitau.com/dynamic-pricing/</link>
		<pubDate>Mon, 14 May 2018 13:30:46 +0000</pubDate>
		<dc:creator><![CDATA[categitau]]></dc:creator>
		<guid isPermaLink="false">http://categitau.com/?p=1851</guid>
		<description></description>
		<content:encoded><![CDATA[Have you ordered an Uber during this rainy season and wondered why the prices fluctuate from time to time?, or when it comes to purchasing a ticket through an airline, the closer you get to the traveling date, the more expensive tickets all of a sudden become? The reason for this is a pricing strategy known as <em><a href="https://en.wikipedia.org/wiki/Dynamic_pricing"><strong>dynamic pricing</strong></a>, </em>an approach to pricing goods based on <strong>real-time</strong> changes in the market such as demand for certain goods and services, time of purchase, change in certain conditions etc. The keyword here being <em>real-time</em>. We're all familiar with the <strong>demand-based variable pricing</strong> an example being the rise in <a href="https://en.wikipedia.org/wiki/Matatu">Matatu</a> fare prices during peak hours and when there's high demand for them. But where does Machine Learning come in when talking about dynamic pricing?<!--more-->

Throughout this rainy season, a lot of businesses are adjusting their prices from time to time, from the popular Uber to the Matatus we use on the daily and I could bet that gumboots prices have shot up. These changes in prices especially how Uber does it in real-time got me curious as to how data and Machine Learning is being used by businesses to set flexible prices for products and services. In this post, we'll try to understand dynamic pricing in detail, its importance and use cases together with where machine learning comes in.

There are various different forms of dynamic pricing:
<ol>
 	<li><strong>Peak Pricing</strong> - This is a strategy that is common in transportation businesses. Airlines are a good example. Airlines often charge a higher price to travel during rush hour mostly on weekdays and sometimes on weekends.</li>
 	<li><strong>Surge Pricing</strong> - Companies such as Uber respond dynamically to changes in supply and demand in order to price their services differently. Like most of us have noticed, this frequently happens on stormy evenings and nights when more people request for cabs. <strong>Taxify</strong> also not so long ago introduced dynamic pricing to ensure the drivers are encouraged to go online and offer services when the demand is high.</li>
 	<li><strong>Pricing based on time of purchase</strong> - Again, airlines frequently use this strategy. The price of a certain class my go higher based on the number of days left to the flight.</li>
 	<li><strong>Segmented Pricing</strong> - In this type of dynamic pricing, some group of customers are charged more based on their willingness to pay more for a given service or product.</li>
 	<li><strong>Change in Conditions</strong> - Using dynamic pricing strategies, a business can boost profits more under certain market conditions. For example, a restaurant and bar in New York can change the price of a Tequila shot every five minutes based on demand. If more people order one tequila brand, the price of another might drop. Interesting right?</li>
</ol>
There are a tone of other areas where dynamic pricing can be utilized. Like in Gaming applications, online retail shops, banking, it could also be used to reduce customer churn whereby when your application starts identifying a customer as being likely to churn, it reduces the price of certain products to keep the customer interested.

For many years, retailers have been using what is known as traditional, static, or fixed pricing where a fixed price point is determined and maintained for an extended amount of time. This method becomes difficult when a new product is being introduced into the market and are unable to accurately predict the effect of the changing demand into the price. Retailers may also set the price in accordance to their desired target market. However, the product or service might appeal to a different consumer segment that doesn't agree with their set price thus limiting the amount of products sold.

Recently, <a href="https://en.wikipedia.org/wiki/EBay">Ebay</a> a giant e-commerce corporation, announced in December 2017, that it will acquire a Canadian data analysis firm <strong>Terapeak</strong> which is good at predicting supply, demand and pricing products so as to give their sellers price guidance and comparisons. You can read more about this <a href="https://www.ebayinc.com/stories/news/ebay-signs-agreement-to-acquire-terapeak/">here</a>.

The Importance of using "big data" and Machine Learning to improve price decision support in business has been rapidly increasing and the urgent need of building models for dynamic price prediction has been raised, bringing together statistical researchers with a business sense to solve modern business problems. Data Mining, Machine Learning and Statistical Methods can be useful in predicting the purchase behaviour of an online customer by selecting an appropriate price range for them based on dynamic pricing.

By obtaining a solution for price prediction for products and services, it will be easier for sellers to sell and enlarge the number of goods or services being sold as well as increase the shopping community. With the flexible prices, retailers can bring in higher profits for each sale made. There are lots of other advantages of dynamic pricing especially in e-commerce businesses.

We'll now look at a competition on <strong>Kaggle</strong> known as the <a href="https://www.kaggle.com/c/mercari-price-suggestion-challenge"><strong>Mercari Price Suggestion </strong></a><span style="color: #13c4a5;"><b><u>Challenge</u></b></span>. Mercari is Japan's biggest community-powered shopping application. The purpose of this challenge is to build a system that offers pricing suggestions to their sellers.
<h3>Using Machine Learning to Suggest Product Prices to Online Retailers</h3>
The basic Idea here is to find the best price by analyzing product characteristics. I will briefly take you through my thought process for building such a model using the various submissions made on the Mercari Kaggle competition.

Disclaimer: This is just an overview of how Machine Learning can be used to suggest product prices using the Mercari competiton as an example. You can find the various codes to build such a system on the Kaggle kernel and discussion page of the competition. If you're an R fan, you can find a detailed guide to predicting the prices <a href="https://www.kaggle.com/jeremiespagnolo/beginner-s-guide-to-mercari-in-r-0-50586/code">here</a>.

The following diagram shows the framework for building such a model or rather any machine learning model.

[caption id="attachment_1861" align="aligncenter" width="809"]<img class="wp-image-1861 size-full" src="http://categitau.com/wp-content/uploads/2018/05/frame.png" alt="" width="809" height="178" /> Figure:1[/caption]
<h4>Data Collection</h4>
You can get the data set from the Kaggle competition <em>"Mercari Price Suggestion Challenge". </em>The following table shows the features provided by Mercari.

[caption id="attachment_1863" align="aligncenter" width="676"]<img class="wp-image-1863 size-large" src="http://categitau.com/wp-content/uploads/2018/05/Screenshot-from-2018-05-13-11-45-27-700x315.png" alt="" width="676" height="304" /> Table:1[/caption]
<h4>Pre-Processing of the Data</h4>
This will involve handling missing values in the data as well as some text-processing on the item-description. A lot of knowledge on Natural Language Processing will be needed here to extract meaningful insights from the item description, as well as cleaning the text data. Preprocessing is also required in order to prepare data sheets for the specific tools for the analysis purpose and the model that's going to be used.
<h4>Data Exploration</h4>
Data Exploration is done to get meaningful information from the data. In this case we could explore the data to find out things like:
<ul>
 	<li>Price range of the dataset</li>
 	<li>Average price of items</li>
 	<li>Median price</li>
 	<li>The distribution of the price - Here you will notice that the data is heavily skewed and will need to use the log of the prices instead to get a more Normal distribution'</li>
 	<li>Top Brands and their categories</li>
 	<li>Find the words or phrases with high frequency using the combination of words which are referred to as unigrams, bigrams and tri-grams.</li>
</ul>
<h4>Feature Engineering</h4>
This is the process of using knowledge of the data to create features that will make your machine learning algorithm work. This is a fundamental step in any machine learning problem. Kaggle competition data is usually already split into test and training sets so we'll go right to the modeling.
<h4>Modeling</h4>
For this specific competition it was seen that <a href="https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/">XGBoost</a> also known as extreme gradient boosting was a relatively accurate model to use for this competition as well as RMSLE(Root Mean Squared Logarithmic Error)as the statistical performance indicator. The lower RMSLE, the better the model.

Suggesting product prices to online retailers is just one of the many ways machine learning can be used in Dynamic Pricing. Companies such as Airbnb are using data to build algorithms that take into account things like location, type of property, duration etc to help people set their prices for their location. A lot of ticket companies have also started to dynamically change prices when concerts and games don't sell well.

Machine Learning can also be used to predict the purchase behavior of online customers by selecting an appropriate price range based on dynamic pricing.
<h4>Conclusion</h4>
Dynamic pricing is one of the many applications of Machine Learning that is rapidly growing. Can you think of other areas that can utilize dynamic pricing? Let me know in the comments.
<h4>References</h4>
BEGINNER'S GUIDE TO MERCARI IN R - [0.50586]
<a href="https://www.kaggle.com/jeremiespagnolo/beginner-s-guide-to-mercari-in-r-0-50677">https://www.kaggle.com/jeremiespagnolo/beginner-s-guide-to-mercari-in-r-0-50677</a>

Walters, Troy.(2017). A Very Extensive Mercari Exploratory Analysis. <a href="https://www.kaggle.com/captcalculator/a-very-extensive-mercari-exploratory-analysis">https://www.kaggle.com/captcalculator/a-very-extensive-mercari-exploratory-analysis</a>

Joseph Pisani(2016).HOw much for that tequlla shot? <a href="https://apnews.com/eef9b62171c74854a79e619bfe27d4ce/how-much-tequila-shot-price-always-changing">https://apnews.com/eef9b62171c74854a79e619bfe27d4ce/how-much-tequila-shot-price-always-changing</a>

XGBoost- A Competitve Approach for Online Price Prediction(2018)-<a href="http://programme.exordo.com/mwdsi2018/delegates/presentation/35/">http://programme.exordo.com/mwdsi2018/delegates/presentation/35/</a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1851</wp:post_id>
		<wp:post_date><![CDATA[2018-05-14 13:30:46]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-14 10:30:46]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[dynamic-pricing]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="data-science"><![CDATA[Data Science]]></category>
		<category domain="category" nicename="machine-learning"><![CDATA[Machine Learning]]></category>
		<category domain="category" nicename="research"><![CDATA[Research]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_d799e022d3b35e2c9cd5da38180080d4]]></wp:meta_key>
		<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
		<wp:meta_value><![CDATA[@realcategitau]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1583714513;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:2385;}i:1;a:1:{s:2:"id";i:2403;}i:2;a:1:{s:2:"id";i:1835;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[1859]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_dc2ed537c12170ec4ad20c87ccacbb29]]></wp:meta_key>
		<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_mess]]></wp:meta_key>
		<wp:meta_value><![CDATA[Find out Machine Learning's role in pricing goods based on real-time changes in the market. #MachineLearning #Data Science #Python #R]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_skip_18590462]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_skip_18590466]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_google_plus_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_pinterest_image_url]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_cache_timestamp]]></wp:meta_key>
		<wp:meta_value><![CDATA[431872]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_facebook_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_linkedin_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_twitter_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_pinterest_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_totes]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_total_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[2]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>A Basic Introduction to Cohort Analysis in R</title>
		<link>http://categitau.com/introduction-to-cohort-analysis-in-r/</link>
		<pubDate>Tue, 29 May 2018 22:23:33 +0000</pubDate>
		<dc:creator><![CDATA[categitau]]></dc:creator>
		<guid isPermaLink="false">http://categitau.com/?p=1880</guid>
		<description></description>
		<content:encoded><![CDATA["<em>Just like a doctor has to stay up to date with the latest medical developments, learning never stops for a data scientist. Technology is evolving pretty quickly, so as this field; If you are not passionate about the field and do not enjoy learning, then data science is not for you</em>." <!--more-->

Since Last week, I have been reading on a topic called<strong> Cohort Analysis </strong>which I admit, was totally new to me then. Coincidentally, during this time I was also binge watching a TV series known as <a href="https://en.wikipedia.org/wiki/Silicon_Valley_(TV_series)">Silicon Valley</a>.<strong> NB: Spoiler Alert! </strong>In Ssn3 Ep09, a Startup called <strong>Pied Piper</strong> was celebrating its 500,000th app install. Exciting? Imagine your application having those many installs.

[caption id="attachment_1884" align="aligncenter" width="480"]<img class="wp-image-1884 size-full" src="http://categitau.com/wp-content/uploads/2018/05/giphy.gif" alt="" width="480" height="268" /> How most companies celebrate when they see the number of sign-ups into their platform[/caption]

But, is number of installs really a good metric? I don't think so. While it is useful for a company to know how many people have downloaded their application or are on their site, that metric is quite useless on its own. While Pied Piper was hitting the 500,000th install mark, the actual number of active users of the app, as they came to find out later was actually 19,000! Yup! Instead on focusing on the daily active users of the application, Pied Piper was focusing on the number of installs of their product which really doesn't say anything about their product. That kind of metric is known as a vanity metric, which is basically a measure that doesn't matter. With 19,000 active users, clearly there's something they were doing wrong. By using Cohort Analysis, the startup could've figured out the problem way earlier. You can have a look at this <a href="https://amplitude.com/blog/2016/06/23/pied-piper-boost-retention">article</a> to find out what Pied Piper could've done to improve their retention rate.

In this post we'll look at cohort analysis together with its importance in any business and company. We'll also look at how to perform cohort analysis using R and the famous plot library <a href="http://ggplot2.tidyverse.org/">ggplot 2</a>!
<h3>What is Cohort Analysis?</h3>
<a href="https://en.wikipedia.org/wiki/Cohort_analysis">Cohort Analysis</a> is the study of how a certain group of people (cohorts) that share common characteristics or experiences behave within a defined time span. These groups of people also known as <strong>cohorts</strong> are usually divided depending on when they initially subscribed to the business or company's service. An example would be a group of users who joined on a particular day or month. This allows companies to see patterns in their data clearly across the life-cycle of a customer as well as explaining customer behavior changes across time by factoring in the date in which the customer was first acquired. There are many other characteristics one can consider besides the user start date such as: <em>Defining Cohorts based on the sources of registration, be it using Search, Email, Social Network, by campaigns, by the platform or device your users use etc.</em>

In this post, we'll focus on grouping cohorts based on their start date.

This helps to answer questions like;
<ul>
 	<li>Do users who signed up a year ago use your products the same way/differently than those who signed up last month?</li>
 	<li>Are the old users bringing in more revenue or are the new ones?</li>
 	<li>What is the retention rate of our customers?</li>
 	<li>Is the rate at which we are losing customers getting better or worse?</li>
 	<li>What % of the revenue came from new vs. repeat customers?</li>
 	<li>Where do we lose customers?</li>
</ul>
&nbsp;
<h3>Applications of Cohort Analysis</h3>
<ul>
 	<li>E-Commerce - The firm may be interested in finding out the percentage of customers that signed up in the last two weeks and made a purchase.</li>
 	<li>Software developers - May be interested in finding out the number of users that sign up to their platform after a certain upgrade of their application or when a new feature has been added to the platform. This will help them gauge if it was a right move or the wrong one.</li>
 	<li>Gaming - A gaming industry might be losing revenue but at the same time still be having lots of users signing up. After deep analysis they might find out that the expert gamers who have been using their platform the most(cohort1) bring in more income than the new users coming in, hence will start focusing on making sure the lag time is low and also add more advanced features to the game.</li>
 	<li>Marketing - By the use of cohort analysis, marketers are able to see if their marketing efforts at a certain period made any difference. They could also examine individual cohorts to gauge responses to short-term marketing efforts like email campaigns.</li>
 	<li>Companies can also see how the behavior and the performance of Individual groups of users change day to day, week to week, relative to when they acquired those users.</li>
 	<li>It allows the ability of a company to drill down to the users of each cohort to gain a better understanding of their behavior such as: How many users stopped using their service and how much revenue did they bring in.</li>
</ul>
<h3></h3>
<h3>Metrics used in Cohort Analysis</h3>
<strong>Monthly Active Users (MAU)</strong>

This is a Key Performance Indicator often used by online games, mobile applications and social networking sites. It is calculated by counting the number of unique users for a 30-day period. For some companies, a user is said to be active by logging in a platform and interacting with the platform, from liking posts, posting something, commenting etc. This metric helps in determining the value of a company and find the number of users who use their platforms and return to the site on a monthly basis. Aside from monthly basis, active users can also be measured daily (daily active user) and weekly (Weekly active user).

<strong>Monthly Recurring Revenue (MRR)</strong>

This is income that a business can count on receiving every single month. This is one of the most meaningful metrics a Software as a Service (Saas) business can measure. MRR not only tells you how much your customers are paying you for subscriptions every month, but also reveals if whether your business growth is sustainable.

<strong>Cost of Acquisition (CAC)</strong>

This is simply the cost of acquiring a new user. Might be through marketing, advertising, holding an event, etc. This is calculated by taking the total marketing cost divided by the number of newly acquired users. For example; if your company spends $100 on marketing and it manages to acquire 10 new users then each user has a CAC of 10. If this new user is not expected to contribute at least $10 during his lifetime as a user, then the effort doesn't make any sense.

<strong>Customer Lifetime Value (LTV)</strong>

This is the value you gain from a user throughout their lifetime. This value has to exceed the cost of acquiring a new user (CAC), otherwise the business may never be profitable. This is calculated up to different periods; could be 12 months(what you make from this client in 12 months), 36 months, etc. LTV = number of orders a customer is expected to make over their lifetime as a user(in recurrence) and multiply it by the company's average contribution margin per order.

You can find more on these metrics in a blog written by a VC fund company known as Samaipata Ventures<a href="https://medium.com/samaipata-ventures/samaipatas-3-step-5-cohort-analysis-b75f534464fa"> here</a>.

&nbsp;
<h3>Performing Cohort Analysis in R</h3>
Let's assume that we are running an E-Commerce business and we'd like to analyze user retention together with finding out how our users are spending money over time. Here are the stages one should take while performing such an analysis:
<ol>
 	<li>Determine the business question that you'd like to answer.</li>
 	<li>Define metrics that will be able to help you answer the questions you came up with during step1.</li>
 	<li>Define the cohorts that are relevant and of interest.</li>
 	<li>Data sourcing, cleaning and exploration.</li>
 	<li>Create the cohorts and extract data according to cohorts.</li>
 	<li>Calculate measures.</li>
 	<li>Analyze results and adjust the parameters.</li>
 	<li>Present and explain the results.</li>
</ol>
Ready to get your hands dirty with some code? Click on the image below, which should take you to my Rnotebook where you will learn how to perform cohort analysis using R. There's still room for improving the code, let me know if you have any suggestions.

<a href="https://nbviewer.jupyter.org/github/CateGitau/cohort_analysis_R/blob/4571d30a2b57f9759d3e76403176568ae0d9ee50/Cohort_analysis.ipynb"><img class="aligncenter wp-image-1887 size-large" src="http://categitau.com/wp-content/uploads/2018/05/patrick-tomasso-208114-unsplash-700x338.png" alt="https://nbviewer.jupyter.org/github/CateGitau/cohort_analysis_R/blob/master/Cohort_analysis.ipynb" width="676" height="326" /></a>

&nbsp;]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1880</wp:post_id>
		<wp:post_date><![CDATA[2018-05-29 22:23:33]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-29 19:23:33]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[introduction-to-cohort-analysis-in-r]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_e4c915c44c853df86d198dc745fe22a4]]></wp:meta_key>
		<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
		<wp:meta_value><![CDATA[@realcategitau]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_71f142289e8d94d3f23d6cf7f5accf42]]></wp:meta_key>
		<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1583699283;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:668;}i:1;a:1:{s:2:"id";i:1835;}i:2;a:1:{s:2:"id";i:1787;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_mess]]></wp:meta_key>
		<wp:meta_value><![CDATA[A Basic Introduction to Cohort Analysis in R]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_skip_18590462]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_skip_18590466]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[1887]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_totes]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_pinterest_image_url]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_cache_timestamp]]></wp:meta_key>
		<wp:meta_value><![CDATA[431875]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_facebook_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_pinterest_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_twitter_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_linkedin_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_google_plus_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_total_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_190af93164c42903a168b1517ed785e0]]></wp:meta_key>
		<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_0f656d6cbd0687d6b6a80c3b6537151b]]></wp:meta_key>
		<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Success Stories of Reinforcement Learning</title>
		<link>http://categitau.com/success-stories-of-reinforcement-learning/</link>
		<pubDate>Fri, 16 Nov 2018 16:38:13 +0000</pubDate>
		<dc:creator><![CDATA[categitau]]></dc:creator>
		<guid isPermaLink="false">http://categitau.com/?p=1917</guid>
		<description></description>
		<content:encoded><![CDATA[<strong>Tl;dr</strong>

In September 2018, I got the opportunity to attend the <a href="http://www.deeplearningindaba.com/">Deep Learning Indaba</a> conference that was held in Stellenbosch University, South Africa. Deep Learning Indaba was formed with an aim to strengthen African Machine Learning as well as to increase African participation and contribution to the advances in artificial intelligence and machine learning, and address issues of diversity in these fields of science. One of the lectures that I really enjoyed was on <strong>Success Stories of Reinforcement Learning </strong>where we got introduced to reinforcement learning as well as how it was used to build some pretty awesome computer programs. This lecture was presented by <a href="https://en.wikipedia.org/wiki/David_Silver_(programmer)">David Silver</a>. Professor David Silver Leads the reinforcement learning research group at <a href="https://en.wikipedia.org/wiki/DeepMind">DeepMind</a> which is an AI company based in London that was acquired by Google in 2014. He was also a researcher on <a href="https://en.wikipedia.org/wiki/AlphaGo">AlphaGo</a>, a computer program that plays the board game Go. You can find other slides and videos from the conference <a href="http://www.deeplearningindaba.com/slides-2018.html">here.</a>

A <a href="https://medium.com/nairobiai/in-the-spirit-of-masakhane-post-deep-learning-indaba-nairobi-5ee452e49d0d">post deep learning Indaba meet-up</a> was organized here in Nairobi, Kenya to explore the latest that was discussed during Deep Learning Indaba 2018 and was hosted by <a href="https://www.meetup.com/NairobiAI/events/254798950/">Nairobi AI</a>. I decided to speak on David Silver's presentation, which then forced me to do some more research on the topic. This is what inspired this blog post.

In one of my previous <a href="http://categitau.com/introduction-to-supervised-learning/">posts</a>, I introduced Machine Learning and talked briefly about the two most common types of Machine Learning which are <strong>Supervised Learning</strong> and <strong>Unsupervised Learning</strong>. There's also <strong>Reinforcement Learning</strong> which I've never touched on mainly because I had very little knowledge on the topic and It's rarely used. In this post, I will introduce you to Reinforcement Learning and also look at how It's being applied. In simple words, how robots will take over the world *chuckles*.

&nbsp;
<h1><b>Reinforcement Learning(RL)</b></h1>
<a href="https://en.wikipedia.org/wiki/Reinforcement_learning">Reinforcement Learning</a> is said to be the true hope of Artificial Intelligence because of the immense potential that it possesses. Reinforcement Learning is learning what to do based on the environment and how to map the situations into actions. The success is measured by a scalar reward signal. The learner is not told which actions to take but instead must learn or discover the actions which will yield maximum reward.

Let's try and understand this better using a good example I came across while reading <strong>James Clear's</strong> Book <a href="https://jamesclear.com/atomic-habits">Atomic Habits</a>:

In his book he mentioned a psychologist named <a href="https://en.wikipedia.org/wiki/Edward_Thorndike"><strong>Edward Thorndike</strong></a> who conducted an experiment to study the behavior of animals, and he started by working with cats. He would place each cat inside a device known as a puzzle box. The box was designed in such a way that the cat could escape through a door by some simple act such as pulling a loop of cord, pressing a lever, or stepping on a platform. Once the cat is able to open the door, it could dart out and run over to a bowl of food. In the beginning, the animals moved around the box at random trying to find a way out. But as soon as the lever had been pressed and the door opened, the process of learning began. Gradually, each cat learned to associate the action of pressing the lever to escape. The more trials he made the less time it took the cats to escape. From his studies, Thorndike described the learning process by stating "behaviors followed by satisfying consequences (rewards) tend to be repeated and those that produce unpleasant consequences (punishments) are less likely to be repeated".

Let's try to formalize the above example. The problem being solved in this example is <strong>opening the box. </strong>Where the cat here is an <strong>agent</strong> trying to manipulate the<strong> environment</strong>(which is the box) by taking <strong>actions</strong> like sticking their paws through openings, poking their nose into the corners etc and tries to go from one<strong> state</strong> (each movement it takes) to another. The cats get a <strong>reward</strong> (food) when it accomplishes the task of opening the box and would not be able to get to the food (<strong>punishment</strong>)when it's unable to open the box. This is a simplified description of reinforcement learning.

[caption id="attachment_1924" align="aligncenter" width="676"]<img class="wp-image-1924 size-large" src="http://categitau.com/wp-content/uploads/2018/11/reinforcement-learning-fig1-700-700x270.jpg" alt="" width="676" height="261" /> Figure1 : The structure of Reinforcement Learning[/caption]

&nbsp;
<h1><b>Deep Learning(DL)</b></h1>
[caption id="attachment_1925" align="alignleft" width="298"]<img class="wp-image-1925 size-full" src="http://categitau.com/wp-content/uploads/2018/11/images.jpeg" alt="" width="298" height="169" /> Figure 2: Multi-layered Neural Networks in Deep Learning[/caption]

We can’t talk about Reinforcement Learning without getting into Deep Learning. DL is defined as a general-purpose framework for representation learning. An agent , given an objective, learns from some representations that achieve the objective using minimal domain knowledge. It allows us to train an agent to predict outputs, given a set of inputs. For example, you might train a deep learning algorithm to recognize cats on a photograph. You would do that by feeding it millions of images that either contain cats or not. The program then establishes patterns by classifying and clustering the image data. These patterns will then inform a predictive model that is able to look at a new set of images and predict whether they contain cats or not based on the model that was created using training data.

Deep Learning algorithms do this through multilayered neural networks which mimic the network of neurons in our brain. Each layer would process something different like detecting the eyes of the cat, the other layers detects the shape of the nose and so on.

This is different from RL which is an autonomous, self-teaching system that essentially learns by trial and error and not from inputs.

&nbsp;
<h1><b>Deep Reinforcement Learning</b></h1>
Deep RL is a combination of both RL and DL, using Q-Learning as its base. <strong>Q-</strong><strong>learning</strong> is one of the primary reinforcement learning methods which does not assume that the agent knows anything about the state-transition and reward models. However, the agent will discover what are the good and bad actions by trial and error. In RL, the agent chooses its actions by consulting its mental model, let's call it its <a href="https://itnext.io/reinforcement-learning-with-q-tables-5f11168862c8">Q-table</a>. This is a simple lookup process that answers the question: "When I am in &lt;some state&gt;, what is the best action to take?". Deep Learning comes into play when the environments where the state-action space is so large and it no longer becomes feasible to store all (state-action). So, what you can do is create a neural network that e.g predicts the reward for an input(state, action). So what you have is a Neural Network that predicts the Q-value(<em><span class="st">value over state-action pairs</span></em>), based on the input(state, action). This way is more tractable than storing every possible state-action.
<h1><b>Deep Reinforcement Learning in Practice</b></h1>
Deep Reinforcement Learning uses neural networks to represent the following:
<ul>
 	<li><b>Value function(</b>checks how good a state is) - Represents how good is a state for an agent to be in. It is equal to expected total reward for an agent starting from state <em>s</em>. The value function depends on the policy by which the agent picks actions to perform.</li>
 	<li><b>Policy(</b>The way an agent chooses an action) - The way by which the agent chooses which action to perform is named the agent <b>policy</b> which is a function that takes the current environment state to return an action.</li>
 	<li><b>Model - </b>Model is of course the model that's used to train the agent.</li>
</ul>
The choice of Optimization Algorithms and Loss Functions for a deep learning model plays a big role in producing optimum and faster results.

<b>Optimizing loss function - </b>In most learning networks, error is calculated as the difference between the actual output and the predicted output. The function that is used to compute this error is known as Loss Function. For accurate predictions, one needs to minimize the calculated error. In a neural network, this is done using back propagation. The current error is typically propagated backwards to a previous layer, where it is used to modify the weights and bias in such a way that the error is minimized. The weights are modified using a function called Optimization Function. Thus, <b>loss functions</b> are helpful to train a neural network. Given an input and a target, they calculate the loss, i.e difference between output and target variable.

In recent years, we’ve seen a lot of improvements in this fascinating area of research. The following are some of the successes of Reinforcement Learning.
<h2><b>Success story #1: TD-Gammon</b></h2>
[caption id="attachment_1928" align="aligncenter" width="428"]<img class="wp-image-1928 " src="http://categitau.com/wp-content/uploads/2018/11/12-Figure2-1-700x936.png" alt="" width="428" height="572" /> Figure 3: Illustration of TD Gammon's Neural Network[/caption]

<a href="https://en.wikipedia.org/wiki/TD-Gammon">TD-Gammon</a> is a game learning program consisting of a neural network that is able to teach itself to play backgammon solely by playing against itself and learning from the results. TD-Gammon consists of a simple three-layer neural network trained using a reinforcement learning technique known as<a href="https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node74.html"> TD-Lambda</a> or <a href="https://en.wikipedia.org/wiki/Temporal_difference_learning"><strong>Temporal Difference Learning</strong></a>. The neural network acts as a “value function” which predicts the value, or <i>reward</i>, of a particular state of the game for the current player. During training, the neural network iterates over all possible moves for the current player and evaluates each valid move and the move with the highest value is selected. Because the network evaluates moves for both players, it’s effectively playing against itself. Although TD-Gammon has greatly surpassed all previous computer programs in its ability to play backgammon, that was not why it was developed. Rather, its purpose was to explore some exciting new ideas and approaches to traditional problems in the field of reinforcement learning. You can read more about it <a href="http://www.bkgm.com/articles/tesauro/tdl.html">here</a>.
<h2><b>Success story #2: DQN in Atari</b></h2>
[caption id="attachment_1930" align="aligncenter" width="676"]<img class="wp-image-1930 size-large" src="http://categitau.com/wp-content/uploads/2018/11/rl-700x505.png" alt="" width="676" height="488" /> Figure 4:Structure of Deep Reinforcement Learning in Atari Games[/caption]

Deep Q-Network (DQN) is the first deep reinforcement learning method proposed by DeepMind and used in <a href="https://en.wikipedia.org/wiki/Atari_Games">Atari games</a>. These are the video games we used to play before play stations and Xbox came through :). The <strong>State</strong> is the current situation that the agent(your program) is in. Which is the current frame in your Atari game. An action is a command that you can give in the game in the hope of reaching a certain state and reward. In the case of Atari games, actions are all sent via the joystick. Rewards are given after performing an action, and are normally a function of your starting state, the action you performed, and your end state. The goal of your reinforcement learning program is to <i>maximize long term rewards</i>. In the case of Atari, rewards simply correspond to <i>changes in score.</i>

&nbsp;

https://youtu.be/p4Kem0wQoHs

&nbsp;

In late 2013, DeepMind achieved a breakthrough in the world of reinforcement learning: using deep reinforcement learning, they implemented a system that could learn to play many classic Atari games with human (and sometimes superhuman) performance. The computer program has never seen this game before and does not know the rules. It learns by deep reinforcement learning to maximize its score given only the pixels and game score as the input. You can read more about this in the following paper by DeepMind:<a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"> Playing Atari with Deep Reinforcement Learning</a>. There's also an article that I stumbled onto on how to Build your own deep reinforcement learning program that plays the Atari game which you can get <a href="https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26">here</a>.

<strong>Deep Q Network</strong> can also be used to model the probability that one user may click on one specific piece of news. Under the setting of reinforcement learning, the probability for a user to click on a piece of news (and future recommended news) is essentially the reward that our agent can get.
<h2></h2>
<h2><b>Success story #3: Deep RL in Robotics</b></h2>
https://youtu.be/hx_bgoTF7bs

https://youtu.be/jwSbzNHGflM
<h2><b>Success story #4a: Alpha Go</b></h2>
[caption id="attachment_1933" align="aligncenter" width="602"]<img class="wp-image-1933 size-full" src="http://categitau.com/wp-content/uploads/2018/11/main-qimg-525c3d41f153eb29ef439364cdb3ac3a.png" alt="" width="602" height="600" /> Figure 5:Board game Go[/caption]

Alpha Go is a computer system developed by Google <a href="https://en.wikipedia.org/wiki/DeepMind">DeepMind</a> that can play the game Go. The game of Go starts with an empty board. Each player has an effectively unlimited supply of pieces (called <b>stones</b>), one taking the black stones, the other taking white. The main objective of the game is to use your stones to form territories by surrounding vacant areas of the board. It is also possible to capture your opponent's stones by completely surrounding them. AlphaGo is the first computer program to defeat a world champion at Go. <b>Google DeepMind's Challenge Match</b>, was a five-game Go match between 18-time world champion<a href="https://en.wikipedia.org/wiki/Lee_Sedol"> Lee Sedol</a> and<a href="https://en.wikipedia.org/wiki/AlphaGo"> AlphaGo</a> played in<a href="https://en.wikipedia.org/wiki/Seoul"> Seoul</a>, South Korea between 9 and 15 March 2016. AlphaGo won all but the fourth game.(There's still hope for humanity). DeepMind went ahead and created an <a href="https://www.youtube.com/watch?v=8tq1C8spV_g&amp;feature=youtu.be">Alpha Go movie</a> based on the game it played against the Go world champion Lee Sedol. Everyone should watch!

[caption id="attachment_1940" align="aligncenter" width="676"]<img class="wp-image-1940 size-large" src="http://categitau.com/wp-content/uploads/2018/11/Screenshot-from-2018-11-16-14-12-22-700x355.png" alt="" width="676" height="343" /> Figure 6: Training AlphaGo[/caption]

Alpha Go utilized two deep neural networks:
<ul>
 	<li><strong>Policy Network(output move probabilities)</strong> - The policy network was first trained using <strong>Supervised Learning</strong> to accurately predict human expert moves and was subsequently refined by policy-gradient reinforcement learning. While the Supervised Learning policy network is good in predicting the most likely moves, Reinforcement Learning helps with the prediction of the best possible winning moves.</li>
 	<li><strong>Value Network(outputs a position evaluation)</strong>  - This is the final stage of training which involves estimating the probability that the current move leads to a win</li>
</ul>
<h2></h2>
<h2><b>Success story 4b: Alpha Zero</b></h2>
This is the latest evolution of Alpha Go. Alpha Zero is more powerful and is arguably the strongest Go player in history. The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs(Engines) are based on a combination of sophisticated search techniques with expert domain knowledge that have been refined by human experts over several decades. Alpha Zero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case. Previous versions of Alpha Go like I had stated, were initially trained on thousands of human amateur and professional games to learn how to play the game Go. AlphaGo Zero, skips the Supervised Learning step and learns to play simply by playing against itself, starting completely from random play.

https://youtu.be/4Sm922Xp5N4

&nbsp;
<h2><b>Success story #5:  Dota2</b></h2>
<a href="http://blog.dota2.com/">Dota 2</a> is a multiplayer online battle arena video game. It is played in matches between two teams of five players, with each team occupying and defending their own separate base on the map. Each of the ten players independently control a powerful character, known as a "hero", who all have unique abilities and differing styles of play. During a match, players collect experience points and items for their heroes to successfully defeat the opposing team's heroes in player versus player combat. A team wins by being the first to destroy a large structure located in the opposing team's base, called the "Ancient".

<strong>Open AI Five </strong>is a team of 5 neural networks that has started defeating amateur human teams at Dota2. The program defeated a human in 2017 and lost to a professional human team in 2018.
<h3>Conclusion</h3>
If you've made it this far congratulations! This article has introduced you to to the world of Reinforcement Learning and its application in real world. There are a number of other applications that I haven't mentioned like how Deep Mind was able to create a program that has achieved human-level performance at the game<a href="https://deepmind.com/research/publications/capture-the-flag/"> <strong>Capture the Flag</strong></a>. RL has also been used in chemistry, where it uses deep learning to plan the chemical synthesis of organic molecules, e.g. drugs, agro-chemicals etc. There's a whole paper on this, you can find it <a href="https://arxiv.org/pdf/1702.00020.pdf">here</a>. As you can see RL can also be applied in other areas other than games. Let me know if you can think of other ways RL can be applied in real world, other than in games.
<h3>Resources</h3>
DeepMind's Website - <a href="https://deepmind.com/">https://deepmind.com/</a>

Deep Reinforcement Learning Demystified -<a href="https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa"> https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa</a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1917</wp:post_id>
		<wp:post_date><![CDATA[2018-11-16 16:38:13]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-11-16 13:38:13]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[success-stories-of-reinforcement-learning]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_skip_18590466]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_skip_18590462]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[attitude_sidebarlayout]]></wp:meta_key>
		<wp:meta_value><![CDATA[default]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[1932]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
		<wp:meta_value><![CDATA[@categitau_]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1583710045;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:2385;}i:1;a:1:{s:2:"id";i:1835;}i:2;a:1:{s:2:"id";i:2403;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_mess]]></wp:meta_key>
		<wp:meta_value><![CDATA[Learn how Robots will take over the world...lol JK.  Here's  my post on the success stories of Reinforcement Learning. Based on the  presentation David Silver made during DL Indaba  2018 in Stellenbosch, SA .#DataScience #DeepLearning]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_87d3f1a9102da6ed04c9215a2b6c875a]]></wp:meta_key>
		<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_34b757491ebe949a199d34c3e117894a]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="500" height="375" src="https://www.youtube.com/embed/p4Kem0wQoHs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_34b757491ebe949a199d34c3e117894a]]></wp:meta_key>
		<wp:meta_value><![CDATA[1544083821]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_ea79f781ab5b64031c87c2d2266e241e]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="500" height="375" src="https://www.youtube.com/embed/hx_bgoTF7bs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_ea79f781ab5b64031c87c2d2266e241e]]></wp:meta_key>
		<wp:meta_value><![CDATA[1544083821]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_cced89813315923241a78361e0c08727]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="500" height="281" src="https://www.youtube.com/embed/jwSbzNHGflM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_cced89813315923241a78361e0c08727]]></wp:meta_key>
		<wp:meta_value><![CDATA[1544083821]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_332441a219c61ea8eaca445ce394fdc1]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="500" height="281" src="https://www.youtube.com/embed/4Sm922Xp5N4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_332441a219c61ea8eaca445ce394fdc1]]></wp:meta_key>
		<wp:meta_value><![CDATA[1544083821]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_6896b7b3a987963a8c004712e15d6c77]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="676" height="507" src="https://www.youtube.com/embed/p4Kem0wQoHs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_6896b7b3a987963a8c004712e15d6c77]]></wp:meta_key>
		<wp:meta_value><![CDATA[1544084078]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_a428a0246feb3f1c91a05ee3eabf68f7]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="676" height="507" src="https://www.youtube.com/embed/hx_bgoTF7bs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_a428a0246feb3f1c91a05ee3eabf68f7]]></wp:meta_key>
		<wp:meta_value><![CDATA[1544084078]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_670741f3d4d858e144807a5100965fcd]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="676" height="380" src="https://www.youtube.com/embed/jwSbzNHGflM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_670741f3d4d858e144807a5100965fcd]]></wp:meta_key>
		<wp:meta_value><![CDATA[1544084078]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_5849f348702846202462abe01f7d331f]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="676" height="380" src="https://www.youtube.com/embed/4Sm922Xp5N4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_5849f348702846202462abe01f7d331f]]></wp:meta_key>
		<wp:meta_value><![CDATA[1544084078]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_73ee32f183de71c98500eb6823a38d1d]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="525" height="394" src="https://www.youtube.com/embed/p4Kem0wQoHs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_a1c6d8053fc26a7ef318a12c51883516]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="640" height="480" src="https://www.youtube.com/embed/p4Kem0wQoHs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_a1c6d8053fc26a7ef318a12c51883516]]></wp:meta_key>
		<wp:meta_value><![CDATA[1544089305]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_5879cb32fc9eb2b05702c2df9bd56ddb]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="640" height="480" src="https://www.youtube.com/embed/hx_bgoTF7bs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_5879cb32fc9eb2b05702c2df9bd56ddb]]></wp:meta_key>
		<wp:meta_value><![CDATA[1544089305]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_90dd0fee71b0e8fd6901d1f801dff8b0]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="640" height="360" src="https://www.youtube.com/embed/jwSbzNHGflM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_90dd0fee71b0e8fd6901d1f801dff8b0]]></wp:meta_key>
		<wp:meta_value><![CDATA[1544089305]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_a27b440ba1aa4d19b05c1a6093c301ad]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="640" height="360" src="https://www.youtube.com/embed/4Sm922Xp5N4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_a27b440ba1aa4d19b05c1a6093c301ad]]></wp:meta_key>
		<wp:meta_value><![CDATA[1544089305]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_73ee32f183de71c98500eb6823a38d1d]]></wp:meta_key>
		<wp:meta_value><![CDATA[1544094509]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_f8b984333fbe70d9e0d2ac0a98f22d15]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="525" height="394" src="https://www.youtube.com/embed/hx_bgoTF7bs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_f8b984333fbe70d9e0d2ac0a98f22d15]]></wp:meta_key>
		<wp:meta_value><![CDATA[1544094509]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_91903fa2faad7fa15e14968b29e8705f]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="525" height="295" src="https://www.youtube.com/embed/jwSbzNHGflM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_91903fa2faad7fa15e14968b29e8705f]]></wp:meta_key>
		<wp:meta_value><![CDATA[1544094509]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_8294ec90f842dc5f285a04c40e6b31f3]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="525" height="295" src="https://www.youtube.com/embed/4Sm922Xp5N4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_8294ec90f842dc5f285a04c40e6b31f3]]></wp:meta_key>
		<wp:meta_value><![CDATA[1544094509]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_totes]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_pinterest_image_url]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[swp_cache_timestamp]]></wp:meta_key>
		<wp:meta_value><![CDATA[431878]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_4989143857292511a7d645e0eb40a997]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="980" height="735" src="https://www.youtube.com/embed/p4Kem0wQoHs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_4989143857292511a7d645e0eb40a997]]></wp:meta_key>
		<wp:meta_value><![CDATA[1553898441]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_050378010129f8bdd1fdfc12a537dee3]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="980" height="735" src="https://www.youtube.com/embed/hx_bgoTF7bs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_050378010129f8bdd1fdfc12a537dee3]]></wp:meta_key>
		<wp:meta_value><![CDATA[1553898441]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_e5f47e3d14e8b62470512f030a4cebd4]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="980" height="551" src="https://www.youtube.com/embed/jwSbzNHGflM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_e5f47e3d14e8b62470512f030a4cebd4]]></wp:meta_key>
		<wp:meta_value><![CDATA[1553898441]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_230ca249eadf075f894e7c884c06a52c]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="980" height="551" src="https://www.youtube.com/embed/4Sm922Xp5N4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_230ca249eadf075f894e7c884c06a52c]]></wp:meta_key>
		<wp:meta_value><![CDATA[1553898441]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_total_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_7cb798b4abf81b11e1a78afa992be6b9]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="700" height="525" src="https://www.youtube.com/embed/p4Kem0wQoHs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_7cb798b4abf81b11e1a78afa992be6b9]]></wp:meta_key>
		<wp:meta_value><![CDATA[1553898770]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_bd7b658c7f5d6db716fe515c8ed3ffdb]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="700" height="525" src="https://www.youtube.com/embed/hx_bgoTF7bs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_bd7b658c7f5d6db716fe515c8ed3ffdb]]></wp:meta_key>
		<wp:meta_value><![CDATA[1553898770]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_e19dda0abf47ce65e4d8d6641a5f83d2]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="700" height="394" src="https://www.youtube.com/embed/jwSbzNHGflM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_e19dda0abf47ce65e4d8d6641a5f83d2]]></wp:meta_key>
		<wp:meta_value><![CDATA[1553898770]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_bc3d655b99c57e8b36e838dcd7b03b8d]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="700" height="394" src="https://www.youtube.com/embed/4Sm922Xp5N4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_bc3d655b99c57e8b36e838dcd7b03b8d]]></wp:meta_key>
		<wp:meta_value><![CDATA[1553898770]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_facebook_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_pinterest_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_twitter_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_linkedin_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_google_plus_shares]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_225b599f513dffaed153abb67212eda4]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="840" height="630" src="https://www.youtube.com/embed/p4Kem0wQoHs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_225b599f513dffaed153abb67212eda4]]></wp:meta_key>
		<wp:meta_value><![CDATA[1554763806]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_a6115dd58c3f00dd587428720fd7f90c]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="840" height="630" src="https://www.youtube.com/embed/hx_bgoTF7bs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_a6115dd58c3f00dd587428720fd7f90c]]></wp:meta_key>
		<wp:meta_value><![CDATA[1554763806]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_d33a5fc8c4ebb83bee9c57d3d9b10d5b]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="840" height="473" src="https://www.youtube.com/embed/jwSbzNHGflM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_d33a5fc8c4ebb83bee9c57d3d9b10d5b]]></wp:meta_key>
		<wp:meta_value><![CDATA[1554763807]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_7c6b904778e857f76be0ecd32717a294]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="840" height="473" src="https://www.youtube.com/embed/4Sm922Xp5N4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_7c6b904778e857f76be0ecd32717a294]]></wp:meta_key>
		<wp:meta_value><![CDATA[1554763807]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_22123821973a9d4078d83033a071d74a]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="660" height="495" src="https://www.youtube.com/embed/p4Kem0wQoHs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_22123821973a9d4078d83033a071d74a]]></wp:meta_key>
		<wp:meta_value><![CDATA[1554763917]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_5518cd6357703970e687450970ccef00]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="660" height="495" src="https://www.youtube.com/embed/hx_bgoTF7bs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_5518cd6357703970e687450970ccef00]]></wp:meta_key>
		<wp:meta_value><![CDATA[1554763917]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_a43d898f1fad9a2b9cd483ad015b770b]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="660" height="371" src="https://www.youtube.com/embed/jwSbzNHGflM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_a43d898f1fad9a2b9cd483ad015b770b]]></wp:meta_key>
		<wp:meta_value><![CDATA[1554763917]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_b094569634cf173689cf764bd13cde70]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="660" height="371" src="https://www.youtube.com/embed/4Sm922Xp5N4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_b094569634cf173689cf764bd13cde70]]></wp:meta_key>
		<wp:meta_value><![CDATA[1554763917]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_cea1642c6145e3f35f949c575bdbc5d0]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="760" height="570" src="https://www.youtube.com/embed/p4Kem0wQoHs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_cea1642c6145e3f35f949c575bdbc5d0]]></wp:meta_key>
		<wp:meta_value><![CDATA[1554764033]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_189baa97f8154262c388f29d9c300812]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="760" height="570" src="https://www.youtube.com/embed/hx_bgoTF7bs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_189baa97f8154262c388f29d9c300812]]></wp:meta_key>
		<wp:meta_value><![CDATA[1554764033]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_aa36e411e8d3541981b60aab7f77a19f]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="760" height="428" src="https://www.youtube.com/embed/jwSbzNHGflM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_aa36e411e8d3541981b60aab7f77a19f]]></wp:meta_key>
		<wp:meta_value><![CDATA[1554764033]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_eda1525cc7c6133b7cb78d2cfb7b7aac]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe width="760" height="428" src="https://www.youtube.com/embed/4Sm922Xp5N4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_eda1525cc7c6133b7cb78d2cfb7b7aac]]></wp:meta_key>
		<wp:meta_value><![CDATA[1554764033]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>1911</wp:comment_id>
			<wp:comment_author><![CDATA[Introduction To Machine Learning &#8211; Catherine Gitau]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>https://categitau.com/foundations-of-machine-learning-part-1/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[192.185.33.134]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2020-01-12 03:53:43]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2020-01-12 00:53:43]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] In reinforcement learning on the other hand, we start out with no training/input data and our agent learns what to do based on the environment and how to map certain situations into actions in order to maximize reward. I&#8217;ve written a blog post on the same with some examples of how RL is being utilized in this post. [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[a:3:{s:4:"time";d:1578928124.465158939361572265625;s:5:"event";s:15:"status-approved";s:4:"user";s:9:"categitau";}]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[a:2:{s:4:"time";d:1578790423.57983493804931640625;s:5:"event";s:9:"check-ham";}]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>475</wp:comment_id>
			<wp:comment_author><![CDATA[Ian]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[kosenian@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[196.23.154.85]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2018-11-19 10:43:55]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2018-11-19 07:43:55]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Alpha Zero- soo coool]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[a:3:{s:4:"time";d:1542887487.2950038909912109375;s:5:"event";s:15:"status-approved";s:4:"user";s:9:"categitau";}]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[a:3:{s:4:"time";d:1542613435.208198070526123046875;s:5:"event";s:11:"check-error";s:4:"meta";a:1:{s:8:"response";s:7:"invalid";}}]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Road to Becoming a Visible Expert</title>
		<link>https://categitau.com/road-to-becoming-a-visible-expert/</link>
		<pubDate>Tue, 24 Dec 2019 13:50:56 +0000</pubDate>
		<dc:creator><![CDATA[categitau]]></dc:creator>
		<guid isPermaLink="false">http://categitau.com/?p=2292</guid>
		<description></description>
		<content:encoded><![CDATA[It's been a long time coming but I finally decided to get back to writing. Great to have you reading my first post in 2019! Like some wise man once said; 'better late than never'. This past year's been full of ups and downs and a lot of learning and traveling involved. Just a quick recap of what I have been up-to before I go on:
<ul>
 	<li><!--more-->Attended an in-class <a href="https://www.fast.ai/">Fast.ai</a> Deep Learning course at the University of San Francisco from March 2019 - May 2019.</li>
 	<li>Attended <a href="http://www.user2019.fr/">UseR Conference</a> in Toulouse, France 2019. <strong>(call for diversity scholarships is now open. <a href="https://user2020.r-project.org/news/2019/11/18/call-for-diversity-applications/">Apply!</a>)</strong></li>
 	<li>Attended the<a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=2ahUKEwja-tHCo4rmAhVI6RoKHShABFAQFjAAegQIBhAC&amp;url=http%3A%2F%2Fwww.deeplearningindaba.com%2F&amp;usg=AOvVaw0mnWHf2tpUdlBYWjqhwZqG"> Deep Learning Indaba</a>. Nairobi, Kenya 2019.</li>
 	<li>Speaker ACCA Power of Digital Round Table(Machine Learning). Nairobi, Kenya.</li>
 	<li>Currently a Student at the <a href="https://aimsammi.org/">African Masters in Machine Intelligence(AMMI)</a> in AIMS Ghana.</li>
</ul>
Due to my recent admission to the African Masters in Machine Intelligence, I've realized that there's more to machine learning than what I knew before.  I came to then learn that there are actually two types of data scientists; Those that are machine learning enthusiasts that don't really know what happens behind the scenes, and those that know the mathematics behind the machine learning algorithms. I'd say, I was mostly the former and I'm sure most people are and I cannot emphasize more on the importance of knowing what goes on behind all those <a href="https://scikit-learn.org/stable/">sklearn</a> libraries that you import.

&nbsp;

<img class="aligncenter wp-image-2294 size-large" src="https://categitau.com/wp-content/uploads/2019/11/mlmeme-700x719.jpeg" alt="" width="640" height="657" />

&nbsp;

If you didn't know; behind all those flashy machine learning products and applications, there's mathematics and algorithms that make them what they are. This is the main reason why I've decided to get back to writing so as to share the important concepts in machine learning as well as learn more by teaching others.

<img class="size-full wp-image-2295 aligncenter" src="https://categitau.com/wp-content/uploads/2019/11/mlmeme2.jpg" alt="" width="750" height="743" />

&nbsp;

Apart from that, I realized that I've just been taking it all in and not pushing out any work which makes it hard for me to measure my progress and show proof of work. I was also re-reading this book called <a href="https://hingemarketing.com/library/article/the-visible-expert">The Visible Expert</a>. This book gives you advice on how to become a well-known expert in your field by employing various methods to produce leads. These methods include creating content through blog posts, articles, videos, speaking engagements, networking, relationship building and focusing on target markets. If your plan is to become an expert in your field, make sure you get yourself a  copy of that book.

I'm sure you've noticed that most of the people well known for their work in machine learning or in any other field, have either written multiple papers, been invited to speak at several conferences and meets and are well known experts in a specific area be it NLP or Computer Vision etc. So, I recently made a decision to get a broad understanding about every area in machine learning but be an expert in one field which is Natural Language Processing. This decision came right after watching <a href="https://en.wikipedia.org/wiki/Andrew_Ng">Andrew Ng's</a> video on career advice and how to read research papers. which I found very useful and has some general advice for everyone and not necessarily for people in the machine learning field. If interested, find it<a href="https://www.youtube.com/watch?v=733m6qBH-jI&amp;list=PLs0O4aQGzbYCtF0glSExYFjlok-qDJ2kQ"> here</a>.

Throughout the next year, I will be writing posts on machine learning concepts that I learn along the way as well as get deeper into NLP. I'm not sure about how frequent I will be writing because of the intensity of my masters program but be sure to expect a post at least once or twice every month. My classmate also started a YouTube channel talking about various machine learning concepts and algorithms so if you're more of a video than article person, have a look at her channel <a href="https://www.youtube.com/watch?v=adQOlAgyga8">here</a> and subscribe!

Feel free to also subscribe to my website to get a notification when I put out a post, or follow my medium page <a href="https://medium.com/@categitau">@categitau</a>. I would also really appreciate feedback. It's been a minute since I last wrote, so my writing is a bit rusty but will improve with time.

Thank you for reading and Happy Holidays!

[caption id="attachment_2368" align="alignnone" width="480"]<img class="size-full wp-image-2368" src="https://categitau.com/wp-content/uploads/2019/12/giphy.gif" alt="" width="480" height="346" /> Source: https://giphy.com[/caption]

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2292</wp:post_id>
		<wp:post_date><![CDATA[2019-12-24 13:50:56]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2019-12-24 10:50:56]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[road-to-becoming-a-visible-expert]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>1</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[2372]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[force_ssl]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1583586642;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:141;}i:1;a:1:{s:2:"id";i:1719;}i:2;a:1:{s:2:"id";i:1917;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>1842</wp:comment_id>
			<wp:comment_author><![CDATA[Jean Paul Ishimwe]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[jishimwe@aimsammi.org]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[154.65.19.226]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2019-12-25 02:19:40]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2019-12-24 23:19:40]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[This is really interesting. Thanks for making it.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[a:3:{s:4:"time";d:1577229628.8929541110992431640625;s:5:"event";s:15:"status-approved";s:4:"user";s:9:"categitau";}]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[a:2:{s:4:"time";d:1577229580.3207919597625732421875;s:5:"event";s:9:"check-ham";}]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>1843</wp:comment_id>
			<wp:comment_author><![CDATA[Isz]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[iszmbuthia@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[154.154.192.38]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2019-12-25 13:49:23]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2019-12-25 10:49:23]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Machines may not be my forte but I sure do admire your writing and accomplishments..Will be here alot more often..]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[a:3:{s:4:"time";d:1577744824.3582999706268310546875;s:5:"event";s:15:"status-approved";s:4:"user";s:9:"categitau";}]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[a:2:{s:4:"time";d:1577270963.3754389286041259765625;s:5:"event";s:9:"check-ham";}]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>1879</wp:comment_id>
			<wp:comment_author><![CDATA[categitau]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[catherinegitau94@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[154.65.19.226]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2019-12-31 01:27:49]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2019-12-30 22:27:49]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thank you! You taught us well :)]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>1843</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[a:3:{s:4:"time";d:1577744869.1740601062774658203125;s:5:"event";s:9:"check-ham";s:4:"user";s:9:"categitau";}]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>1880</wp:comment_id>
			<wp:comment_author><![CDATA[categitau]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[catherinegitau94@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[154.65.19.226]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2019-12-31 01:28:12]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2019-12-30 22:28:12]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Welcome, come back for more!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>1842</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[a:3:{s:4:"time";d:1577744892.257688045501708984375;s:5:"event";s:9:"check-ham";s:4:"user";s:9:"categitau";}]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>1939</wp:comment_id>
			<wp:comment_author><![CDATA[Kelvin Kahuro]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[kelvinkahuro6@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[197.248.161.122]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2020-01-15 12:41:03]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2020-01-15 09:41:03]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Will be waiting for a lot more..Kudos]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[a:2:{s:4:"time";d:1579081263.84930706024169921875;s:5:"event";s:9:"check-ham";}]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[a:3:{s:4:"time";d:1579086545.2811710834503173828125;s:5:"event";s:15:"status-approved";s:4:"user";s:9:"categitau";}]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Introduction To Machine Learning</title>
		<link>https://categitau.com/foundations-of-machine-learning-part-1/</link>
		<pubDate>Sat, 04 Jan 2020 02:01:40 +0000</pubDate>
		<dc:creator><![CDATA[categitau]]></dc:creator>
		<guid isPermaLink="false">http://categitau.com/?p=2385</guid>
		<description></description>
		<content:encoded><![CDATA[<!-- wp:paragraph -->
<p>This article is part of a series that I have decided to put up so as to help myself and others have a deeper understanding on the foundations of machine learning and move from just having a shallow idea about what ML is to being confident enough to have smart ML conversations when attending conferences and not feel some impostor syndrome creep in (speaking from experience) among other things. </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>And trust, if you've got the foundations on your finger tips, everything else will be so much easier to understand. This post is meant to give a general introduction to the whole concept of machine learning then later get deeper into each topic in the other parts. </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>I feel like I have revisited this introduction to machine learning a lot of times but I wanted structure in this series. So, I will link my other posts if you would like some more information about that particular topic. If you're already familiar with what machine learning is, please feel free to skip through, but you never know, you might learn a thing or two that you didn't know before. </p>
<!-- /wp:paragraph -->

<!-- wp:heading -->
<h2>So, what is Machine Learning?</h2>
<!-- /wp:heading -->

<!-- wp:quote -->
<blockquote class="wp-block-quote"><p><strong>Machine learning</strong> (<strong>ML</strong>) is the <a href="https://en.wikipedia.org/wiki/Branches_of_science">scientific study</a> of <a href="https://en.wikipedia.org/wiki/Algorithm">algorithms</a> and <a href="https://en.wikipedia.org/wiki/Statistical_model">statistical models</a> that <a href="https://en.wikipedia.org/wiki/Computer_systems">computer systems</a> use to perform a specific task without using explicit instructions, relying on patterns and <a href="https://en.wikipedia.org/wiki/Inference">inference</a> instead.</p><cite>Wikipedia</cite></blockquote>
<!-- /wp:quote -->

<!-- wp:paragraph -->
<p>Breaking this down, machine learning is the study of building generic(generalized) algorithms and statistical models that perform some certain task e.g. predicting the prices of things, using patterns gotten from a set of data without having to write specific code or instructions based on just that one specific problem. </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>For example, let's say we have a classification problem that we want to solve using a classification algorithm. This classification algorithm can be used to solve multiple classification problems such as predicting whether an email is spam or not, or whether a certain image is of a cat or a dog without having to change anything in the code. This one algorithm is the same but you use different data that is fed to the algorithm so that it can learn from it. Machine Learning is just a general term that is used to cover all these generic algorithms. If you don't know what a classification algorithm is, just read on and revisit this example when done.</p>
<!-- /wp:paragraph -->

<!-- wp:heading -->
<h2>Kinds of Machine Learning Algorithms</h2>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>Machine Learning can be broken down into three kinds of algorithms; <strong>supervised learning, unsupervised learning </strong>and<strong> reinforcement Learning</strong>. </p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->
<h3>Supervised Learning</h3>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>Let's say we work at a bank and someone just walked in and is requesting for a loan. Most traditional banks would look at several factors like previous track record of the person for repaying debts or credit history, one's income, capital etc. All these factors will influence whether or not this person gets a loan. If we've been working at the bank for several years it would be easy for us to just look at their profile and decide to give loan or not based on previous patterns we've noticed. But let's say you get an intern, the intern will have to learn previous patterns so as to be able to correctly classify whether one should get a loan or not. That's basically what we're training the computers to do.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>The computers learn this by giving it a set of data that it learns from. This data contains the input features as well as whether a person received a loan or not (output features). So that if a new customer comes in requesting for a loan, it can easily predict the likelihood of that person paying the loan based on the previous data or patterns from the data that was fed to it.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>This process of learning that consists of the output feature/dependent variables(whether a person will pay loan or not), which is to be predicted from a given set of independent variables(credit history, age, capital, income, etc) is known as <strong><a href="https://en.wikipedia.org/wiki/Supervised_learning">supervised learning.</a></strong></p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>Supervised learning can then be divided into two algorithms;<strong> classification </strong>and <strong>regression</strong> algorithms.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p><strong>Classification</strong> - The example I gave above of predicting whether or not a person should get a loan is an example of a classification algorithm. It is used to predict which class a data point belongs to, which is usually a discrete value. Some other examples include: </p>
<!-- /wp:paragraph -->

<!-- wp:list -->
<ul><li>Predicting whether an email is spam or not</li><li>Predicting whether it's going to rain or not</li><li>Customer segmentation - classifying your customers into different groups</li><li>Predicting whether a review is either positive, negative or neutral</li></ul>
<!-- /wp:list -->

<!-- wp:paragraph -->
<p>Note that a classification problem requires the examples to be classified into one of two or more classes and can be divided further into binary-classification problem, multi-class classification problem and multi-label classification problem. I've written a comprehensive introduction to supervised learning explaining these in a previous post which you can find <a href="https://categitau.com/introduction-to-supervised-learning/">here</a></p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p><strong>Regression</strong> - Let's say in our previous example we were trying to predict the amount to give out as loan instead. This problem then becomes a regression problem since we are now trying to predict a continuous quantity which in this case is amount. So, where a classification algorithm can be used to determine whether or not it will rain tomorrow, a regression algorithm will be used to predict the amount of rainfall. Some examples include:</p>
<!-- /wp:paragraph -->

<!-- wp:list -->
<ul><li>Predicting the price of a house</li><li>Predicting the sales of a particular product</li><li>Salary estimation</li></ul>
<!-- /wp:list -->

<!-- wp:heading {"level":3} -->
<h3>Unsupervised Learning</h3>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>Let's now say we didn't have information about whether previous clients defaulted on paying a loan or not. We can however discover hidden relationships in the data that will help us put our customers into groups. Unsupervised machine learning methods are those that don't make explicit outcome predictions, but instead find hidden relationships in the data.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>This method has various examples such as:</p>
<!-- /wp:paragraph -->

<!-- wp:list -->
<ul><li>Can be used in customer segmentation</li><li>Fraud detection</li><li>Used in building recommendation systems</li></ul>
<!-- /wp:list -->

<!-- wp:paragraph -->
<p>I have previously written a post on unsupervised learning techniques, if you want some more examples you can read about it <a href="http://categitau.com/finding-groups-in-data-clustering-techniques/">here</a>. Although, I will still revisit most of those techniques in greater detail later in this series.</p>
<!-- /wp:paragraph -->

<!-- wp:heading {"level":3} -->
<h3>Reinforcement Learning</h3>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>In reinforcement learning on the other hand, we start out with no training/input data and our agent learns what to do based on the environment and how to map certain situations into actions in order to maximize reward. I've written a blog post on the same with some examples of how RL is being utilized in this <a href="https://categitau.com/success-stories-of-reinforcement-learning/">post</a>. </p>
<!-- /wp:paragraph -->

<!-- wp:heading -->
<h2>Conclusion</h2>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>Now that we have an idea about what machine learning is and what it's broadly classified into, in the next post, we'll get deeper into supervised learning, the various algorithms used and how we get to train a supervised machine learning model. Make sure to reach out to me <a href="https://categitau.com/contact-me/">here</a> if you have any questions, or follow me on  <a href="https://twitter.com/categitau_">@categitau_</a> or on <a href="https://www.linkedin.com/in/cate-gitau/">linkedIn</a> . Till next time :)  </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p></p>
<!-- /wp:paragraph -->]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2385</wp:post_id>
		<wp:post_date><![CDATA[2020-01-04 02:01:40]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2020-01-03 23:01:40]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[foundations-of-machine-learning-part-1]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="data-science"><![CDATA[Data Science]]></category>
		<category domain="category" nicename="machine-learning"><![CDATA[Machine Learning]]></category>
		<category domain="post_tag" nicename="machine-learning"><![CDATA[machine learning]]></category>
		<category domain="category" nicename="supervised-learning"><![CDATA[supervised learning]]></category>
		<category domain="post_tag" nicename="supervised-learning"><![CDATA[supervised learning]]></category>
		<category domain="post_tag" nicename="unsupervised-learning"><![CDATA[unsupervised Learning]]></category>
		<category domain="category" nicename="unsupervised-learning"><![CDATA[unsupervised learning]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[2445]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[force_ssl_children]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[force_ssl]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1583695122;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:2403;}i:1;a:1:{s:2:"id";i:1774;}i:2;a:1:{s:2:"id";i:1115;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
		<wp:meta_value><![CDATA[@categitau_]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[spay_email]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_mess]]></wp:meta_key>
		<wp:meta_value><![CDATA[I started a series on the foundations of machine learning: Here's part 1 where I give a gentle introduction to Machine Learning. Enjoy :) ]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>1921</wp:comment_id>
			<wp:comment_author><![CDATA[Deep dive into Supervised Learning &#8211; Catherine Gitau]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>https://categitau.com/deep-dive-into-supervised-learning/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[192.185.33.134]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2020-01-13 22:18:31]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2020-01-13 19:18:31]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Part1 &#8211; Introduction to Machine Learning [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[a:2:{s:4:"time";d:1578943111.1892430782318115234375;s:5:"event";s:9:"check-ham";}]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[a:3:{s:4:"time";d:1578945363.8464500904083251953125;s:5:"event";s:15:"status-approved";s:4:"user";s:9:"categitau";}]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Deep dive into Supervised Learning</title>
		<link>https://categitau.com/deep-dive-into-supervised-learning/</link>
		<pubDate>Mon, 13 Jan 2020 19:18:41 +0000</pubDate>
		<dc:creator><![CDATA[categitau]]></dc:creator>
		<guid isPermaLink="false">http://categitau.com/?p=2403</guid>
		<description></description>
		<content:encoded><![CDATA[<!-- wp:paragraph -->
<p>This is part 2 of my foundations of machine learning series. You can find the other articles listed below:</p>
<!-- /wp:paragraph -->

<!-- wp:list -->
<ul><li><a href="https://categitau.com/foundations-of-machine-learning-part-1/">Part1 - Introduction to Machine Learning</a></li></ul>
<!-- /wp:list -->

<!-- wp:heading -->
<h2>Machine Learning Process</h2>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>Anyone interested in the field of machine learning already has an overview of how the whole process looks like which usually involves:</p>
<!-- /wp:paragraph -->

<!-- wp:list -->
<ul><li><strong>Data collection </strong> - Collecting data that's going to be used to train the model.</li><li><strong>Data pre-processing</strong> - Data cleaning and validation, dealing with missing data, formatting and placing the data into the optimal format and extracting new features in the data.</li><li><strong>Exploratory analysis</strong> - Understanding how the data looks like and if there are any interesting patterns within.</li><li><strong>Training of the model</strong> - Learning takes place</li><li><strong>Evaluation of the model</strong> - Testing the models performance.</li><li><strong>Tuning </strong>- Fine tuning the model to maximize its performance.</li></ul>
<!-- /wp:list -->

<!-- wp:paragraph -->
<p>I will not go over the data collection, pre-processing and exploratory analysis stages. We will assume we already have the data and it's cleaned and already preprocessed. </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>In this article we're going to get a little deeper into supervised learning and try to understand how models actually learn and how we end up building a good model in the end. </p>
<!-- /wp:paragraph -->

<!-- wp:heading -->
<h2>Supervised Learning</h2>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p>In my previous post, I talked about supervised learning being the process of learning that consists of the output feature/dependent variable which is to be predicted from a given set of input features/independent variables.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>Suppose that I have data about houses which has the attributes: size in square feet and price as shown in the table below:</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":2472,"sizeSlug":"large"} -->
<figure class="wp-block-image size-large"><img src="https://categitau.com/wp-content/uploads/2020/01/blog_dia2.png" alt="" class="wp-image-2472"/></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>Each row [latex]{(i)}[/latex], represents a house. So, given the data above, we can build a program that learns to predict the price of other houses based on its size. The question now is, how? We will denote the size as our input variable <strong>x </strong>and the price as our output variable <strong>y</strong>. </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p><strong>[latex]{x^{(i)}}[/latex]</strong> and our output or target variable<strong> [latex]{y^{(i)}}[/latex]</strong>  are known as the <strong>training samples </strong>and the goal is to train a computer using these training samples so that it can learn some function <strong>[latex]{h}[/latex]</strong> that best maps an input <strong>[latex]{x^{(i)}}[/latex]</strong>  to the output <strong>[latex]{y^{(i)}}[/latex] </strong>such that, given a new set of inputs, the function is able to correctly predict the output. This function <strong>[latex]{h}[/latex]</strong> is called the <strong>hypothesis</strong> which is a rule that we come up with to help us predict the price of a house given its features. This hypothesis is usually unknown. The superscript [latex]{(i)}[/latex] here denotes each training sample/row or in this case each house with its attributes.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>Most of the time, we come up with the hypothesis by making an assumption about the data which is mostly done when we're exploring our data. If we draw out the relationship between the size of the house and price using the data that we have we end up with the graph below:</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":2471,"sizeSlug":"large"} -->
<figure class="wp-block-image size-large"><img src="https://categitau.com/wp-content/uploads/2020/01/blog_dia1.png" alt="" class="wp-image-2471"/></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>From the graph above, we can assume that our data flows in a linear form i.e as the size of the house increases the price also tends to increase and decide to use the <strong>linear regression algorithm </strong>as our hypothesis. These algorithms that make an assumption about how the data flows are known as<strong> parametric algorithms</strong>. <strong>Non-parametric</strong> algorithms are said to have no assumptions about the underlying data.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>Examples of parametric machine learning algorithms include:</p>
<!-- /wp:paragraph -->

<!-- wp:list -->
<ul><li>Linear Regression</li><li>Logistic Regression</li><li>Perceptron</li><li>Naive Bayes</li></ul>
<!-- /wp:list -->

<!-- wp:paragraph -->
<p>Examples of non-parametric machine learning algorithms include:</p>
<!-- /wp:paragraph -->

<!-- wp:list -->
<ul><li>k-Nearest Neighbors</li><li>Decision trees</li><li>Support Vector Machines</li></ul>
<!-- /wp:list -->

<!-- wp:paragraph -->
<p>Back to our example, we have approximated the output <strong>y</strong> (price) as a linear function of <strong>x</strong>(size) such that: <strong>[latex]  h_\theta(x) = \theta_0 + \theta_1x_1 [/latex] </strong></p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p><strong>[latex] h_\theta(x)[/latex] </strong>is our predicted <strong>y </strong>and <strong>[latex] \theta[/latex]</strong> are the model parameters.  This is similar to the linear function that takes the form y=mx + c. The model parameters are learned during the training phase.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>Identifying the <strong>hypothesis </strong>is usually the first step in building a model. </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>Next, we need to come up with a <strong>criteria </strong>which is also known as the <strong>cost/loss function </strong>which ideally tells us how well our model is performing. In this case, how well our model is able to approximate the true price of the house. This loss function is calculated by getting the difference between the true value of [latex] y^{(i)}[/latex] and the predicted value of [latex] y^{(i)}[/latex]. So, if the predictions deviate too much from actual results, the loss function will give a large value. There are different kinds of loss functions used depending on your problem which we will cover as we go through the various algorithms in the coming blog posts.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>The next step is the training phase where we use an optimization algorithm known as the <strong>learning algorithm. </strong>This learning algorithm is used to help find the value of the parameters [latex] \theta [/latex] of the function <strong>[latex]{h}[/latex]</strong> that minimize the criteria or loss function. This is where the learning takes place. The most popular optimization algorithm used is <strong>gradient descent</strong>. There's another set of parameters known as the <strong>hyperparameters. </strong>These are values that are specified by the user before starting training. This happens during the tuning phase to improve the performance of the model.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>The diagram below shows the overview of the whole process:</p>
<!-- /wp:paragraph -->

<!-- wp:image {"align":"center","id":2453,"sizeSlug":"large"} -->
<div class="wp-block-image"><figure class="aligncenter size-large"><img src="https://categitau.com/wp-content/uploads/2020/01/blog-1.png" alt="" class="wp-image-2453"/><figcaption>Source:http://cs229.stanford.edu/notes/cs229-notes1.pdf</figcaption></figure></div>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>In the example I have given above, we are trying to predict a continuous variable price. We call this a <strong>regression </strong>problem. If we were predicting a small number of discrete values such as type of house, we call this a <strong>classification </strong>problem.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>Building a good machine learning model is an iterative process and our main goal is to build a model that generalizes well on the unseen data. i.e how well the the model performs to new data that it has not seen. Generalization allows the model to make good predictions on data that it has not seen before. </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>When we talk about generalization, we often mention the two main causes of poor model performance which is <strong>overfitting </strong>and <strong>underfitting. </strong></p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p><strong>Overfitting </strong>refers to a model that has learnt the training data a bit too well and performs poorly on unseen data. I like using the example of a student that 'accidentally' comes across answers to an upcoming exam. This student then ends up memorizing the answers but unfortunately, the exam gets changed last minute. It's obvious that the said student will not perform well, since he had not studied the whole topic(didn't generalize) but rather decided to cram answers to a different exam. Same thing that happens when your model overfits. Models also tend to overfit when the complexity of the model is too high compared to the data that it's trying to learn from.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p><strong>Underfitting</strong> refers to a model that does not perform well on the training data as well as generalize on new data. This often happens when the model is too simple to learn the data that it is given as input.  The model is said to underfit when the complexity of the model is too less for it to learn the data that it's been given. </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>There are various methods that can be used to prevent your model from overfitting or underfitting which I will talk about in more detail in another post. </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>So far we have looked at the overall process of modeling, training and evaluating your model. In the upcoming posts, we will get our hands dirty with some of the popular algorithms used in machine learning and code them from scratch to understand exactly what happens behind the scenes, following the same exact steps we've discussed above.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>Feel like I've missed a step? Kindly share in the comments bellow. Till next time :) </p>
<!-- /wp:paragraph -->

<!-- wp:heading -->
<h2>References</h2>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p><a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf">Stanford CS229 Supervised Learning Lecture Notes</a>. </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p><a href="https://chemicalstatistician.wordpress.com/2014/01/14/machine-learning-lesson-of-the-day-parametric-vs-non-parametric-models/">Parametric vs Non-Parametric algorithms</a></p>
<!-- /wp:paragraph -->]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2403</wp:post_id>
		<wp:post_date><![CDATA[2020-01-13 19:18:41]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2020-01-13 16:18:41]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[deep-dive-into-supervised-learning]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="data-science"><![CDATA[data science]]></category>
		<category domain="post_tag" nicename="machine-learning"><![CDATA[machine learning]]></category>
		<category domain="post_tag" nicename="supervised-learning"><![CDATA[supervised learning]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[2446]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[spay_email]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_mess]]></wp:meta_key>
		<wp:meta_value><![CDATA[I've just published "Deep dive into Supervised Learning". Feedback is highly welcome :) #machinelearning #python #datascience]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1583716846;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:1851;}i:1;a:1:{s:2:"id";i:1774;}i:2;a:1:{s:2:"id";i:2385;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_publicize_twitter_user]]></wp:meta_key>
		<wp:meta_value><![CDATA[@categitau_]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[force_ssl]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[force_ssl_children]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_encloseme]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_pingme]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>1919</wp:comment_id>
			<wp:comment_author><![CDATA[Kennedy KK]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[kkairu@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[197.237.35.250]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2020-01-13 21:05:47]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2020-01-13 18:05:47]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Based on the above diagram does it mean a house of 0 square feet has a price?]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[a:2:{s:4:"time";d:1578938747.430016040802001953125;s:5:"event";s:9:"check-ham";}]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[a:3:{s:4:"time";d:1578941919.437899112701416015625;s:5:"event";s:15:"status-approved";s:4:"user";s:9:"categitau";}]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>1922</wp:comment_id>
			<wp:comment_author><![CDATA[categitau]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[catherinegitau94@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[154.65.19.226]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2020-01-13 22:57:10]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2020-01-13 19:57:10]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I get your point now. I'll change the x axis to not start from zero. Thanks for noticing :)]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>1919</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[a:3:{s:4:"time";d:1578945431.0133941173553466796875;s:5:"event";s:9:"check-ham";s:4:"user";s:9:"categitau";}]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
				</channel>
</rss>
	